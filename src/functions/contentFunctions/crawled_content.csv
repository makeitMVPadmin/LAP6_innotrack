title,url,source,date,publisher,picture,description,full_content,generated_title,enhanced_summary,topic,api_content
"Samsung makes Galaxy AI more accessible with the launch of Galaxy A56, A36, and A26",https://indianexpress.com/article/technology/mobile-tabs/samsung-galaxy-a56-a36-and-a26-launch-specs-features-9863837/,GNews,2025-03-02T04:37:36Z,The Indian Express,https://images.indianexpress.com/2025/03/samsung-galaxy-a56-a36.jpg,"On Sunday, just a day ahead of Mobile World Congress (MWC) 2025, Samsung added three new devices to its popular Galaxy A series––Galaxy A56, Galaxy A36, and Galaxy A26. These are the latest mid-range Samsung smartphones that make Galaxy AI more accessible. However, unlike the Galaxy S/Z series, the AI experiences on these three devices are quite limited, and Samsung refers to them as “awesome intelligence”. “The new Galaxy A series marks an important step in our mission of AI for all, by opening Galaxy’s incredible mobile AI experiences to even more people around the world,” saidTM Roh, president and head of mobile eXperience (MX) Business atSamsungElectronics, in a press statement. Best Face, a photo-editing feature that ensures everyone looks perfect in selfies, is limited to the most expensive Galaxy A56, priced at $499.99. The Object Eraser andGoogleCircle to Search features are available on all three devices. The prices for the Galaxy A36 and Galaxy A26 start at $415 and $375, respectively. As of now, there is no official information on how much these devices will cost in India. These phones, with their redesigned camera array, look refreshing. While the Galaxy A56 and Galaxy A36 have a glass-metal build, the Galaxy A26 looks a tad different with an Infinite-U notch and a plastic frame. All three phones are IP67-rated for water and dust resistance and ship with the latestAndroid15-based One UI 7 skin. They are also eligible for six years of major OS updates. While all three devices feature a 50 MP main shooter and an 8 MP ultra-wide-angle lens, the Galaxy A56 and Galaxy A36 include a 5 MP macro camera and a 12 MP selfie shooter, while the Galaxy A26 has a 2 MP macro lens and a 13 MP front-facing camera. Among the three, the Galaxy A56 is the most modern and premium-looking phone, featuring a 6.7-inch FHD 120Hz narrow bezel display. It is powered by the Exynos 1580 chip, with 8 GB or 12 GB of RAM and 128 GB or 256 GB of storage. Similarly, the Galaxy A36 and Galaxy A26 also come with a 6.7-inch 120Hz display, with configurations of 6 GB, 8 GB, or 12 GB (Galaxy A56 only) of RAM and 128 GB or 256 GB of storage. They are powered by the Snapdragon 6 Gen 3 and Exynos 1380 chipsets, respectively. All three phones pack a 5,000 mAh battery, with up to 45W fast charging on the Galaxy A56 and Galaxy A36, while the Galaxy A26 is limited to 25W charging. Intel's promised $28 billion chip fabrication plants in Ohio are facing further delays, with the first factory now expected to be completed in 2030. The company has been cutting capital expenses and making changes to align with market demand and manage capital responsibly.  This No Is Already Registered. Thanks For Registered Mobile No.","On Sunday, just a day ahead of Mobile World Congress (MWC) 2025, Samsung added three new devices to its popular Galaxy A series––Galaxy A56, Galaxy A36, and Galaxy A26. These are the latest mid-range Samsung smartphones that make Galaxy AI more accessible. However, unlike the Galaxy S/Z series, the AI experiences on these three devices are quite limited, and Samsung refers to them as “awesome intelligence”. “The new Galaxy A series marks an important step in our mission of AI for all, by opening Galaxy’s incredible mobile AI experiences to even more people around the world,” saidTM Roh, president and head of mobile eXperience (MX) Business atSamsungElectronics, in a press statement. Best Face, a photo-editing feature that ensures everyone looks perfect in selfies, is limited to the most expensive Galaxy A56, priced at $499.99. The Object Eraser andGoogleCircle to Search features are available on all three devices. The prices for the Galaxy A36 and Galaxy A26 start at $415 and $375, respectively. As of now, there is no official information on how much these devices will cost in India. These phones, with their redesigned camera array, look refreshing. While the Galaxy A56 and Galaxy A36 have a glass-metal build, the Galaxy A26 looks a tad different with an Infinite-U notch and a plastic frame. All three phones are IP67-rated for water and dust resistance and ship with the latestAndroid15-based One UI 7 skin. They are also eligible for six years of major OS updates. While all three devices feature a 50 MP main shooter and an 8 MP ultra-wide-angle lens, the Galaxy A56 and Galaxy A36 include a 5 MP macro camera and a 12 MP selfie shooter, while the Galaxy A26 has a 2 MP macro lens and a 13 MP front-facing camera. Among the three, the Galaxy A56 is the most modern and premium-looking phone, featuring a 6.7-inch FHD 120Hz narrow bezel display. It is powered by the Exynos 1580 chip, with 8 GB or 12 GB of RAM and 128 GB or 256 GB of storage. Similarly, the Galaxy A36 and Galaxy A26 also come with a 6.7-inch 120Hz display, with configurations of 6 GB, 8 GB, or 12 GB (Galaxy A56 only) of RAM and 128 GB or 256 GB of storage. They are powered by the Snapdragon 6 Gen 3 and Exynos 1380 chipsets, respectively. All three phones pack a 5,000 mAh battery, with up to 45W fast charging on the Galaxy A56 and Galaxy A36, while the Galaxy A26 is limited to 25W charging. Intel's promised $28 billion chip fabrication plants in Ohio are facing further delays, with the first factory now expected to be completed in 2030. The company has been cutting capital expenses and making changes to align with market demand and manage capital responsibly.  This No Is Already Registered. Thanks For Registered Mobile No.","Samsung makes Galaxy AI more accessible with the launch of Galaxy A56, A36, and A26","

Key Points:
",AI,"On Sunday, just a day ahead of Mobile World Congress (MWC) 2025, Samsung added three new devices to its popular Galaxy A series––Galaxy A56, Galaxy A36, and Galaxy A26. These are the latest mid-range Samsung smartphones that make Galaxy AI more acces... [444 chars]"
Samsung's latest A-series mid-rangers are raising the bar,https://www.androidauthority.com/samsung-galaxy-a26-a36-a56-5g-3530959/,GNews,2025-03-01T23:00:20Z,Android Authority,https://www.androidauthority.com/wp-content/uploads/2025/02/Samsung-Galaxy-A36-All-Colors-In-Hand.jpg,"Affiliate links on Android Authority may earn us a commission.Learn more. Published on7 hours ago Samsung started the year strong, introducing its latest flagship hardware as it announced theGalaxy S25 seriesback in January. But for as much we love those kind of premium devices, it’s mid-range phones that really drive sales, and today Samsung’s got its latest batch of A-series phones to introduce. You’ve probably already heard a ton about this hardware thanks to all the leaks over the past few months, so let’s dive right into it and get all the official details on the Galaxy A26 5G, Galaxy A36 5G, and Galaxy A56 5G. Samsung may be introducing this A-series trio today, but let’s get one big asterisk out of the way first: Not all three of these affordable new Galaxy phones are arriving on the same timetable — at least depending on where you live. In the US, Samsung is getting started by releasing the Galaxy A26 5G and Galaxy A36 5G. The A36 5G lands first, with sales formally beginning on March 26. The Galaxy A26 5G is set to follow that one up a couple of days later, on March 28. If you’re looking to stand out a little bit, you might want to check out the Galaxy A36 5G at Best Buy, where you’ll exclusively be able to find its Awesome Lime colorway. What about the Galaxy A56 5G? Although Samsung has committed to bringing the phone to the US, that’s not going to be happening for at least several more months. Right now, the company isn’t announcing an ETA more specific than “later this year,” but when pressing for more info, we were told that we could expect to hear more about those plans sometime in Q3. Across the pond, the situation is quite a bit different. In the UK, all three new Galaxy A models are set to go up for sale on March 19. Samsung further complicates the situation by offering each of these phones in alternate storage and memory configurations: Admittedly, we’d love to see that extra RAM and bonus storage similarly available in the States, but at least with exchange rates being what they are, Samsung’s positioned these models quite a bit more affordably for US shoppers. Samsung’s not trying anything too controversial with any of these models, and while they’re largely iterative refreshes on the Ax5 phones that came before them, we’re still seeing a few important upgrades that serve to make these budget phones feel that much more premium. For instance, the A26 5G introduces a glass back and gets an IP67 ingress rating, both new for Samsung at this price point. Thanks to moves like that, all three of these phones really feel very similar, with only small changes between each: Samsung got this new generation started with theGalaxy A16 5G, and for a phone that costs only around $200, that handset offered an experience that felt a lot more premium that you might expect. With these three latest additions, Samsung’s only trying to continue that trend, and thanks to a few key upgrades along the way, you’ll hopefully be able to find a budget model that precisely aligns with your needs. Here’s what you need to look out for: Samsung’s a big fan of its linear lens layout this year, which really just packages all three rear cameras into one vertical bar, rather than the discrete lenses like we like we got before. All three models feature a comparable collection of sensors, but the A26 5G clearly takes a few hits due to its lower price point, making do with only a 2MP macro lens. We get an upgrade on the highest end for the A56 5G, which pushes its ultra-wide camera up to 12MP. While the A26 5G does have a slightly higher-res 13MP front-facer, the A36 5G and A56 5G swap that out for a 12MP selfie cam that adds support for 10-bit HDR. Really, though, the star here is Samsung’s processing and camera software. All three phones offer features like Object Eraser for using AI to quickly remove distracting and unwanted elements from your pics. You can also define custom filters to invent a vibe for your photos that’s uniquely your own. The Galaxy A56 5G manages to eke ahead with a few more exclusive features, adding Best Face to help you to get the cutest poses out of your messy group shots, and enhanced Nightography for snapping pics with minimal illumination. All three phones offer a 6.7-inch FHD+ Super AMOLED display, featuring a 120Hz refresh rate — and a super-smooth screen like that is an upgrade this year for the A26 5G. For the A36 5G and A56 5G, Samsung is also making sure that you can read that screen on even the sunniest days, equipping them both with panels capable of 1,200 nits in their high-brightness mode. Even if the A26 5G doesn’t see that same upgrade, the manufacturer does point out that this year its screen gets to enjoy a newly slimmed-down bezel. Samsung’s really been doing some good work towards making affordable phones feel like models in a higher price tier, and the Galaxy A26 5G, A36 5G, and A56 5G are perfect examples of that effort. Although they feature plastic frames, the use ofGorilla Glass Victus Pluson both front and back means you won’t actually be touching plastic most of the time you’re using these phones. That premium feeling is enhanced by Samsung’s efforts to slim down the hardware — these all measure under 8mm thick, while the last generation was above. Much like the storage and RAM options, the question of colors is a little more limited in the US. We’ve listed all the options available internationally up in the specs chart you saw above, but if you’re looking for a Galaxy A36 5G in America, your main choice is really just Awesome Black or Awesome Lavender — as we mentioned earlier, Awesome Lime is going to be a Best Buy exclusive. And if you’re shopping for the Galaxy A26 5G in the US, well…we hope you like Black. Because that’s all we’re getting. Samsung has yet to share which color options will be available for the US market when the Galaxy A56 5G arrives later this year. Once again, Samsung doesn’t give us a huge degree of variation across these three phones, and no matter which you pick up, you’re getting a 5,000mAh battery. The main distinction is in terms of charging speed instead of capacity, as only the Galaxy A36 5G and Galaxy A56 5G support 45W charging with Super Fast Charge 2.0. Samsung says that on the A36 5G, that means you’ll be able to charge the device to 70% in under half an hour. Just keep in mind that if you want to take advantage of 45W charging, you’ll need a compatible charger and cable — Samsung isn’t including either in the boxes here. Galaxy users everywhere are still patiently (or not) waiting for theirOne UI 7updates, which have yet to leave beta and start arriving for pre-2025 Samsung devices. And while that day is almost certainly coming up soon, anyone picking up the Galaxy A26 5G, A36 5G, or A56 5G won’t have to worry, with the phones arriving equipped with Samsung’s Android 15-based platform. Circle to Search first debuted on Samsung phones, and the company has continued to feature the useful tool. We saw it expand to older A-series phones last summer, and it’s back for this new set with all thelatest Circle to Search enhancements, like recognizing phone numbers and website addresses. Because this is Samsung, of course, the phones include its Knox security suite, and you can configure exactly what you want to permit in the Knox Matrix dashboard. That gives all these phones a great start, but what’s arguably even more exciting is the long-term support Samsung is promising here. Whether you’re buying the lowly Galaxy A26 5G or responsibly splurging on the Galaxy A56 5G, you can look forward to six major OS releases and six years of security updates. For smartphones in this budget realm, that’s absolutely incredible. Just think: If you manage to hold on to the A26 5G that whole time, you’re effectively paying under $50 a year for the phone. Really, there’s no bad choice here, and if you’re shopping for a phone on a limited budget, Samsung’s offering some very attractive options across a wide range of price points — especially when we factor in the Galaxy A16 5G. Will you be tempted to pick up the A36 5G when it lands later this month? Or are you thinking it might make sense to hold out for the A56 5G a little further down the line?","Affiliate links on Android Authority may earn us a commission.Learn more. Published on7 hours ago Samsung started the year strong, introducing its latest flagship hardware as it announced theGalaxy S25 seriesback in January. But for as much we love those kind of premium devices, it’s mid-range phones that really drive sales, and today Samsung’s got its latest batch of A-series phones to introduce. You’ve probably already heard a ton about this hardware thanks to all the leaks over the past few months, so let’s dive right into it and get all the official details on the Galaxy A26 5G, Galaxy A36 5G, and Galaxy A56 5G. Samsung may be introducing this A-series trio today, but let’s get one big asterisk out of the way first: Not all three of these affordable new Galaxy phones are arriving on the same timetable — at least depending on where you live. In the US, Samsung is getting started by releasing the Galaxy A26 5G and Galaxy A36 5G. The A36 5G lands first, with sales formally beginning on March 26. The Galaxy A26 5G is set to follow that one up a couple of days later, on March 28. If you’re looking to stand out a little bit, you might want to check out the Galaxy A36 5G at Best Buy, where you’ll exclusively be able to find its Awesome Lime colorway. What about the Galaxy A56 5G? Although Samsung has committed to bringing the phone to the US, that’s not going to be happening for at least several more months. Right now, the company isn’t announcing an ETA more specific than “later this year,” but when pressing for more info, we were told that we could expect to hear more about those plans sometime in Q3. Across the pond, the situation is quite a bit different. In the UK, all three new Galaxy A models are set to go up for sale on March 19. Samsung further complicates the situation by offering each of these phones in alternate storage and memory configurations: Admittedly, we’d love to see that extra RAM and bonus storage similarly available in the States, but at least with exchange rates being what they are, Samsung’s positioned these models quite a bit more affordably for US shoppers. Samsung’s not trying anything too controversial with any of these models, and while they’re largely iterative refreshes on the Ax5 phones that came before them, we’re still seeing a few important upgrades that serve to make these budget phones feel that much more premium. For instance, the A26 5G introduces a glass back and gets an IP67 ingress rating, both new for Samsung at this price point. Thanks to moves like that, all three of these phones really feel very similar, with only small changes between each: Samsung got this new generation started with theGalaxy A16 5G, and for a phone that costs only around $200, that handset offered an experience that felt a lot more premium that you might expect. With these three latest additions, Samsung’s only trying to continue that trend, and thanks to a few key upgrades along the way, you’ll hopefully be able to find a budget model that precisely aligns with your needs. Here’s what you need to look out for: Samsung’s a big fan of its linear lens layout this year, which really just packages all three rear cameras into one vertical bar, rather than the discrete lenses like we like we got before. All three models feature a comparable collection of sensors, but the A26 5G clearly takes a few hits due to its lower price point, making do with only a 2MP macro lens. We get an upgrade on the highest end for the A56 5G, which pushes its ultra-wide camera up to 12MP. While the A26 5G does have a slightly higher-res 13MP front-facer, the A36 5G and A56 5G swap that out for a 12MP selfie cam that adds support for 10-bit HDR. Really, though, the star here is Samsung’s processing and camera software. All three phones offer features like Object Eraser for using AI to quickly remove distracting and unwanted elements from your pics. You can also define custom filters to invent a vibe for your photos that’s uniquely your own. The Galaxy A56 5G manages to eke ahead with a few more exclusive features, adding Best Face to help you to get the cutest poses out of your messy group shots, and enhanced Nightography for snapping pics with minimal illumination. All three phones offer a 6.7-inch FHD+ Super AMOLED display, featuring a 120Hz refresh rate — and a super-smooth screen like that is an upgrade this year for the A26 5G. For the A36 5G and A56 5G, Samsung is also making sure that you can read that screen on even the sunniest days, equipping them both with panels capable of 1,200 nits in their high-brightness mode. Even if the A26 5G doesn’t see that same upgrade, the manufacturer does point out that this year its screen gets to enjoy a newly slimmed-down bezel. Samsung’s really been doing some good work towards making affordable phones feel like models in a higher price tier, and the Galaxy A26 5G, A36 5G, and A56 5G are perfect examples of that effort. Although they feature plastic frames, the use ofGorilla Glass Victus Pluson both front and back means you won’t actually be touching plastic most of the time you’re using these phones. That premium feeling is enhanced by Samsung’s efforts to slim down the hardware — these all measure under 8mm thick, while the last generation was above. Much like the storage and RAM options, the question of colors is a little more limited in the US. We’ve listed all the options available internationally up in the specs chart you saw above, but if you’re looking for a Galaxy A36 5G in America, your main choice is really just Awesome Black or Awesome Lavender — as we mentioned earlier, Awesome Lime is going to be a Best Buy exclusive. And if you’re shopping for the Galaxy A26 5G in the US, well…we hope you like Black. Because that’s all we’re getting. Samsung has yet to share which color options will be available for the US market when the Galaxy A56 5G arrives later this year. Once again, Samsung doesn’t give us a huge degree of variation across these three phones, and no matter which you pick up, you’re getting a 5,000mAh battery. The main distinction is in terms of charging speed instead of capacity, as only the Galaxy A36 5G and Galaxy A56 5G support 45W charging with Super Fast Charge 2.0. Samsung says that on the A36 5G, that means you’ll be able to charge the device to 70% in under half an hour. Just keep in mind that if you want to take advantage of 45W charging, you’ll need a compatible charger and cable — Samsung isn’t including either in the boxes here. Galaxy users everywhere are still patiently (or not) waiting for theirOne UI 7updates, which have yet to leave beta and start arriving for pre-2025 Samsung devices. And while that day is almost certainly coming up soon, anyone picking up the Galaxy A26 5G, A36 5G, or A56 5G won’t have to worry, with the phones arriving equipped with Samsung’s Android 15-based platform. Circle to Search first debuted on Samsung phones, and the company has continued to feature the useful tool. We saw it expand to older A-series phones last summer, and it’s back for this new set with all thelatest Circle to Search enhancements, like recognizing phone numbers and website addresses. Because this is Samsung, of course, the phones include its Knox security suite, and you can configure exactly what you want to permit in the Knox Matrix dashboard. That gives all these phones a great start, but what’s arguably even more exciting is the long-term support Samsung is promising here. Whether you’re buying the lowly Galaxy A26 5G or responsibly splurging on the Galaxy A56 5G, you can look forward to six major OS releases and six years of security updates. For smartphones in this budget realm, that’s absolutely incredible. Just think: If you manage to hold on to the A26 5G that whole time, you’re effectively paying under $50 a year for the phone. Really, there’s no bad choice here, and if you’re shopping for a phone on a limited budget, Samsung’s offering some very attractive options across a wide range of price points — especially when we factor in the Galaxy A16 5G. Will you be tempted to pick up the A36 5G when it lands later this month? Or are you thinking it might make sense to hold out for the A56 5G a little further down the line?",Samsung's latest A-series mid-rangers are raising the bar,"

Key Points:
",AI,"Paul Jones / Android Authority
Samsung started the year strong, introducing its latest flagship hardware as it announced the Galaxy S25 series back in January. But for as much we love those kind of premium devices, it’s mid-range phones that really d... [11369 chars]"
Samsung reveals Galaxy A56 with more AI and a modest spec bump,https://www.theverge.com/news/621957/samsung-galaxy-a56-a36-a26-launch-mwc,GNews,2025-03-01T23:00:00Z,The Verge,https://platform.theverge.com/wp-content/uploads/sites/2/2025/02/galaxy-a56-launch.jpg?quality=90&strip=all&crop=2.4270729166667%2C0%2C95.145854166667%2C88.559342447917&w=1200,"News Samsung reveals Galaxy A56 with more AI and a modest spec bump The new line of A-series devices has slightly larger screens and a bigger emphasis on AI image editing tools. The new line of A-series devices has slightly larger screens and a bigger emphasis on AI image editing tools. by Emma Roth Mar 1, 2025, 11:00 PM UTC Link Facebook Threads The Galaxy A56 comes in a range of colors . Image: Samsung Emma Roth is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Samsung is bringing even more AI to its line of budget-friendly phones. The new Galaxy A56, A36, and A26 now come with what the company calls “awesome intelligence,” enabling an array of new AI-powered image editing features already available with Samsung’s pricier S25 lineup. One of these features is Best Face, an AI tool that lets you swap the facial expressions of up to five people in a motion photo in case someone blinks or looks away from the camera. It’s similar to Google Pixel’s Best Take and launched with the Galaxy S25 in January . There’s an “improved” object removal tool and photo filters, along with Google’s Circle to Search feature that allows you to search for text and images just by circling it on your screen. (Samsung added Circle to Search to select A-series devices last August.) The phones will also support up to six years of Android OS and security upgrades, which is longer than what Samsung previously offered for the previous generation. The Galaxy A36 has a last-gen Snapdragon 6 Gen 3 chip. Besides new AI features, Samsung made some small design tweaks to each phone, as well. In addition to an oval-shaped rear camera housing, all three devices now have the same 6.7-inch full HD Plus display with an up to 120Hz refresh rate, as opposed to the smaller 6.6-inch screen on the A55 and A35, and 6.5 inches on the A25. The A56 comes with the same 12MP ultrawide sensor, 50MP main camera, and 5MP macro camera. It also has a 12MP selfie shooter (which is lower than the A55’s 32MP front-facing sensor). Though the A56 comes with an upgraded Exynos 1580 chip, the A36 has a last-gen Snapdragon 6 Gen 3 chip. All three A-series devices have a 5,000 mAh battery, but only the A56 and A36 support 45W charging and come with a “larger vapor chamber” to improve performance. Samsung has also extended IP67 dust and water resistance to the A26 for the first time. The phones will ship with Android 15 and come in a range of colors that vary by device. The Galaxy A26 gets a slightly larger 6.7-inch display, too. Image: Samsung At $499, the Galaxy A56 is the most expensive of the bunch and will launch in the US “later this year.” The A36 will be available on March 26th exclusively at Best Buy starting at $399, while the A26 will cost $299 when it launches on March 28th. The Galaxy A56, A36, and A26 will also be available for purchase in the UK on March 19th for £499, £399, and £299, respectively. See More : AI Mobile News Samsung Tech More in this stream See all What to expect at MWC 2025 Dominic Preston Feb 28 Comments Comment Icon Bubble Most Popular Most Popular Google’s co-founder tells AI staff to stop ‘building nanny products’ With Alexa Plus, Amazon finally reinvents its best product Asus ROG Flow Z13 (2025) review: hold up, integrated graphics are good now? The armless PP-1 turntable is made from a solid block of aluminum AMD’s Radeon RX 9070 and 9070 XT start at $549, ship March 6th Installer A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe. Email (required) Sign Up By submitting your email, you agree to our Terms and Privacy Notice . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Advertiser Content From This is the title for the native ad","News Samsung reveals Galaxy A56 with more AI and a modest spec bump The new line of A-series devices has slightly larger screens and a bigger emphasis on AI image editing tools. The new line of A-series devices has slightly larger screens and a bigger emphasis on AI image editing tools. by Emma Roth Mar 1, 2025, 11:00 PM UTC Link Facebook Threads The Galaxy A56 comes in a range of colors . Image: Samsung Emma Roth is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Samsung is bringing even more AI to its line of budget-friendly phones. The new Galaxy A56, A36, and A26 now come with what the company calls “awesome intelligence,” enabling an array of new AI-powered image editing features already available with Samsung’s pricier S25 lineup. One of these features is Best Face, an AI tool that lets you swap the facial expressions of up to five people in a motion photo in case someone blinks or looks away from the camera. It’s similar to Google Pixel’s Best Take and launched with the Galaxy S25 in January . There’s an “improved” object removal tool and photo filters, along with Google’s Circle to Search feature that allows you to search for text and images just by circling it on your screen. (Samsung added Circle to Search to select A-series devices last August.) The phones will also support up to six years of Android OS and security upgrades, which is longer than what Samsung previously offered for the previous generation. The Galaxy A36 has a last-gen Snapdragon 6 Gen 3 chip. Besides new AI features, Samsung made some small design tweaks to each phone, as well. In addition to an oval-shaped rear camera housing, all three devices now have the same 6.7-inch full HD Plus display with an up to 120Hz refresh rate, as opposed to the smaller 6.6-inch screen on the A55 and A35, and 6.5 inches on the A25. The A56 comes with the same 12MP ultrawide sensor, 50MP main camera, and 5MP macro camera. It also has a 12MP selfie shooter (which is lower than the A55’s 32MP front-facing sensor). Though the A56 comes with an upgraded Exynos 1580 chip, the A36 has a last-gen Snapdragon 6 Gen 3 chip. All three A-series devices have a 5,000 mAh battery, but only the A56 and A36 support 45W charging and come with a “larger vapor chamber” to improve performance. Samsung has also extended IP67 dust and water resistance to the A26 for the first time. The phones will ship with Android 15 and come in a range of colors that vary by device. The Galaxy A26 gets a slightly larger 6.7-inch display, too. Image: Samsung At $499, the Galaxy A56 is the most expensive of the bunch and will launch in the US “later this year.” The A36 will be available on March 26th exclusively at Best Buy starting at $399, while the A26 will cost $299 when it launches on March 28th. The Galaxy A56, A36, and A26 will also be available for purchase in the UK on March 19th for £499, £399, and £299, respectively. See More : AI Mobile News Samsung Tech More in this stream See all What to expect at MWC 2025 Dominic Preston Feb 28 Comments Comment Icon Bubble Most Popular Most Popular Google’s co-founder tells AI staff to stop ‘building nanny products’ With Alexa Plus, Amazon finally reinvents its best product Asus ROG Flow Z13 (2025) review: hold up, integrated graphics are good now? The armless PP-1 turntable is made from a solid block of aluminum AMD’s Radeon RX 9070 and 9070 XT start at $549, ship March 6th Installer A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe. Email (required) Sign Up By submitting your email, you agree to our Terms and Privacy Notice . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Advertiser Content From This is the title for the native ad",Samsung reveals Galaxy A56 with more AI and a modest spec bump,"

Key Points:
",AI,"is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.
Samsung is bringing even more AI to its line of budget-friendly phones. The new Galaxy A56, A36, and A... [2327 chars]"
China tells its AI leaders to avoid U.S. travel over security concerns: report,https://www.theglobeandmail.com/world/article-china-tells-its-ai-leaders-to-avoid-us-travel-over-security-concerns/,GNews,2025-03-01T20:20:57Z,The Globe and Mail,https://www.theglobeandmail.com/resizer/v2/6ZIIHSMOZJCMNB7N5PM4V22WK4.jpg?auth=89bcd7b0511e3afaef88c937c52e689630a513804fec956d30ec4e9803948a62&width=1200&height=800&quality=80&smart=true,"Share Save for later Please log in to bookmark this story. Log In Create Free Account Chinese authorities are instructing the country’s top artificial intelligence entrepreneurs and researchers to avoid travel to the United States, the Wall Street Journal reported on Friday, citing people familiar with the matter. The authorities are concerned that Chinese AI experts traveling abroad could divulge confidential information about the nation’s progress, the newspaper said. Authorities also fear that executives could be detained and used as a bargaining chip in U.S.-China negotiations, the Journal said, drawing parallels to the detention of a Huawei executive in Canada at Washington’s request during the first Trump administration. The U.S. and China are locked in a global AI race, with Chinese startup DeepSeek recently launching AI models that it claims rival or surpass U.S. industry leaders such as OpenAI and Alphabet Inc’s Google, at significantly lower cost. The White House and China’s State Council Information Office, which handles media enquiries on behalf of the government, did not immediately respond to requests from Reuters for comment. Chinese President Xi Jinping told a meeting of top Communist Party officials on Friday to improve China’s overall security, including in the realms of cybersecurity and artificial intelligence, China’s state broadcaster reported on Saturday. “We should give top priority to defending the country’s political security,” Xi was quoted as having told other members of the governing Politburo. Last month, the Chinese leader held a rare meeting with some of the biggest names in the world’s second-largest economy’s technology sector, urging them to “show their talent” and be confident in the power of China’s model and market. Chinese executives who choose to travel are instructed to report their plans before leaving and, upon returning, to brief authorities on what they did and whom they met, the Journal report said. DeepSeek founder Liang Wenfeng declined an invitation to attend an AI summit in Paris in February, according to the report. Another founder of a major Chinese AI startup cancelled a planned U.S. trip last year following instructions from Beijing, the Journal added.","Share Save for later Please log in to bookmark this story. Log In Create Free Account Chinese authorities are instructing the country’s top artificial intelligence entrepreneurs and researchers to avoid travel to the United States, the Wall Street Journal reported on Friday, citing people familiar with the matter. The authorities are concerned that Chinese AI experts traveling abroad could divulge confidential information about the nation’s progress, the newspaper said. Authorities also fear that executives could be detained and used as a bargaining chip in U.S.-China negotiations, the Journal said, drawing parallels to the detention of a Huawei executive in Canada at Washington’s request during the first Trump administration. The U.S. and China are locked in a global AI race, with Chinese startup DeepSeek recently launching AI models that it claims rival or surpass U.S. industry leaders such as OpenAI and Alphabet Inc’s Google, at significantly lower cost. The White House and China’s State Council Information Office, which handles media enquiries on behalf of the government, did not immediately respond to requests from Reuters for comment. Chinese President Xi Jinping told a meeting of top Communist Party officials on Friday to improve China’s overall security, including in the realms of cybersecurity and artificial intelligence, China’s state broadcaster reported on Saturday. “We should give top priority to defending the country’s political security,” Xi was quoted as having told other members of the governing Politburo. Last month, the Chinese leader held a rare meeting with some of the biggest names in the world’s second-largest economy’s technology sector, urging them to “show their talent” and be confident in the power of China’s model and market. Chinese executives who choose to travel are instructed to report their plans before leaving and, upon returning, to brief authorities on what they did and whom they met, the Journal report said. DeepSeek founder Liang Wenfeng declined an invitation to attend an AI summit in Paris in February, according to the report. Another founder of a major Chinese AI startup cancelled a planned U.S. trip last year following instructions from Beijing, the Journal added.",China tells its AI leaders to avoid U.S. travel over security concerns: report,"

Key Points:
",AI,"Chinese authorities are instructing the country’s top artificial intelligence entrepreneurs and researchers to avoid travel to the United States, the Wall Street Journal reported on Friday, citing people familiar with the matter.
The authorities are ... [1908 chars]"
"Sports News Today Live Updates on March 2, 2025: India vs New Zealand Prediction: Who’ll win today’s IND vs NZ match in Dubai? AI, fantasy team and more",https://www.livemint.com/sports/latest-sports-news-on-march-2-2025-live-updates-11740886034293.html,GNews,2025-03-01T18:30:00Z,Livemint,https://www.livemint.com/lm-img/img/2025/03/02/1600x900/IND_vs_NZ_1740889930046_1740889946965.jpeg,"LIVE UPDATES Sports News Today Live Updates on March 2, 2025: Rohit Sharma did PR stunt by resting himself in Sydney? Amit Mishra sets the record straight: ‘Name one captain who…’ 4 min read . Updated: 02 Mar 2025, 11:09 AM IST Livemint Sports News Today Live Updates on March 2, 2025: Stay up-to-date with the latest in sports, from cricket series and hockey tournaments to badminton championships and beyond. We bring you highlights, player performances, and expert analysis across major sports. With real-time updates on match results, league standings, and standout moments, you’ll never miss a key play or turning point. Premium Sports News Today Live Updates: Rohit Sharma did PR stunt by resting himself in Sydney? Amit Mishra sets the record straight: ‘Name one captain who…’","LIVE UPDATES Sports News Today Live Updates on March 2, 2025: Rohit Sharma did PR stunt by resting himself in Sydney? Amit Mishra sets the record straight: ‘Name one captain who…’ 4 min read . Updated: 02 Mar 2025, 11:09 AM IST Livemint Sports News Today Live Updates on March 2, 2025: Stay up-to-date with the latest in sports, from cricket series and hockey tournaments to badminton championships and beyond. We bring you highlights, player performances, and expert analysis across major sports. With real-time updates on match results, league standings, and standout moments, you’ll never miss a key play or turning point. Premium Sports News Today Live Updates: Rohit Sharma did PR stunt by resting himself in Sydney? Amit Mishra sets the record straight: ‘Name one captain who…’","Sports News Today Live Updates on March 2, 2025: India vs New Zealand Prediction: Who’ll win today’s IND vs NZ match in Dubai? AI, fantasy team and more","

Key Points:
",AI,"LIVE UPDATES
Sports News Today Live Updates on March 2, 2025: India vs New Zealand Prediction: Who’ll win today’s IND vs NZ match in Dubai? AI, fantasy team and more
3 min read . Updated: 02 Mar 2025, 10:03 AM IST
Sports News Today Live Updates on Ma... [352 chars]"
'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket,https://www.news18.com/cricket/can-we-clone-five-jasprit-bumrahs-rahul-dravid-jokes-about-pacer-to-explain-how-ai-can-be-used-in-cricket-9246316.html,GNews,2025-03-01T16:21:47Z,News18,https://images.news18.com/ibnlive/uploads/2025/03/Dravid-Bumrah-AI-2025-03-95b1c5edba524cc47e2c6f6e1449eab1-16x9.png,"'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket Curated By : Cricketnext Staff News18.com Last Updated: March 01, 2025, 21:51 IST Rahul Dravid hopes cricket can use AI and data analysis for injury prediction without losing the sport's human element. He emphasizes maintaining the uniqueness of players like Jasprit Bumrah. reset Follow us on Flipboard Follow us on Google News Rahul Dravid (PC: Instagram) and Jasprit Bumrah (PTI Photo). Former India head coach Rahul Dravid hopes that cricket can use technologies like Artificial Intelligence and data analysis to aid in facets like injury prediction, without replacing the ‘human element’ in the sport. He explained it by saying that he wouldn’t want any technology that creates five ‘clones’ of pacer Jasprit Bumrah after his back injury because that would take away the ‘fun’ and ‘uniqueness’ of the sport. Bumrah picked up a stress injury in his back in January during the 2024-25 Border-Gavaskar Trophy. He was advised weeks of rest and has only recently returned to the cricket field, though the injury did rule him out of the ongoing 2025 Champions Trophy in Pakistan and the UAE. Dravid was a big driving force of use of data analysis in the Indian men’s team in the 2023-24 season but still advises only moving forward with balance. related stories “Can we clone five Bumrahs or something? I mean, I’m just saying that, but that wouldn’t be fun either, right? I mean, what would be the fun in that, right? Where’s the uniqueness then?"" Dravid said at a Mumbai event on Saturday, as quoted by news agency PTI . “I think maybe leave sport alone for a bit and we don’t want to get AI too involved in sport. There’s got to be that human element to it. There’s got to be a level of uniqueness to sport. So, that would be my wish really that we’d never get to a point where we are… It becomes easy,"" he said. “Bumrah is unique because it’s so hard to do what Bumrah does."" Explaining what he hoped for AI to do, Dravid said: “On the sporting field, one of the things that you are really hopeful about what AI might be able to do, is (to predict) injuries."" “You just look at the level of injuries that we have and no one really has a perfect answer as to why people pick up stress fractures and there is no one size fits all; I am just using a stress fracture for the back as an example for fast bowlers. You have seen over the last years so much of data, so much of sports medicine, science and stuff going into it but no one being able to really predict that, that’s sad, that’s (about) people’s careers, lives. So if AI can get us there and we will be able to predict injuries,"" he added. top videos View all Swipe Left For Next Video View all “In this day and age, you’ve got to be able to use all of this data and technology for your benefit. (But) you can’t become a slave of it, but certainly there are huge advantages to using artificial intelligence. You’ve got to find that balance between recognising that the data is important,"" the batting stalwart said. Dravid said that during his stint, he used to collect tons of data and information and filter only a fraction of important bits to the players, that too after judging the right time and moment for it. tags : Champions trophy Indian cricket team jasprit bumrah Rahul Dravid Location : Mumbai, India, India First Published: March 01, 2025, 21:51 IST News cricket 'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket Read More","'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket Curated By : Cricketnext Staff News18.com Last Updated: March 01, 2025, 21:51 IST Rahul Dravid hopes cricket can use AI and data analysis for injury prediction without losing the sport's human element. He emphasizes maintaining the uniqueness of players like Jasprit Bumrah. reset Follow us on Flipboard Follow us on Google News Rahul Dravid (PC: Instagram) and Jasprit Bumrah (PTI Photo). Former India head coach Rahul Dravid hopes that cricket can use technologies like Artificial Intelligence and data analysis to aid in facets like injury prediction, without replacing the ‘human element’ in the sport. He explained it by saying that he wouldn’t want any technology that creates five ‘clones’ of pacer Jasprit Bumrah after his back injury because that would take away the ‘fun’ and ‘uniqueness’ of the sport. Bumrah picked up a stress injury in his back in January during the 2024-25 Border-Gavaskar Trophy. He was advised weeks of rest and has only recently returned to the cricket field, though the injury did rule him out of the ongoing 2025 Champions Trophy in Pakistan and the UAE. Dravid was a big driving force of use of data analysis in the Indian men’s team in the 2023-24 season but still advises only moving forward with balance. related stories “Can we clone five Bumrahs or something? I mean, I’m just saying that, but that wouldn’t be fun either, right? I mean, what would be the fun in that, right? Where’s the uniqueness then?"" Dravid said at a Mumbai event on Saturday, as quoted by news agency PTI . “I think maybe leave sport alone for a bit and we don’t want to get AI too involved in sport. There’s got to be that human element to it. There’s got to be a level of uniqueness to sport. So, that would be my wish really that we’d never get to a point where we are… It becomes easy,"" he said. “Bumrah is unique because it’s so hard to do what Bumrah does."" Explaining what he hoped for AI to do, Dravid said: “On the sporting field, one of the things that you are really hopeful about what AI might be able to do, is (to predict) injuries."" “You just look at the level of injuries that we have and no one really has a perfect answer as to why people pick up stress fractures and there is no one size fits all; I am just using a stress fracture for the back as an example for fast bowlers. You have seen over the last years so much of data, so much of sports medicine, science and stuff going into it but no one being able to really predict that, that’s sad, that’s (about) people’s careers, lives. So if AI can get us there and we will be able to predict injuries,"" he added. top videos View all Swipe Left For Next Video View all “In this day and age, you’ve got to be able to use all of this data and technology for your benefit. (But) you can’t become a slave of it, but certainly there are huge advantages to using artificial intelligence. You’ve got to find that balance between recognising that the data is important,"" the batting stalwart said. Dravid said that during his stint, he used to collect tons of data and information and filter only a fraction of important bits to the players, that too after judging the right time and moment for it. tags : Champions trophy Indian cricket team jasprit bumrah Rahul Dravid Location : Mumbai, India, India First Published: March 01, 2025, 21:51 IST News cricket 'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket Read More",'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket,"

Key Points:
",AI,"'Can We Clone Five Bumrahs?': Rahul Dravid Jokes About Pacer To Explain How AI Can Be Used In Cricket
Curated By :
News18.com
Last Updated: March 01, 2025, 21:51 IST
Rahul Dravid hopes cricket can use AI and data analysis for injury prediction withou... [2996 chars]"
"After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT",https://www.techradar.com/computing/artificial-intelligence/after-rolling-out-sora-beyond-the-us-openai-plans-to-put-the-video-ai-tool-inside-chatgpt,GNews,2025-03-01T15:30:00Z,TechRadar,https://cdn.mos.cms.futurecdn.net/KAzXdHQampgZReDAG3YRAo-1200-80.jpg,"Computing Software Artificial Intelligence After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT News By David Nield published 1 March 2025 More tools in the ChatGPT app Comments ( 0 ) ( ) When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works . Sora creates videos from text prompts (Image credit: OpenAI) OpenAI is reportedly going to add Sora to the ChatGPT app As yet there's no timeline for the integration The full Sora experience will continue to be a separate app Having launched in the US last December , and going live for a bunch of European countries earlier this week , OpenAI 's video generation tool Sora is expanding quickly – and the plan is to eventually put it inside the ChatGPT interface. That's according to discussions at a company meeting, as reported by TechCrunch . Right now, Sora lives on its own separate website, and isn't part of the ChatGPT apps on web or mobile – apps that do include image generation capabilities. Sora product lead Rohan Sahai apparently said that OpenAI has plans to put Sora in more places, as well as to build on the features and tools of the AI video maker. However, it seems likely Sora would also remain as a separate, standalone experience too. The version of Sora put inside ChatGPT may not be as comprehensive as the current web tool, Sahai admitted. Part of the reason the integration isn't already in place is to avoid cluttering up the ChatGPT interface too much. More on the way The Sora web interface (Image credit: Future) Right now there's no timeline for any of this, though Sora inside ChatGPT could help drive more paid subscriptions: right now, you need to be a ChatGPT Plus ($20-a-month) or ChatGPT Pro ($200-a-month) subscriber to be able to use Sora. You get different limits on video resolution and the number of videos you make, depending on how much you pay. Every user gets a certain number of credits each month, and videos that are longer and of higher quality cost more credits. The same user credentials are used to sign into both ChatGPT and Sora, so some of the work is already done. However, the Sora website has elements like a featured video showcase that would be difficult to cram into ChatGPT. Get daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Sahai reportedly also said that an improved version of the AI model running underneath Sora is on the way, and said that OpenAI was also working on an image generator powered by Sora that could produce more photorealistic images. Stay tuned. You might also like ChatGPT-4.5 is here for paying subscribers OpenAI confirms 400 million weekly ChatGPT users Exciting new features for all ChatGPT users See more Computing News TOPICS OpenAI ChatGPT David Nield Social Links Navigation Freelance Contributor Dave is a freelance tech journalist who has been writing about gadgets, apps and the web for more than two decades. Based out of Stockport, England, on TechRadar you'll find him covering news, features and reviews, particularly for phones, tablets and wearables. Working to ensure our breaking news coverage is the best in the business over weekends, David also has bylines at Gizmodo, T3, PopSci and a few other places besides, as well as being many years editing the likes of PC Explorer and The Hardware Handbook. You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. Logout More about artificial intelligence Now that ChatGPT Voice Mode is free, is it even worth paying for ChatGPT Plus anymore? What is Apple Intelligence: everything you need to know about the AI toolkit Latest Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it See more latest Most Popular Netflix has 8 new movies and shows with 100% on Rotten Tomatoes so far in 2025 – here they are De'Longhi's new bean-to-cup coffee machine could make you a milk-frothing maestro The price of AMD’s most powerful processor ever has been slashed by almost half and I can't understand why ICYMI: the 7 biggest tech stories of the week, from a next-gen Alexa to the new iPhone 16e This smart baby monitor with dual mode and enhanced alerts from Momcozy will give new parents peace of mind Netflix's trailer for a new comedy show that looks like Knives Out meets Weekend at Bernie's – Welcome to the Family seems like chaotic fun How to watch Brit Awards 2025 online from anywhere and for free Around $40 billion worth of illicit crypto transactions took place in 2024 Is this the end for electric supercars? More luxury automakers, including Aston Martin, delay plans for EVs Is this the perfect portable router? GL-iNet's latest model offers dual SIM, load balancing, gigabit LAN ports and four massive antennas Alexa+ – Here’s how to sign up for early access LATEST ARTICLES 1 5 new movies on Paramount Plus in March 2025 with over 90% on Rotten Tomatoes 2 iPhone 16e vs iPhone 16: which model is right for you? 3 Everything leaving Netflix in March 2025 – catch Sixteen Candles, Mad Max: Fury Road, and more before they're gone 4 Massive TV sale at Walmart is live: clearance prices on 4K, QLED and OLED TVs from $148 5 Quordle hints and answers for Sunday, March 2 (game #1133)","Computing Software Artificial Intelligence After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT News By David Nield published 1 March 2025 More tools in the ChatGPT app Comments ( 0 ) ( ) When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works . Sora creates videos from text prompts (Image credit: OpenAI) OpenAI is reportedly going to add Sora to the ChatGPT app As yet there's no timeline for the integration The full Sora experience will continue to be a separate app Having launched in the US last December , and going live for a bunch of European countries earlier this week , OpenAI 's video generation tool Sora is expanding quickly – and the plan is to eventually put it inside the ChatGPT interface. That's according to discussions at a company meeting, as reported by TechCrunch . Right now, Sora lives on its own separate website, and isn't part of the ChatGPT apps on web or mobile – apps that do include image generation capabilities. Sora product lead Rohan Sahai apparently said that OpenAI has plans to put Sora in more places, as well as to build on the features and tools of the AI video maker. However, it seems likely Sora would also remain as a separate, standalone experience too. The version of Sora put inside ChatGPT may not be as comprehensive as the current web tool, Sahai admitted. Part of the reason the integration isn't already in place is to avoid cluttering up the ChatGPT interface too much. More on the way The Sora web interface (Image credit: Future) Right now there's no timeline for any of this, though Sora inside ChatGPT could help drive more paid subscriptions: right now, you need to be a ChatGPT Plus ($20-a-month) or ChatGPT Pro ($200-a-month) subscriber to be able to use Sora. You get different limits on video resolution and the number of videos you make, depending on how much you pay. Every user gets a certain number of credits each month, and videos that are longer and of higher quality cost more credits. The same user credentials are used to sign into both ChatGPT and Sora, so some of the work is already done. However, the Sora website has elements like a featured video showcase that would be difficult to cram into ChatGPT. Get daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Sahai reportedly also said that an improved version of the AI model running underneath Sora is on the way, and said that OpenAI was also working on an image generator powered by Sora that could produce more photorealistic images. Stay tuned. You might also like ChatGPT-4.5 is here for paying subscribers OpenAI confirms 400 million weekly ChatGPT users Exciting new features for all ChatGPT users See more Computing News TOPICS OpenAI ChatGPT David Nield Social Links Navigation Freelance Contributor Dave is a freelance tech journalist who has been writing about gadgets, apps and the web for more than two decades. Based out of Stockport, England, on TechRadar you'll find him covering news, features and reviews, particularly for phones, tablets and wearables. Working to ensure our breaking news coverage is the best in the business over weekends, David also has bylines at Gizmodo, T3, PopSci and a few other places besides, as well as being many years editing the likes of PC Explorer and The Hardware Handbook. You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. Logout More about artificial intelligence Now that ChatGPT Voice Mode is free, is it even worth paying for ChatGPT Plus anymore? What is Apple Intelligence: everything you need to know about the AI toolkit Latest Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it See more latest Most Popular Netflix has 8 new movies and shows with 100% on Rotten Tomatoes so far in 2025 – here they are De'Longhi's new bean-to-cup coffee machine could make you a milk-frothing maestro The price of AMD’s most powerful processor ever has been slashed by almost half and I can't understand why ICYMI: the 7 biggest tech stories of the week, from a next-gen Alexa to the new iPhone 16e This smart baby monitor with dual mode and enhanced alerts from Momcozy will give new parents peace of mind Netflix's trailer for a new comedy show that looks like Knives Out meets Weekend at Bernie's – Welcome to the Family seems like chaotic fun How to watch Brit Awards 2025 online from anywhere and for free Around $40 billion worth of illicit crypto transactions took place in 2024 Is this the end for electric supercars? More luxury automakers, including Aston Martin, delay plans for EVs Is this the perfect portable router? GL-iNet's latest model offers dual SIM, load balancing, gigabit LAN ports and four massive antennas Alexa+ – Here’s how to sign up for early access LATEST ARTICLES 1 5 new movies on Paramount Plus in March 2025 with over 90% on Rotten Tomatoes 2 iPhone 16e vs iPhone 16: which model is right for you? 3 Everything leaving Netflix in March 2025 – catch Sixteen Candles, Mad Max: Fury Road, and more before they're gone 4 Massive TV sale at Walmart is live: clearance prices on 4K, QLED and OLED TVs from $148 5 Quordle hints and answers for Sunday, March 2 (game #1133)","After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT","

Key Points:
",AI,"OpenAI is reportedly going to add Sora to the ChatGPT app
As yet there's no timeline for the integration
The full Sora experience will continue to be a separate app
Having launched in the US last December, and going live for a bunch of European count... [2116 chars]"
"Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it",https://www.techradar.com/home/smart-home/amazons-new-ai-powered-alexa-is-a-lot-more-fun-to-talk-to-than-chatgpt-or-siri-and-i-like-it,GNews,2025-03-01T15:30:00Z,TechRadar,https://cdn.mos.cms.futurecdn.net/5phSxeNDVBT5ghzQo6eDAU-1200-80.jpg,"Home Smart Home Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it Opinion By Graham Barlow published 1 March 2025 Where have you been, Alexa? Comments ( 0 ) ( ) When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works . (Image credit: Future) Now we know that an AI-powered Alexa+ is coming to your Amazon device very soon (assuming you live in the US of course – international release dates are still to be confirmed), we can welcome Amazon back to the virtual assistant race with open arms. Alexa, where have you been? Despite leading the charge on virtual assistants way back in 2014, Alexa effectively dropped out of the development race for a year or two there while Silicon Valley’s young AI upstarts, like ChatGPT and Gemini , took over the virtual assistant space. While they were adding voice modes and starting to interact with our calendars and inboxes, Alexa was still stuck as being nothing more than a glorified egg timer that could talk. It’s not like Amazon’s main virtual assistant competitor, Apple ’s Siri, has really done anything to take advantage of the stall in Alexa’s development. Apple was equally wrong footed by the AI equivalent of the Cambrian explosion , but at least Apple has made some attempt to get into the AI wars with Apple Intelligence , even if it seems to be endlessly playing catch-up, tied to a yearly release schedule when the rest of the AI market was simply reinventing itself every three months. Alexa+ on a Fire TV. (Image credit: Amazon) Alexa is back, baby But now Alexa is back, and it’s got a bit of an attitude. One thing I noticed about yesterday’s Alexa+ demo from Amazon was that the presenters, like Amazon Devices lead Panos Panay, frequently referred to Alexa as “her”, and there was absolutely no mention of changing Alexa to a different voice, or even changing “her” gender. (I still find it weird to call an AI either “he” or “she”, so I’m sticking with “it” in this article. Don’t cancel me, please.) In contrast, when OpenAI introduced ChatGPT ’s Advanced Voice Mode, adding a realistic voice for the first time to ChatGPT, it put a lot of effort into showing off all the different voices you could get it to use, half of which are male and half of which are female. Apple’s Siri is also very cosmopolitan. It can talk in multiple accents and offers a choice between male and female voices. To be clear, I think it’s unlikely that Amazon won’t offer a way to customize the voice of Alexa+ – you can get the current Alexa to change voices by simply saying “Change voice” – but Amazon is clearly trying to present Alexa in a more personal and human way than the other big tech companies have so far with their talking AIs. Fun to talk to Not only does Amazon refer to Alexa as “her” but she (damnit, see how easy it is to start thinking of Alexa as a person?) has much more of a personality than the responses you get from ChatGPT, Siri and Gemini. Alexa even makes jokes and throws in funny comments. In the videos of the demo of Alexa+ you can see that she (OK, I give in) sounds light, breezy and fun to talk to: @techradar ♬ original sound - TechRadar Alexa+ even manages to crack a joke or two: @techradar ♬ original sound - TechRadar Incidentally, talking of changing Alexa voices, Amazon used to offer celebrity voices for Alexa as downloads. The US-only feature cost $4.99 each for Samuel L Jackson, Shaquille O-Neal and Melissa McCarthy, but the company pulled them all in 2023. I would love to have been able to talk to a Samuel L Jackson version of Alexa+ with this enlarged range of responses and attitude. Can you imagine how cool it would be if Samuel L Jackson Alexa+ could channel some of the energy from his movie roles into your Alexa device? Jules Winnfield from Pulp Fiction telling me what’s on my calendar today and that yes, ""I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers"", would pump start my day in a way that no amount of caffeine could ever match. In contrast, talking to ChatGPT or Siri at the moment lacks the same sort of emotional spark that even basic Alexa+ has. They are functional, yes, but not as fun as the direction Amazon is taking Alexa+. By making an AI chatbot with a bit of personality Amazon might have just found a niche that means it can recover from its slow start in the AI arms race. Not to mention that Alexa+ is designed primarily to be on a device that sits in your home, not on a phone in your pocket or a computer at your desk. While that might not be much of a technical difference, the difference in the way you use Alexa+ means you’re in a different location, at a different distance, and ultimately in a different frame of mind when you use it. Nobody is expecting Alexa+ to be able to produce C++ code, for example. Alexa+ is simply there for all the fun things in life like playing music, watching movies and reading your kids a story. Sure, it can do clever things like book a meal at your favorite restaurant and organize your diary, too, but while Apple or OpenAI are going to own the AI in your pocket or at work, it’s looking like Amazon is going to own the AI in your home. I just wish it would bring back the custom voices, so Alexa+ can truly be his or her brother's keeper. You might also like All your burning Alexa+ questions answered – by one of the people who built it Amazon's upgraded Alexa+ will enable Fire TV devices to skip to a particular scene in a movie just by describing it Alexa Plus explained: 9 things you need to know about Amazon's new AI-powered assistant Get daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. TOPICS ChatGPT Graham Barlow Social Links Navigation Senior Editor, AI Graham is the Senior Editor for AI at TechRadar. With over 25 years of experience in both online and print journalism, Graham has worked for various market-leading tech brands including Computeractive, PC Pro, iMore, MacFormat, Mac|Life, Maximum PC, and more. He specializes in reporting on everything to do with AI and has appeared on BBC TV shows like BBC One Breakfast and on Radio 4 commenting on the latest trends in tech. Graham has an honors degree in Computer Science and spends his spare time podcasting and blogging. You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. Logout More about smart home Alexa+ – Here’s how to sign up for early access Philips Hue starter kits just hit a record-low price – and the best deal isn't at Amazon Latest After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT See more latest Most Popular How to address Shadow IT challenges in the age of GenAI AI’s missing puzzle piece: why businesses need neuro-symbolic intelligence The role of strategic print management in efficiency and security Fact vs. fiction: dissecting the improbability of a Zero-Day doomsday scenario Should ransomware payments be illegal? Amazon's goal is to put an Echo screen in everyone’s house You can pay $19.99 a month for Alexa Plus – but why would you? What is Digital Experience Monitoring and why is it critical for every organization? In the age of AI, everybody could lose the right to anonymity The hidden costs of data subject access requests (DSARs) on privacy The truth about GenAI security: your business can't afford to “wait and see” LATEST ARTICLES 1 5 new movies on Paramount Plus in March 2025 with over 90% on Rotten Tomatoes 2 iPhone 16e vs iPhone 16: which model is right for you? 3 Everything leaving Netflix in March 2025 – catch Sixteen Candles, Mad Max: Fury Road, and more before they're gone 4 Massive TV sale at Walmart is live: clearance prices on 4K, QLED and OLED TVs from $148 5 Quordle hints and answers for Sunday, March 2 (game #1133)","Home Smart Home Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it Opinion By Graham Barlow published 1 March 2025 Where have you been, Alexa? Comments ( 0 ) ( ) When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works . (Image credit: Future) Now we know that an AI-powered Alexa+ is coming to your Amazon device very soon (assuming you live in the US of course – international release dates are still to be confirmed), we can welcome Amazon back to the virtual assistant race with open arms. Alexa, where have you been? Despite leading the charge on virtual assistants way back in 2014, Alexa effectively dropped out of the development race for a year or two there while Silicon Valley’s young AI upstarts, like ChatGPT and Gemini , took over the virtual assistant space. While they were adding voice modes and starting to interact with our calendars and inboxes, Alexa was still stuck as being nothing more than a glorified egg timer that could talk. It’s not like Amazon’s main virtual assistant competitor, Apple ’s Siri, has really done anything to take advantage of the stall in Alexa’s development. Apple was equally wrong footed by the AI equivalent of the Cambrian explosion , but at least Apple has made some attempt to get into the AI wars with Apple Intelligence , even if it seems to be endlessly playing catch-up, tied to a yearly release schedule when the rest of the AI market was simply reinventing itself every three months. Alexa+ on a Fire TV. (Image credit: Amazon) Alexa is back, baby But now Alexa is back, and it’s got a bit of an attitude. One thing I noticed about yesterday’s Alexa+ demo from Amazon was that the presenters, like Amazon Devices lead Panos Panay, frequently referred to Alexa as “her”, and there was absolutely no mention of changing Alexa to a different voice, or even changing “her” gender. (I still find it weird to call an AI either “he” or “she”, so I’m sticking with “it” in this article. Don’t cancel me, please.) In contrast, when OpenAI introduced ChatGPT ’s Advanced Voice Mode, adding a realistic voice for the first time to ChatGPT, it put a lot of effort into showing off all the different voices you could get it to use, half of which are male and half of which are female. Apple’s Siri is also very cosmopolitan. It can talk in multiple accents and offers a choice between male and female voices. To be clear, I think it’s unlikely that Amazon won’t offer a way to customize the voice of Alexa+ – you can get the current Alexa to change voices by simply saying “Change voice” – but Amazon is clearly trying to present Alexa in a more personal and human way than the other big tech companies have so far with their talking AIs. Fun to talk to Not only does Amazon refer to Alexa as “her” but she (damnit, see how easy it is to start thinking of Alexa as a person?) has much more of a personality than the responses you get from ChatGPT, Siri and Gemini. Alexa even makes jokes and throws in funny comments. In the videos of the demo of Alexa+ you can see that she (OK, I give in) sounds light, breezy and fun to talk to: @techradar ♬ original sound - TechRadar Alexa+ even manages to crack a joke or two: @techradar ♬ original sound - TechRadar Incidentally, talking of changing Alexa voices, Amazon used to offer celebrity voices for Alexa as downloads. The US-only feature cost $4.99 each for Samuel L Jackson, Shaquille O-Neal and Melissa McCarthy, but the company pulled them all in 2023. I would love to have been able to talk to a Samuel L Jackson version of Alexa+ with this enlarged range of responses and attitude. Can you imagine how cool it would be if Samuel L Jackson Alexa+ could channel some of the energy from his movie roles into your Alexa device? Jules Winnfield from Pulp Fiction telling me what’s on my calendar today and that yes, ""I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers"", would pump start my day in a way that no amount of caffeine could ever match. In contrast, talking to ChatGPT or Siri at the moment lacks the same sort of emotional spark that even basic Alexa+ has. They are functional, yes, but not as fun as the direction Amazon is taking Alexa+. By making an AI chatbot with a bit of personality Amazon might have just found a niche that means it can recover from its slow start in the AI arms race. Not to mention that Alexa+ is designed primarily to be on a device that sits in your home, not on a phone in your pocket or a computer at your desk. While that might not be much of a technical difference, the difference in the way you use Alexa+ means you’re in a different location, at a different distance, and ultimately in a different frame of mind when you use it. Nobody is expecting Alexa+ to be able to produce C++ code, for example. Alexa+ is simply there for all the fun things in life like playing music, watching movies and reading your kids a story. Sure, it can do clever things like book a meal at your favorite restaurant and organize your diary, too, but while Apple or OpenAI are going to own the AI in your pocket or at work, it’s looking like Amazon is going to own the AI in your home. I just wish it would bring back the custom voices, so Alexa+ can truly be his or her brother's keeper. You might also like All your burning Alexa+ questions answered – by one of the people who built it Amazon's upgraded Alexa+ will enable Fire TV devices to skip to a particular scene in a movie just by describing it Alexa Plus explained: 9 things you need to know about Amazon's new AI-powered assistant Get daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. TOPICS ChatGPT Graham Barlow Social Links Navigation Senior Editor, AI Graham is the Senior Editor for AI at TechRadar. With over 25 years of experience in both online and print journalism, Graham has worked for various market-leading tech brands including Computeractive, PC Pro, iMore, MacFormat, Mac|Life, Maximum PC, and more. He specializes in reporting on everything to do with AI and has appeared on BBC TV shows like BBC One Breakfast and on Radio 4 commenting on the latest trends in tech. Graham has an honors degree in Computer Science and spends his spare time podcasting and blogging. You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. Logout More about smart home Alexa+ – Here’s how to sign up for early access Philips Hue starter kits just hit a record-low price – and the best deal isn't at Amazon Latest After rolling out Sora beyond the US, OpenAI plans to put the video AI tool inside ChatGPT See more latest Most Popular How to address Shadow IT challenges in the age of GenAI AI’s missing puzzle piece: why businesses need neuro-symbolic intelligence The role of strategic print management in efficiency and security Fact vs. fiction: dissecting the improbability of a Zero-Day doomsday scenario Should ransomware payments be illegal? Amazon's goal is to put an Echo screen in everyone’s house You can pay $19.99 a month for Alexa Plus – but why would you? What is Digital Experience Monitoring and why is it critical for every organization? In the age of AI, everybody could lose the right to anonymity The hidden costs of data subject access requests (DSARs) on privacy The truth about GenAI security: your business can't afford to “wait and see” LATEST ARTICLES 1 5 new movies on Paramount Plus in March 2025 with over 90% on Rotten Tomatoes 2 iPhone 16e vs iPhone 16: which model is right for you? 3 Everything leaving Netflix in March 2025 – catch Sixteen Candles, Mad Max: Fury Road, and more before they're gone 4 Massive TV sale at Walmart is live: clearance prices on 4K, QLED and OLED TVs from $148 5 Quordle hints and answers for Sunday, March 2 (game #1133)","Amazon’s new AI-powered Alexa+ is a lot more fun to talk to than ChatGPT or Siri, and I like it","

Key Points:
",AI,"Now we know that an AI-powered Alexa+ is coming to your Amazon device very soon (assuming you live in the US of course – international release dates are still to be confirmed), we can welcome Amazon back to the virtual assistant race with open arms. ... [4700 chars]"
DeepSeek Reports 545% Daily Profit Despite Free AI Services,https://analyticsindiamag.com/ai-news-updates/deepseek-reports-545-daily-profit-despite-free-ai-services/,GNews,2025-03-01T13:36:33Z,Analytics India Magazine,https://analyticsindiamag.com/wp-content/uploads/2025/02/This-Govt-Funded-AI-Initiative-is-Indias-Real-Answer-to-DeepSeek.jpg,"Chinese AI startupDeepSeekhas reported a theoretical daily profit margin of545% for its inference services, despite limitations in monetisation and discounted pricing structures. The company shared these details in arecent GitHubpost, outlining the operational costs and revenue potential of its DeepSeek-V3 and R1 models. Based on DeepSeek-R1’s pricing model—charging $0.14 per million input tokens for cache hits, $0.55 per million for cache misses, and $2.19 per million output tokens—the theoretical revenue generated daily is $562,027. However, the company acknowledged that actual earnings were significantly lower due to lower pricing for DeepSeek-V3, free access to web and app services, and automatic nighttime discounts. “Our pricing strategy prioritises accessibility and long-term adoption over immediate revenue maximisation,” DeepSeek said. According to the company, DeepSeeks inference services run on NVIDIA H800 GPUs, with matrix multiplications and dispatch transmissions using the FP8 format, while core MLA computations and combine transmissions operate in BF16. The company scales its GPU usage based on demand, deploying all nodes during peak hours and reducing them at night to allocate resources for research and training. The GitHub post revealed that over a 24-hour period from February 27, 2025, to 12:00 PM on February 28, 2025, 12:00 PM, DeepSeek recorded peak node occupancy at 278, with an average of 226.75 nodes in operation. With each node containing eight H800 GPUs and an estimated leasing cost of $2 per GPU per hour, the total daily expenditure reached$87,072. The above revelation could affect the US stock market. The launch ofDeepSeek’s latest model, R1, which the company claims was trained on a $6 million budget, triggered a sharp market reaction. NVIDIA’s stock tumbled 17%, wiping out nearly $600 billion in value, driven by concerns over the model’s efficiency. However, NVIDIA chief Jensen Huang, during the recent earnings call, said the company’sinference demand is accelerating, fuelled by test-time scaling and new reasoning models. “Models like OpenAI’s, Grok 3, and DeepSeek R1 are reasoning models that apply inference-time scaling. Reasoning models can consume 100 times more compute,” he said. “DeepSeek-R1 has ignited global enthusiasm. It’s an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model,” Huang said. According to a recent report, DeepSeek plans to release its next reasoning model, the DeepSeek R2, ‘as early as possible.’ The company initially planned to release it in early May but is now considering an earlier timeline. The model is said to produce ‘better coding’ and reason in languages beyond English.  📣 Want to advertise in AIM?Book here “People went abroad because of better projects and financial incentives, but if you are paid well here, you could visit those countries while still building in India,” said Yashas Karanam,","Chinese AI startupDeepSeekhas reported a theoretical daily profit margin of545% for its inference services, despite limitations in monetisation and discounted pricing structures. The company shared these details in arecent GitHubpost, outlining the operational costs and revenue potential of its DeepSeek-V3 and R1 models. Based on DeepSeek-R1’s pricing model—charging $0.14 per million input tokens for cache hits, $0.55 per million for cache misses, and $2.19 per million output tokens—the theoretical revenue generated daily is $562,027. However, the company acknowledged that actual earnings were significantly lower due to lower pricing for DeepSeek-V3, free access to web and app services, and automatic nighttime discounts. “Our pricing strategy prioritises accessibility and long-term adoption over immediate revenue maximisation,” DeepSeek said. According to the company, DeepSeeks inference services run on NVIDIA H800 GPUs, with matrix multiplications and dispatch transmissions using the FP8 format, while core MLA computations and combine transmissions operate in BF16. The company scales its GPU usage based on demand, deploying all nodes during peak hours and reducing them at night to allocate resources for research and training. The GitHub post revealed that over a 24-hour period from February 27, 2025, to 12:00 PM on February 28, 2025, 12:00 PM, DeepSeek recorded peak node occupancy at 278, with an average of 226.75 nodes in operation. With each node containing eight H800 GPUs and an estimated leasing cost of $2 per GPU per hour, the total daily expenditure reached$87,072. The above revelation could affect the US stock market. The launch ofDeepSeek’s latest model, R1, which the company claims was trained on a $6 million budget, triggered a sharp market reaction. NVIDIA’s stock tumbled 17%, wiping out nearly $600 billion in value, driven by concerns over the model’s efficiency. However, NVIDIA chief Jensen Huang, during the recent earnings call, said the company’sinference demand is accelerating, fuelled by test-time scaling and new reasoning models. “Models like OpenAI’s, Grok 3, and DeepSeek R1 are reasoning models that apply inference-time scaling. Reasoning models can consume 100 times more compute,” he said. “DeepSeek-R1 has ignited global enthusiasm. It’s an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model,” Huang said. According to a recent report, DeepSeek plans to release its next reasoning model, the DeepSeek R2, ‘as early as possible.’ The company initially planned to release it in early May but is now considering an earlier timeline. The model is said to produce ‘better coding’ and reason in languages beyond English.  📣 Want to advertise in AIM?Book here “People went abroad because of better projects and financial incentives, but if you are paid well here, you could visit those countries while still building in India,” said Yashas Karanam,",DeepSeek Reports 545% Daily Profit Despite Free AI Services,"

Key Points:
",AI,"Chinese AI startup DeepSeek has reported a theoretical daily profit margin of 545% for its inference services, despite limitations in monetisation and discounted pricing structures. The company shared these details in a recent GitHub post, outlining ... [2485 chars]"
"James Bond nightclubs, vodka, aftershave: 007 writer on the spy’s future with Amazon",https://www.theguardian.com/film/2025/mar/01/james-bond-william-boyd-spy-amazon-franchise-ai,GNews,2025-03-01T13:27:14Z,The Guardian,https://i.guim.co.uk/img/media/fa6c452d974ca838a50d3487b3ec5ad418b04eff/0_71_3000_1800/master/3000.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdG8tZGVmYXVsdC5wbmc&enable=upscale&s=2a04ffe24ef611556e7fa0e2504fc65b,"Daniel Craig as James Bond in Skyfall. Photograph: François Duhamel/AP View image in fullscreen Daniel Craig as James Bond in Skyfall. Photograph: François Duhamel/AP The Observer James Bond James Bond nightclubs, vodka, aftershave: 007 writer on the spy’s future with Amazon As the Bond franchise heads to the online giant, thriller author William Boyd foresees a slew of spin-offs and says AI is not a threat to human screenwriters Vanessa Thorpe Sat 1 Mar 2025 13.27 GMT Last modified on Sat 1 Mar 2025 17.01 GMT Share A mong the people best placed to predict how any James Bond of the future might look is a British writer with a strong feel for spies and for spying. William Boyd has been drawn back to the terrain repeatedly in his books. What’s more, he wrote his own official Bond novel, Solo , in 2013 . Now Amazon has picked up the rights to the character, Boyd foresees a succession of 007 spin-off products and entertainments. Perhaps even be new AI-generated novels? “Certainly wait for Bond aftershave – and for the theme park and the dinner jackets,” he said. “The new owners will have to commodify everything about their billion-dollar purchase, so there will be nightclubs and vodkas.” But the novelist and screenwriter does not regard this as fresh treachery. The true defilers of the authentic Bond have been at work for decades, making films that bear little relation to creator Ian Fleming ’s original. View image in fullscreen A scene from Spectre … ‘the films have nothing to do with the novels,’ says William Boyd. Photograph: United Artists/Allstar “It is too late. The great schism is that the films have nothing to do with the novels,” said Boyd. “The films are preposterous action movies that have to sell globally and so cannot have too much dialogue.” Fleming’s novels from the 1950s were already old-fashioned by the time the first film, Dr No , came out in 1962 , Boyd argues. “Since then the film have got further and further away from the stories and the gulf is now so wide, it doesn’t really matter.” Anyone who wonders how Fleming’s Bond behaved should go to his own book, he suggests. “I took Fleming’s character and then ran with it, so if anyone is interested, all the information is in Solo ; from Bond’s book-lined study to his favourite marmalade.” skip past newsletter promotion Sign up to Observed Free weekly newsletter Analysis and opinion on the week's news and culture brought to you by the best Observer writers Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion View image in fullscreen The writer William Boyd, who published a Bond novel called Solo in 2013. Photograph: Suki Dhanda/The Observer Boyd’s research took him back to detail dropped from the films, but often borrowed from Fleming’s own life. “The amazing thing is that this not-very-good writer created a figure as mythic as Sherlock Holmes or Hercule Poirot. His novels remain the Bond mother lode, with all their imperfections and their political incorrectness.” Is franchise writing, then, akin to the process of creating fiction with AI, by “scraping” past data? After all, Boyd scoured Fleming’s 14 books, “pen in hand”, before he wrote Solo . “It is true that everything in that novel that seems unusual is actually sourced from Fleming – for example, that Bond was a nervous flyer. He gave Bond all his own foibles.” But AI, Boyd hopes, could only work to generate strict very formulaic fiction. “It might work for romcoms, but it was absolutely useless when I tried it a year or so ago. I needed someone to fake their own death for a screenplay I was writing and I asked it how. AI will become more efficient, of course. I think, though, you will always only be able to get something ‘quite good’. Serious literature is incredibly idiosyncratic and AI will find that hard to match. The only straw to clutch at is the sheer complexity and randomness of human individuality.” View image in fullscreen Boyd’s latest novel, Gabriel’s Moon – the first part of a cold war trilogy – is released in April Photograph: no credit On Boyd’s mind this weekend is not just espionage, but literary franchises and sequels in general, as his new 10-part mystery, The Jura Affair , featuring his regular fictional heroine, Bethany Mellmoth, comes to BBC Radio 4 this Monday. Meanwhile Gabriel’s Moon , the first of his new trilogy of cold war novels, is set for publication in early April. The radio story, read by Ruth Everett, will see Mellmoth, now about 26, turn detective after she picks up a packet left on a tube train that seems to contain a valuable first edition of George Orwell’s book 1984 . “She is like a young Miss Marple in a way, because she becomes a sleuth,” said Boyd, admitting he is open to offers to make Mellmoth into a refreshing Bond-style franchise. “Bethany is a character I’ve been writing about for years now. She is ambitious, but she can’t make a decision about her life. We once made a short film about her starring Lucy Boynton and Jack Lowden , before they were both so famous. We’d hoped it would spin off into a series.” Boyd’s interest in Jura was piqued by the time Orwell spent on the Scottish island as he wrote his dystopian novel. “It was the strange fact of Orwell going to live in this unbelievably remote house on the island, just with his adopted son and his sister helping out. There was no water or electricity, so this public intellectual, who was very ill, was living this peasant life as he wrote 1984 .” In The Jura Affair Mellmoth “stumbles across an elaborate scam”, Boyd explains, and even wonders if Orwell is haunting her. The shape of a thriller like this one, but more particularly of a spy novel, is a distillation of the basic function of a novel, the author believes. “I think a strong narrative is fundamental to writing fiction. If you decline compelling story, then you had better be a terribly good to keep entertaining the reader.” The new cold war trilogy is Boyd’s first attempt at full-length sequels, a bigger kind of literary franchise. “I have nearly finished book number two. They are about a travel writer, Gabriel Dax, who is inveigled into the world of espionage.” In previous novels, such as Any Human Heart or Restless , spies are often present. “I just feel it chimes with us,” Boyd explains. “We get the themes of betrayal, duplicity, or changed identity. After all, we have all been lied to and all lied.” Explore more on these topics James Bond The Observer William Boyd Ian Fleming Amazon Fiction Thrillers Film industry news Share Reuse this content","Daniel Craig as James Bond in Skyfall. Photograph: François Duhamel/AP View image in fullscreen Daniel Craig as James Bond in Skyfall. Photograph: François Duhamel/AP The Observer James Bond James Bond nightclubs, vodka, aftershave: 007 writer on the spy’s future with Amazon As the Bond franchise heads to the online giant, thriller author William Boyd foresees a slew of spin-offs and says AI is not a threat to human screenwriters Vanessa Thorpe Sat 1 Mar 2025 13.27 GMT Last modified on Sat 1 Mar 2025 17.01 GMT Share A mong the people best placed to predict how any James Bond of the future might look is a British writer with a strong feel for spies and for spying. William Boyd has been drawn back to the terrain repeatedly in his books. What’s more, he wrote his own official Bond novel, Solo , in 2013 . Now Amazon has picked up the rights to the character, Boyd foresees a succession of 007 spin-off products and entertainments. Perhaps even be new AI-generated novels? “Certainly wait for Bond aftershave – and for the theme park and the dinner jackets,” he said. “The new owners will have to commodify everything about their billion-dollar purchase, so there will be nightclubs and vodkas.” But the novelist and screenwriter does not regard this as fresh treachery. The true defilers of the authentic Bond have been at work for decades, making films that bear little relation to creator Ian Fleming ’s original. View image in fullscreen A scene from Spectre … ‘the films have nothing to do with the novels,’ says William Boyd. Photograph: United Artists/Allstar “It is too late. The great schism is that the films have nothing to do with the novels,” said Boyd. “The films are preposterous action movies that have to sell globally and so cannot have too much dialogue.” Fleming’s novels from the 1950s were already old-fashioned by the time the first film, Dr No , came out in 1962 , Boyd argues. “Since then the film have got further and further away from the stories and the gulf is now so wide, it doesn’t really matter.” Anyone who wonders how Fleming’s Bond behaved should go to his own book, he suggests. “I took Fleming’s character and then ran with it, so if anyone is interested, all the information is in Solo ; from Bond’s book-lined study to his favourite marmalade.” skip past newsletter promotion Sign up to Observed Free weekly newsletter Analysis and opinion on the week's news and culture brought to you by the best Observer writers Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion View image in fullscreen The writer William Boyd, who published a Bond novel called Solo in 2013. Photograph: Suki Dhanda/The Observer Boyd’s research took him back to detail dropped from the films, but often borrowed from Fleming’s own life. “The amazing thing is that this not-very-good writer created a figure as mythic as Sherlock Holmes or Hercule Poirot. His novels remain the Bond mother lode, with all their imperfections and their political incorrectness.” Is franchise writing, then, akin to the process of creating fiction with AI, by “scraping” past data? After all, Boyd scoured Fleming’s 14 books, “pen in hand”, before he wrote Solo . “It is true that everything in that novel that seems unusual is actually sourced from Fleming – for example, that Bond was a nervous flyer. He gave Bond all his own foibles.” But AI, Boyd hopes, could only work to generate strict very formulaic fiction. “It might work for romcoms, but it was absolutely useless when I tried it a year or so ago. I needed someone to fake their own death for a screenplay I was writing and I asked it how. AI will become more efficient, of course. I think, though, you will always only be able to get something ‘quite good’. Serious literature is incredibly idiosyncratic and AI will find that hard to match. The only straw to clutch at is the sheer complexity and randomness of human individuality.” View image in fullscreen Boyd’s latest novel, Gabriel’s Moon – the first part of a cold war trilogy – is released in April Photograph: no credit On Boyd’s mind this weekend is not just espionage, but literary franchises and sequels in general, as his new 10-part mystery, The Jura Affair , featuring his regular fictional heroine, Bethany Mellmoth, comes to BBC Radio 4 this Monday. Meanwhile Gabriel’s Moon , the first of his new trilogy of cold war novels, is set for publication in early April. The radio story, read by Ruth Everett, will see Mellmoth, now about 26, turn detective after she picks up a packet left on a tube train that seems to contain a valuable first edition of George Orwell’s book 1984 . “She is like a young Miss Marple in a way, because she becomes a sleuth,” said Boyd, admitting he is open to offers to make Mellmoth into a refreshing Bond-style franchise. “Bethany is a character I’ve been writing about for years now. She is ambitious, but she can’t make a decision about her life. We once made a short film about her starring Lucy Boynton and Jack Lowden , before they were both so famous. We’d hoped it would spin off into a series.” Boyd’s interest in Jura was piqued by the time Orwell spent on the Scottish island as he wrote his dystopian novel. “It was the strange fact of Orwell going to live in this unbelievably remote house on the island, just with his adopted son and his sister helping out. There was no water or electricity, so this public intellectual, who was very ill, was living this peasant life as he wrote 1984 .” In The Jura Affair Mellmoth “stumbles across an elaborate scam”, Boyd explains, and even wonders if Orwell is haunting her. The shape of a thriller like this one, but more particularly of a spy novel, is a distillation of the basic function of a novel, the author believes. “I think a strong narrative is fundamental to writing fiction. If you decline compelling story, then you had better be a terribly good to keep entertaining the reader.” The new cold war trilogy is Boyd’s first attempt at full-length sequels, a bigger kind of literary franchise. “I have nearly finished book number two. They are about a travel writer, Gabriel Dax, who is inveigled into the world of espionage.” In previous novels, such as Any Human Heart or Restless , spies are often present. “I just feel it chimes with us,” Boyd explains. “We get the themes of betrayal, duplicity, or changed identity. After all, we have all been lied to and all lied.” Explore more on these topics James Bond The Observer William Boyd Ian Fleming Amazon Fiction Thrillers Film industry news Share Reuse this content","James Bond nightclubs, vodka, aftershave: 007 writer on the spy’s future with Amazon","

Key Points:
",AI,"Among the people best placed to predict how any James Bond of the future might look is a British writer with a strong feel for spies and for spying. William Boyd has been drawn back to the terrain repeatedly in his books. What’s more, he wrote his ow... [6005 chars]"
Microsoft invests in cloud data firm Veeam Software to build AI products,https://finance.yahoo.com/news/microsoft-invests-cloud-data-firm-140759727.html,GNews,2025-02-25T14:07:59Z,Yahoo Finance,https://media.zenfs.com/en/reuters-finance.com/32d19a9717282d4fed432a1089dd824f,"Unlock stock picks and a broker-level newsfeed that powers Wall Street. Upgrade Now Microsoft invests in cloud data firm Veeam Software to build AI products Illustration shows Microsoft logo · Reuters Reuters Tue, Feb 25, 2025, 9:07 AM 1 min read In This Article: MSFT +1.14% (Reuters) - Microsoft has made an undisclosed equity investment in Veeam Software as part of an expanded partnership to build artificial intelligence products, the cloud data company said on Tuesday. Veeam's software is designed to help customers quickly recover their data after cybersecurity incidents, ransomware attacks or accidental data loss. Its core product supports immutable backups to prevent ransomware from modifying or deleting data, ensuring that clean copies remain available for recovery even if hackers encrypt files. Microsoft had invested in cybersecurity firm Rubrik in 2021, which also offers data backup and recovery solutions. Veeam said it would focus on research and development investments and design collaboration, among others, with the support of Microsoft. It will also integrate Microsoft's AI services into its products. U.S. private equity firm Insight Partners, which is the largest shareholder in Veeam, sold a $2 billion stake in the company in a secondary sale valuing the firm at $15 billion, it said in December last year. Veeam was acquired by Insight Partners for about $5 billion in 2020. Founded in 2006, Veeam has more than 550,000 customers globally including corporations such as Deloitte and Canon, according to its website. (Reporting by Jaspreet Singh in Bengaluru; Editing by Sahal Muhammed) View Comments Terms and Privacy Policy Privacy Dashboard More Info Recommended Stories","Unlock stock picks and a broker-level newsfeed that powers Wall Street. Upgrade Now Microsoft invests in cloud data firm Veeam Software to build AI products Illustration shows Microsoft logo · Reuters Reuters Tue, Feb 25, 2025, 9:07 AM 1 min read In This Article: MSFT +1.14% (Reuters) - Microsoft has made an undisclosed equity investment in Veeam Software as part of an expanded partnership to build artificial intelligence products, the cloud data company said on Tuesday. Veeam's software is designed to help customers quickly recover their data after cybersecurity incidents, ransomware attacks or accidental data loss. Its core product supports immutable backups to prevent ransomware from modifying or deleting data, ensuring that clean copies remain available for recovery even if hackers encrypt files. Microsoft had invested in cybersecurity firm Rubrik in 2021, which also offers data backup and recovery solutions. Veeam said it would focus on research and development investments and design collaboration, among others, with the support of Microsoft. It will also integrate Microsoft's AI services into its products. U.S. private equity firm Insight Partners, which is the largest shareholder in Veeam, sold a $2 billion stake in the company in a secondary sale valuing the firm at $15 billion, it said in December last year. Veeam was acquired by Insight Partners for about $5 billion in 2020. Founded in 2006, Veeam has more than 550,000 customers globally including corporations such as Deloitte and Canon, according to its website. (Reporting by Jaspreet Singh in Bengaluru; Editing by Sahal Muhammed) View Comments Terms and Privacy Policy Privacy Dashboard More Info Recommended Stories",Microsoft invests in cloud data firm Veeam Software to build AI products,"

Key Points:
",Software Development,"(Reuters) - Microsoft has made an undisclosed equity investment in Veeam Software as part of an expanded partnership to build artificial intelligence products, the cloud data company said on Tuesday.
Veeam's software is designed to help customers qui... [1092 chars]"
The early days of Linux (2023),https://lwn.net/Articles/928581/,Hacker News,2025-03-01T19:18:21,Hacker News,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offeringa free one-month trial subscription(no credit card required) to get you started. April 12, 2023 This article was contributed by Lars Wirzenius My name is Lars Wirzenius, and I was there when Linux started.  Linux
is now a global success, but its beginnings were rather more humble.
These are my memories of the earliest days of Linux, its creation, and the
start of its path to where it is today. I
started mycomputer science studies at the University
of Helsinkiin the fall of 1988, and met Linus Torvalds, who was the
other new Swedish speaking student in computer science that year. Toward
the end of that first year, we had gotten access to a Unix server, and I
accidentally foundUsenet, the discussion 
system, by mistypingrmasrn, the Usenet reader. I told
Linus about it and we spent way too much time exploring this. After the first year, we both went away to do the mandatory military
service, though in different places. We returned to our university
studies in the fall of 1990, and both took the course on C and Unix
programming, which included a fair bit of theory of the Unix kernel
architecture as well. This led to us reading about other operating
system kernels, such asQNXandPlan 9. Linus
and I discussed with some enthusiasm how an operating system
should be built correctly. We had all the overconfidence of
20-year-old second-year university students. Everyone is better off
that this wasn't recorded for posterity. In January 1991, Linus bought his firstPCfrom a local
shop 
that assembled computers from parts. The PC had a 386 CPU, which was
relatively 
fancy at that time, because
Linus wanted to explore multitasking. Also, since he came from aSinclair QLwith a 32-bit Motorola 68008 CPU, he wanted a 32-bit CPU, and did
not want to step down to a 16-bit one, so a 286 was not an option.
Linus's first PC had a whopping 4 megabytes of RAM and a hard drive. He got a copy of the game Prince of Persia, which occupied most
of his spare time for the next couple of months. He later also bought
a copy ofMINIX, because after
using Unix at the university, he wanted something like that at home as
well. After finishing the game, Linus started learning Intel assembly
language. One day he showed me a program that did multitasking. One
task or thread would write a stream of the letter ""A"" on the screen, the
other ""B""; the context switches were visually obvious when the stream
of As became Bs. This was the first version of what would later become
known as the Linux kernel. Linus would later expand the program, and write most of it in C.
During this time, late spring of 1991, I wrote an implementation of the Csprintf()function
for him, as he hadn't yet learned how to write functions with variable
argument lists. I wanted to spare him the pain of having a different
function for every type of value to write out. The core of this code is
still in the kernel,assnprintf(). As time went on, Linus made his fledgling kernel better and kept
implementing new things. After a while, he had drivers for the keyboard and
the serial port, emulation ofVT100terminal escape sequences
for the screen, and could use it to dial via a modem to the university to
read Usenet from home. Science fiction!
One day, Linus accidentally attempted to use his hard drive to dial the
university, resulting in his master boot sector starting with""ATDT""and the
university modem-pool phone number. After recovering from this, he
implemented file permissions in his kernel. In August 1991, Linus mentioned his new kernel inpublic
for the first time, in thecomp.os.minixnewsgroup. This
included the phrase ""I'm doing a (free) operating system (just a hobby,
won't be big and professional like gnu)"". Such humility.
The system was initially called Freax. A few weeks later, 
Linus asked Ari Lemmke, one of
the administrators offtp.funet.fi, to do an upload of the first
tar archive. Ari chose the name Linux.  The initial
version still contains the original name embedded inone of the
source files. During this time, people were interested in trying out this new
thing, so Linus needed to provide an installation method and
instructions. Since he only had one PC, he came to visit to
install it on mine. Since his computer had been used to develop Linux,
which had simply
grown on top of his Minix installation, it had never actually been
installed before. Thus, mine was the first PC
where Linux was ever installed. While this was happening, I was taking
a nap, and I recommend this method of installing Linux: napping, while
Linus does the hard work. The first releases of Linux used a license that forbade commercial
use. Some of the early contributors suggested a change to a free-software
license. In the fall of 1991, Richard Stallman visited 
Finland and I took Linus to a talk given by Stallman. This, the
pressure from contributors, and my nagging eventually convinced Linus
to choose the GNU GPL license instead, in early 1992. Over the Christmas break, Linus implemented virtual memory in Linux.
This made Linux a much more practical operating system on cheap
machines with little memory. The year 1992 started with the famousdebate with Andrew
Tanenbaum, who is a university professor and the author of MINIX. He had
some opinions about Linux and its architecture. Linus had opinions on
MINIX. The debate has been described as a flame war, but was actually
rather civil in hindsight. More importantly for the future success of Linux was that the X11
system was ported to it, making 1992 the year of the Linux desktop. I had chosen to contribute on the community side, rather than to the
kernel directly, and helped answer questions, write documentation, and
such. I also ran a short-lived newsletter about Linux, which is mainly
interesting for publishing thefirst ever interview with
Linus. The newsletter was effectively replaced by thecomp.os.linux.announcenewsgroup. The first Linux distribution was also started in 1992:Softlanding
Linux Systemor SLS. The next year, SLS morphed into Slackware, which
inspired Ian Murdock to start Debian in 1993, in order to explore a
more community-based development structure. A few other distributions would
follow in the 
years to come. In 1993, both Linus and I got hired as teaching assistants at the
university. We got to share an office. That room had a PC, which Linus
took over, and used for Linux development. I was happy with a DEC
terminal for Usenet access. One day, Linus was bored and the PC at work felt slow. He spent the
day rewriting the Linux kernel command-line parser in assembly
language, for speed. (That was, of course, quite pointless, and the
parser would later be rewritten again in C, for portability. Its speed
does not matter.) A couple of years later, he spent days playing
Quake, ostensibly to stress-test kernel memory management, although
that was with a newer PC. Much fun was had in that room, and there were no
pranks 
whatsoever. None at all. At some point, Linux gained support for Ethernet and TCP/IP. That meant
one could read Usenet without having to use a modem. Alas, early Linux
networking code was occasionally a little rough, having been written
from scratch. At one point, Linux would send some broken packets that
took down all of the Sun machines on the network. As it was difficult to get
the Sun kernel fixed, Linux was banned from the university network
until its bug was fixed. Not having Usenet access from one's desk is a
great motivator. In the spring of 1994 we felt that Linux was done. Finished. Nothing
more to add. One could use Linux to compile itself, to read Usenet, and
run many copies of thexeyesprogram at once.  We
decided to release version 1.0 and arranged arelease event. The
Finnish computer press was invited, and a TV station even sent a crew. Most
of the event consisted of ceremonially compiling Linux 1.0 in the
background, while Linus and others spoke about what Linux was and what it
was good for. Linus explained that commercial Unix for a PC was so
expensive that it was easier to write your own. In 1995 Linus and I did a software engineering course at the university,
which mostly consisted of a large practical project. This was built on top
of Linux, of course. I insisted that a version-control system be used. I
had witnessed students in earlier courses do the shouting kind of version
control: the students shared a source tree over NFS and shouted ""I'm
editing this file"" when they were changing something.  This did not seem
like an effective method to me, so I insisted onCVS,
which I'd just learned about. This experience is why Linus dislikes CVS and
for years refused to use any version control beyond uploading tar balls to
FTP sites. That year was also when Linux was first ported to a new architecture
by Linus. He'd been given a DEC Alpha machine. I would later get the
machine to use as a terminal for reading Usenet. Other people ported
Linux to other architectures, but that did not result in me getting any
more machines to read Usenet on. In 1997 Linus graduated and moved to the US to take a job atTransmeta. I took a
job at a different university in the Helsinki area. In the following years, many things happened. It turned out that there
were still a few missing features from Linux, so people worked on
those. The term ""open source"" was coined and IBM invested a ton of money in
Linux development. Netscape published a version of its web browser as
open source. Skipping a few details and many years, open source basically
took over the world. LWN was started and covered much of this history on a
week-by-week basis. In 1991, Linus wrote that Linux ""won't be big and professional
like gnu"". 
In 2023. Linux is running on every continent, on every ocean, on billions
of devices, in orbit, and on Mars. Not bad for what started as two threads,
writing streams of As and Bs on the screen.  The early days of LinuxPosted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]Thank you, Lars!  Wonderful article!The early days of LinuxPosted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses)Thank you Lars. The story is very interesting and inspiring. It is funny that Linux started as hobby project. :)The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing.The early days of LinuxPosted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link]Typical Lars article - self deprecating and quiet. Thanks for the Linux Documentation Project - I've a huge book next door which is basically all the HOWTOs from about 1994 which got me using Linux properly using first Slackware, then Linux-FT and finally Debian.You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.The early days of LinuxPosted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]Are you sure he was playing Quake and not Doom? Quake didn't come out until well after Linux was finished and wouldn't need memory management stress tested any more, but Doom got ported around the right time, I think.The early days of LinuxPosted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link]Heh... Yes, this was a nice read. But I'm going to add something. I wasn't involved in free software, programming or anything back then. Sure, I bought Dr Dobbs every month, but my major was Comparative and Historical Linguistics of the Sino-Tibetan languages.But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!The early days of LinuxPosted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link]> While this was happening, I was taking a nap, and I recommend this method of installing Linux: napping, while Linus does the hard work.I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history!The early days of LinuxPosted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses)Who has two thumbs and still misses Usenet?............The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)The early days of LinuxPosted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]Pleasure to know you Lars. You inspired me and many others!The early days of LinuxPosted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link]What a nice piece of history. Thanks Lars!Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!The early days of LinuxPosted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]Thank you Lars!The early days of LinuxPosted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]Priceless! :D Thank you for sharing!The early days of LinuxPosted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses)Very nice article. I first encountered Linux in 1994, where some nerds in my grad school had already set up a PC running Linux 1.1.3 (I think) to be the department's mail relay. And within a year or two there were Linux desktops around the theoretical physics group, with X11 and all. I remember the desktop viewport was larger than the screen resolution (640x480 in those days) and you had to pan around with a mouse, but it seemed better than Windows95 on the same machine.The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;)The early days of LinuxPosted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses)Great article, but one minor correction - the first Linux distribution was MCC Interim Linux. SLS didn't arrive until slightly later. I'd been using HJ Lu's boot/root disks, but switched to MCC when it was released.The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(The early days of LinuxPosted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]Wow. Simply one of the best pieces I've read about Linux in a few years. No jokes. Thanks for taking the time to share this.The early days of LinuxPosted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses)Awesome article that brings back lots of memories from that era. I discovered Linux in 1994 with SLS and kernel 1.0.4 or 1.0.5 and by then it looked like like a hack, I remember that it was enough to let ""ping"" run for a few tens of seconds to completely lose network connectivity on that NE2000 card. But everything was fun in it. When you booted from floppies to that white-on-black ""login:"" prompt, you really felt that the computer was waiting for you to be creative today.Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) )The early days of LinuxPosted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]Just gonna point out that Maddog helped arrange the Alpha, which he felt would be a huge boon both for Linux (cross architecture!) and DEC (running Linux!). This, after having Linus come speak at DEC in the US.The early days of LinuxPosted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]Please don't tell Linus that Prince of Persia apparently runs well on Wine:https://appdb.winehq.org/objectManager.php?sClass=applica..., lest Linux development get horribly delayed.The early days of LinuxPosted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]Lars, thanks for this article! Your name, Lars Wirzenius, was ringing a bell. I did not know about your role in the early days of Linux. Then I realized that I knew your name from the Linux Documentation Project. I hope that you still meet Linus from time to time.The early days of LinuxPosted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link]Dear Mr. Wirzenius,Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.The early days of LinuxPosted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link]I saved this article to read on a lazy Sunday afternoon, and indeed what a well written story. I got involved much later (being from a place where computers were rare and terribly expensive that early), and I remember fighting with Slackware <some version> and a Linux 1.2 kernel and X until, much later, I started understanding that my Trident card was not actually supported beyond VGA mode at that time. If I concentrate, I probably could remember the card number, that's how much I was fighting with all these new and wonderful things.And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated!The early days of LinuxPosted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link]That anecdote about Linux fubaring his master boot sector hits very close to home: back in 1993 I had a SunOS system at work and I would create floppies to take home to install Linux on my personal 486.  One day I got distracted and run dd to /dev/sd0 instead of /dev/fd0 and that was the end of that installation: time to reinstall SunOS from scratch and it took a day or two.I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).The early days of LinuxPosted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link]It's amazing how after all these years I still remember Lars's E-mail address — from USENET and the mailing list, I guess. It seems to be burned into my brain :-)Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]I got involved when I was working for Sun in late 1991. I had acquired a Sun 2, and was using it to pull news and email from a local university. When they switched UUCP protocols, I had to find another distro, as an upgrade to the latest Sun release was way outside my budget.I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!The early days of LinuxPosted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]great! thanks to linus and you.HN discussionPosted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]https://news.ycombinator.com/item?id=43225686 Posted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]  Posted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses) The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing. Posted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses) The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right. Posted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]  Posted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link] You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.  Posted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]  Posted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link] But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!  Posted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link] I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history! Thank you, Lars, for this wonderfully written piece of history!  Posted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses) The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-) Posted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses) I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it... That's about it... The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't. Posted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]  Posted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)  The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type. Posted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]   Posted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link] The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)  Posted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]  Posted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!    Posted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]  Posted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]  Posted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses) The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses) corbet is clearly a C programmer; his love of semicolons indicates that. SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses) SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :) Posted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link] FORTRAN - and I can tell it's warped my mind :-)Cheers,Wol Cheers,Wol  Posted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses) Since we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life"" SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Posted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses) SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it. Posted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]  Posted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link] Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* I'm mostly Ok now, though. *twitch*  Posted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]  Posted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]   Posted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses) The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :( Posted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(  Posted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]  Posted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses) Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!  The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) ) Posted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link] (... I'm also jealous that you could afford to risk a whole 8088 like that :) )   Posted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]  Posted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]  Posted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]  Posted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link] Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back. It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.  Posted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link] And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated! Thank you for the trip down the memory lane, much appreciated!  Posted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link] I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).  Posted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link] Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!  Posted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]  Posted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]  Copyright © 2023, Eklektix, Inc.Comments and public postings are copyrighted by their creators.Linux  is a registered trademark of Linus Torvalds","Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offeringa free one-month trial subscription(no credit card required) to get you started. April 12, 2023 This article was contributed by Lars Wirzenius My name is Lars Wirzenius, and I was there when Linux started.  Linux
is now a global success, but its beginnings were rather more humble.
These are my memories of the earliest days of Linux, its creation, and the
start of its path to where it is today. I
started mycomputer science studies at the University
of Helsinkiin the fall of 1988, and met Linus Torvalds, who was the
other new Swedish speaking student in computer science that year. Toward
the end of that first year, we had gotten access to a Unix server, and I
accidentally foundUsenet, the discussion 
system, by mistypingrmasrn, the Usenet reader. I told
Linus about it and we spent way too much time exploring this. After the first year, we both went away to do the mandatory military
service, though in different places. We returned to our university
studies in the fall of 1990, and both took the course on C and Unix
programming, which included a fair bit of theory of the Unix kernel
architecture as well. This led to us reading about other operating
system kernels, such asQNXandPlan 9. Linus
and I discussed with some enthusiasm how an operating system
should be built correctly. We had all the overconfidence of
20-year-old second-year university students. Everyone is better off
that this wasn't recorded for posterity. In January 1991, Linus bought his firstPCfrom a local
shop 
that assembled computers from parts. The PC had a 386 CPU, which was
relatively 
fancy at that time, because
Linus wanted to explore multitasking. Also, since he came from aSinclair QLwith a 32-bit Motorola 68008 CPU, he wanted a 32-bit CPU, and did
not want to step down to a 16-bit one, so a 286 was not an option.
Linus's first PC had a whopping 4 megabytes of RAM and a hard drive. He got a copy of the game Prince of Persia, which occupied most
of his spare time for the next couple of months. He later also bought
a copy ofMINIX, because after
using Unix at the university, he wanted something like that at home as
well. After finishing the game, Linus started learning Intel assembly
language. One day he showed me a program that did multitasking. One
task or thread would write a stream of the letter ""A"" on the screen, the
other ""B""; the context switches were visually obvious when the stream
of As became Bs. This was the first version of what would later become
known as the Linux kernel. Linus would later expand the program, and write most of it in C.
During this time, late spring of 1991, I wrote an implementation of the Csprintf()function
for him, as he hadn't yet learned how to write functions with variable
argument lists. I wanted to spare him the pain of having a different
function for every type of value to write out. The core of this code is
still in the kernel,assnprintf(). As time went on, Linus made his fledgling kernel better and kept
implementing new things. After a while, he had drivers for the keyboard and
the serial port, emulation ofVT100terminal escape sequences
for the screen, and could use it to dial via a modem to the university to
read Usenet from home. Science fiction!
One day, Linus accidentally attempted to use his hard drive to dial the
university, resulting in his master boot sector starting with""ATDT""and the
university modem-pool phone number. After recovering from this, he
implemented file permissions in his kernel. In August 1991, Linus mentioned his new kernel inpublic
for the first time, in thecomp.os.minixnewsgroup. This
included the phrase ""I'm doing a (free) operating system (just a hobby,
won't be big and professional like gnu)"". Such humility.
The system was initially called Freax. A few weeks later, 
Linus asked Ari Lemmke, one of
the administrators offtp.funet.fi, to do an upload of the first
tar archive. Ari chose the name Linux.  The initial
version still contains the original name embedded inone of the
source files. During this time, people were interested in trying out this new
thing, so Linus needed to provide an installation method and
instructions. Since he only had one PC, he came to visit to
install it on mine. Since his computer had been used to develop Linux,
which had simply
grown on top of his Minix installation, it had never actually been
installed before. Thus, mine was the first PC
where Linux was ever installed. While this was happening, I was taking
a nap, and I recommend this method of installing Linux: napping, while
Linus does the hard work. The first releases of Linux used a license that forbade commercial
use. Some of the early contributors suggested a change to a free-software
license. In the fall of 1991, Richard Stallman visited 
Finland and I took Linus to a talk given by Stallman. This, the
pressure from contributors, and my nagging eventually convinced Linus
to choose the GNU GPL license instead, in early 1992. Over the Christmas break, Linus implemented virtual memory in Linux.
This made Linux a much more practical operating system on cheap
machines with little memory. The year 1992 started with the famousdebate with Andrew
Tanenbaum, who is a university professor and the author of MINIX. He had
some opinions about Linux and its architecture. Linus had opinions on
MINIX. The debate has been described as a flame war, but was actually
rather civil in hindsight. More importantly for the future success of Linux was that the X11
system was ported to it, making 1992 the year of the Linux desktop. I had chosen to contribute on the community side, rather than to the
kernel directly, and helped answer questions, write documentation, and
such. I also ran a short-lived newsletter about Linux, which is mainly
interesting for publishing thefirst ever interview with
Linus. The newsletter was effectively replaced by thecomp.os.linux.announcenewsgroup. The first Linux distribution was also started in 1992:Softlanding
Linux Systemor SLS. The next year, SLS morphed into Slackware, which
inspired Ian Murdock to start Debian in 1993, in order to explore a
more community-based development structure. A few other distributions would
follow in the 
years to come. In 1993, both Linus and I got hired as teaching assistants at the
university. We got to share an office. That room had a PC, which Linus
took over, and used for Linux development. I was happy with a DEC
terminal for Usenet access. One day, Linus was bored and the PC at work felt slow. He spent the
day rewriting the Linux kernel command-line parser in assembly
language, for speed. (That was, of course, quite pointless, and the
parser would later be rewritten again in C, for portability. Its speed
does not matter.) A couple of years later, he spent days playing
Quake, ostensibly to stress-test kernel memory management, although
that was with a newer PC. Much fun was had in that room, and there were no
pranks 
whatsoever. None at all. At some point, Linux gained support for Ethernet and TCP/IP. That meant
one could read Usenet without having to use a modem. Alas, early Linux
networking code was occasionally a little rough, having been written
from scratch. At one point, Linux would send some broken packets that
took down all of the Sun machines on the network. As it was difficult to get
the Sun kernel fixed, Linux was banned from the university network
until its bug was fixed. Not having Usenet access from one's desk is a
great motivator. In the spring of 1994 we felt that Linux was done. Finished. Nothing
more to add. One could use Linux to compile itself, to read Usenet, and
run many copies of thexeyesprogram at once.  We
decided to release version 1.0 and arranged arelease event. The
Finnish computer press was invited, and a TV station even sent a crew. Most
of the event consisted of ceremonially compiling Linux 1.0 in the
background, while Linus and others spoke about what Linux was and what it
was good for. Linus explained that commercial Unix for a PC was so
expensive that it was easier to write your own. In 1995 Linus and I did a software engineering course at the university,
which mostly consisted of a large practical project. This was built on top
of Linux, of course. I insisted that a version-control system be used. I
had witnessed students in earlier courses do the shouting kind of version
control: the students shared a source tree over NFS and shouted ""I'm
editing this file"" when they were changing something.  This did not seem
like an effective method to me, so I insisted onCVS,
which I'd just learned about. This experience is why Linus dislikes CVS and
for years refused to use any version control beyond uploading tar balls to
FTP sites. That year was also when Linux was first ported to a new architecture
by Linus. He'd been given a DEC Alpha machine. I would later get the
machine to use as a terminal for reading Usenet. Other people ported
Linux to other architectures, but that did not result in me getting any
more machines to read Usenet on. In 1997 Linus graduated and moved to the US to take a job atTransmeta. I took a
job at a different university in the Helsinki area. In the following years, many things happened. It turned out that there
were still a few missing features from Linux, so people worked on
those. The term ""open source"" was coined and IBM invested a ton of money in
Linux development. Netscape published a version of its web browser as
open source. Skipping a few details and many years, open source basically
took over the world. LWN was started and covered much of this history on a
week-by-week basis. In 1991, Linus wrote that Linux ""won't be big and professional
like gnu"". 
In 2023. Linux is running on every continent, on every ocean, on billions
of devices, in orbit, and on Mars. Not bad for what started as two threads,
writing streams of As and Bs on the screen.  The early days of LinuxPosted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]Thank you, Lars!  Wonderful article!The early days of LinuxPosted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses)Thank you Lars. The story is very interesting and inspiring. It is funny that Linux started as hobby project. :)The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing.The early days of LinuxPosted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link]Typical Lars article - self deprecating and quiet. Thanks for the Linux Documentation Project - I've a huge book next door which is basically all the HOWTOs from about 1994 which got me using Linux properly using first Slackware, then Linux-FT and finally Debian.You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.The early days of LinuxPosted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]Are you sure he was playing Quake and not Doom? Quake didn't come out until well after Linux was finished and wouldn't need memory management stress tested any more, but Doom got ported around the right time, I think.The early days of LinuxPosted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link]Heh... Yes, this was a nice read. But I'm going to add something. I wasn't involved in free software, programming or anything back then. Sure, I bought Dr Dobbs every month, but my major was Comparative and Historical Linguistics of the Sino-Tibetan languages.But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!The early days of LinuxPosted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link]> While this was happening, I was taking a nap, and I recommend this method of installing Linux: napping, while Linus does the hard work.I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history!The early days of LinuxPosted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses)Who has two thumbs and still misses Usenet?............The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)The early days of LinuxPosted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]Pleasure to know you Lars. You inspired me and many others!The early days of LinuxPosted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link]What a nice piece of history. Thanks Lars!Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!The early days of LinuxPosted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]Thank you Lars!The early days of LinuxPosted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]Priceless! :D Thank you for sharing!The early days of LinuxPosted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses)Very nice article. I first encountered Linux in 1994, where some nerds in my grad school had already set up a PC running Linux 1.1.3 (I think) to be the department's mail relay. And within a year or two there were Linux desktops around the theoretical physics group, with X11 and all. I remember the desktop viewport was larger than the screen resolution (640x480 in those days) and you had to pan around with a mouse, but it seemed better than Windows95 on the same machine.The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;)The early days of LinuxPosted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses)Great article, but one minor correction - the first Linux distribution was MCC Interim Linux. SLS didn't arrive until slightly later. I'd been using HJ Lu's boot/root disks, but switched to MCC when it was released.The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(The early days of LinuxPosted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]Wow. Simply one of the best pieces I've read about Linux in a few years. No jokes. Thanks for taking the time to share this.The early days of LinuxPosted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses)Awesome article that brings back lots of memories from that era. I discovered Linux in 1994 with SLS and kernel 1.0.4 or 1.0.5 and by then it looked like like a hack, I remember that it was enough to let ""ping"" run for a few tens of seconds to completely lose network connectivity on that NE2000 card. But everything was fun in it. When you booted from floppies to that white-on-black ""login:"" prompt, you really felt that the computer was waiting for you to be creative today.Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) )The early days of LinuxPosted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]Just gonna point out that Maddog helped arrange the Alpha, which he felt would be a huge boon both for Linux (cross architecture!) and DEC (running Linux!). This, after having Linus come speak at DEC in the US.The early days of LinuxPosted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]Please don't tell Linus that Prince of Persia apparently runs well on Wine:https://appdb.winehq.org/objectManager.php?sClass=applica..., lest Linux development get horribly delayed.The early days of LinuxPosted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]Lars, thanks for this article! Your name, Lars Wirzenius, was ringing a bell. I did not know about your role in the early days of Linux. Then I realized that I knew your name from the Linux Documentation Project. I hope that you still meet Linus from time to time.The early days of LinuxPosted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link]Dear Mr. Wirzenius,Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.The early days of LinuxPosted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link]I saved this article to read on a lazy Sunday afternoon, and indeed what a well written story. I got involved much later (being from a place where computers were rare and terribly expensive that early), and I remember fighting with Slackware <some version> and a Linux 1.2 kernel and X until, much later, I started understanding that my Trident card was not actually supported beyond VGA mode at that time. If I concentrate, I probably could remember the card number, that's how much I was fighting with all these new and wonderful things.And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated!The early days of LinuxPosted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link]That anecdote about Linux fubaring his master boot sector hits very close to home: back in 1993 I had a SunOS system at work and I would create floppies to take home to install Linux on my personal 486.  One day I got distracted and run dd to /dev/sd0 instead of /dev/fd0 and that was the end of that installation: time to reinstall SunOS from scratch and it took a day or two.I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).The early days of LinuxPosted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link]It's amazing how after all these years I still remember Lars's E-mail address — from USENET and the mailing list, I guess. It seems to be burned into my brain :-)Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]I got involved when I was working for Sun in late 1991. I had acquired a Sun 2, and was using it to pull news and email from a local university. When they switched UUCP protocols, I had to find another distro, as an upgrade to the latest Sun release was way outside my budget.I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!The early days of LinuxPosted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]great! thanks to linus and you.HN discussionPosted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]https://news.ycombinator.com/item?id=43225686 Posted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]  Posted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses) The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing. Posted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses) The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right. Posted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]  Posted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link] You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.  Posted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]  Posted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link] But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!  Posted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link] I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history! Thank you, Lars, for this wonderfully written piece of history!  Posted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses) The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-) Posted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses) I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it... That's about it... The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't. Posted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]  Posted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)  The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type. Posted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]   Posted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link] The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)  Posted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]  Posted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!    Posted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]  Posted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]  Posted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses) The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses) corbet is clearly a C programmer; his love of semicolons indicates that. SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses) SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :) Posted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link] FORTRAN - and I can tell it's warped my mind :-)Cheers,Wol Cheers,Wol  Posted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses) Since we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life"" SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Posted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses) SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it. Posted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]  Posted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link] Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* I'm mostly Ok now, though. *twitch*  Posted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]  Posted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]   Posted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses) The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :( Posted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(  Posted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]  Posted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses) Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!  The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) ) Posted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link] (... I'm also jealous that you could afford to risk a whole 8088 like that :) )   Posted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]  Posted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]  Posted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]  Posted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link] Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back. It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.  Posted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link] And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated! Thank you for the trip down the memory lane, much appreciated!  Posted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link] I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).  Posted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link] Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!  Posted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]  Posted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]  Copyright © 2023, Eklektix, Inc.Comments and public postings are copyrighted by their creators.Linux  is a registered trademark of Linus Torvalds",The early days of Linux (2023),"

Key Points:
",Software Development,"Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offeringa free one-month trial subscription(no credit card required) to get you started. April 12, 2023 This article was contributed by Lars Wirzenius My name is Lars Wirzenius, and I was there when Linux started.  Linux
is now a global success, but its beginnings were rather more humble.
These are my memories of the earliest days of Linux, its creation, and the
start of its path to where it is today. I
started mycomputer science studies at the University
of Helsinkiin the fall of 1988, and met Linus Torvalds, who was the
other new Swedish speaking student in computer science that year. Toward
the end of that first year, we had gotten access to a Unix server, and I
accidentally foundUsenet, the discussion 
system, by mistypingrmasrn, the Usenet reader. I told
Linus about it and we spent way too much time exploring this. After the first year, we both went away to do the mandatory military
service, though in different places. We returned to our university
studies in the fall of 1990, and both took the course on C and Unix
programming, which included a fair bit of theory of the Unix kernel
architecture as well. This led to us reading about other operating
system kernels, such asQNXandPlan 9. Linus
and I discussed with some enthusiasm how an operating system
should be built correctly. We had all the overconfidence of
20-year-old second-year university students. Everyone is better off
that this wasn't recorded for posterity. In January 1991, Linus bought his firstPCfrom a local
shop 
that assembled computers from parts. The PC had a 386 CPU, which was
relatively 
fancy at that time, because
Linus wanted to explore multitasking. Also, since he came from aSinclair QLwith a 32-bit Motorola 68008 CPU, he wanted a 32-bit CPU, and did
not want to step down to a 16-bit one, so a 286 was not an option.
Linus's first PC had a whopping 4 megabytes of RAM and a hard drive. He got a copy of the game Prince of Persia, which occupied most
of his spare time for the next couple of months. He later also bought
a copy ofMINIX, because after
using Unix at the university, he wanted something like that at home as
well. After finishing the game, Linus started learning Intel assembly
language. One day he showed me a program that did multitasking. One
task or thread would write a stream of the letter ""A"" on the screen, the
other ""B""; the context switches were visually obvious when the stream
of As became Bs. This was the first version of what would later become
known as the Linux kernel. Linus would later expand the program, and write most of it in C.
During this time, late spring of 1991, I wrote an implementation of the Csprintf()function
for him, as he hadn't yet learned how to write functions with variable
argument lists. I wanted to spare him the pain of having a different
function for every type of value to write out. The core of this code is
still in the kernel,assnprintf(). As time went on, Linus made his fledgling kernel better and kept
implementing new things. After a while, he had drivers for the keyboard and
the serial port, emulation ofVT100terminal escape sequences
for the screen, and could use it to dial via a modem to the university to
read Usenet from home. Science fiction!
One day, Linus accidentally attempted to use his hard drive to dial the
university, resulting in his master boot sector starting with""ATDT""and the
university modem-pool phone number. After recovering from this, he
implemented file permissions in his kernel. In August 1991, Linus mentioned his new kernel inpublic
for the first time, in thecomp.os.minixnewsgroup. This
included the phrase ""I'm doing a (free) operating system (just a hobby,
won't be big and professional like gnu)"". Such humility.
The system was initially called Freax. A few weeks later, 
Linus asked Ari Lemmke, one of
the administrators offtp.funet.fi, to do an upload of the first
tar archive. Ari chose the name Linux.  The initial
version still contains the original name embedded inone of the
source files. During this time, people were interested in trying out this new
thing, so Linus needed to provide an installation method and
instructions. Since he only had one PC, he came to visit to
install it on mine. Since his computer had been used to develop Linux,
which had simply
grown on top of his Minix installation, it had never actually been
installed before. Thus, mine was the first PC
where Linux was ever installed. While this was happening, I was taking
a nap, and I recommend this method of installing Linux: napping, while
Linus does the hard work. The first releases of Linux used a license that forbade commercial
use. Some of the early contributors suggested a change to a free-software
license. In the fall of 1991, Richard Stallman visited 
Finland and I took Linus to a talk given by Stallman. This, the
pressure from contributors, and my nagging eventually convinced Linus
to choose the GNU GPL license instead, in early 1992. Over the Christmas break, Linus implemented virtual memory in Linux.
This made Linux a much more practical operating system on cheap
machines with little memory. The year 1992 started with the famousdebate with Andrew
Tanenbaum, who is a university professor and the author of MINIX. He had
some opinions about Linux and its architecture. Linus had opinions on
MINIX. The debate has been described as a flame war, but was actually
rather civil in hindsight. More importantly for the future success of Linux was that the X11
system was ported to it, making 1992 the year of the Linux desktop. I had chosen to contribute on the community side, rather than to the
kernel directly, and helped answer questions, write documentation, and
such. I also ran a short-lived newsletter about Linux, which is mainly
interesting for publishing thefirst ever interview with
Linus. The newsletter was effectively replaced by thecomp.os.linux.announcenewsgroup. The first Linux distribution was also started in 1992:Softlanding
Linux Systemor SLS. The next year, SLS morphed into Slackware, which
inspired Ian Murdock to start Debian in 1993, in order to explore a
more community-based development structure. A few other distributions would
follow in the 
years to come. In 1993, both Linus and I got hired as teaching assistants at the
university. We got to share an office. That room had a PC, which Linus
took over, and used for Linux development. I was happy with a DEC
terminal for Usenet access. One day, Linus was bored and the PC at work felt slow. He spent the
day rewriting the Linux kernel command-line parser in assembly
language, for speed. (That was, of course, quite pointless, and the
parser would later be rewritten again in C, for portability. Its speed
does not matter.) A couple of years later, he spent days playing
Quake, ostensibly to stress-test kernel memory management, although
that was with a newer PC. Much fun was had in that room, and there were no
pranks 
whatsoever. None at all. At some point, Linux gained support for Ethernet and TCP/IP. That meant
one could read Usenet without having to use a modem. Alas, early Linux
networking code was occasionally a little rough, having been written
from scratch. At one point, Linux would send some broken packets that
took down all of the Sun machines on the network. As it was difficult to get
the Sun kernel fixed, Linux was banned from the university network
until its bug was fixed. Not having Usenet access from one's desk is a
great motivator. In the spring of 1994 we felt that Linux was done. Finished. Nothing
more to add. One could use Linux to compile itself, to read Usenet, and
run many copies of thexeyesprogram at once.  We
decided to release version 1.0 and arranged arelease event. The
Finnish computer press was invited, and a TV station even sent a crew. Most
of the event consisted of ceremonially compiling Linux 1.0 in the
background, while Linus and others spoke about what Linux was and what it
was good for. Linus explained that commercial Unix for a PC was so
expensive that it was easier to write your own. In 1995 Linus and I did a software engineering course at the university,
which mostly consisted of a large practical project. This was built on top
of Linux, of course. I insisted that a version-control system be used. I
had witnessed students in earlier courses do the shouting kind of version
control: the students shared a source tree over NFS and shouted ""I'm
editing this file"" when they were changing something.  This did not seem
like an effective method to me, so I insisted onCVS,
which I'd just learned about. This experience is why Linus dislikes CVS and
for years refused to use any version control beyond uploading tar balls to
FTP sites. That year was also when Linux was first ported to a new architecture
by Linus. He'd been given a DEC Alpha machine. I would later get the
machine to use as a terminal for reading Usenet. Other people ported
Linux to other architectures, but that did not result in me getting any
more machines to read Usenet on. In 1997 Linus graduated and moved to the US to take a job atTransmeta. I took a
job at a different university in the Helsinki area. In the following years, many things happened. It turned out that there
were still a few missing features from Linux, so people worked on
those. The term ""open source"" was coined and IBM invested a ton of money in
Linux development. Netscape published a version of its web browser as
open source. Skipping a few details and many years, open source basically
took over the world. LWN was started and covered much of this history on a
week-by-week basis. In 1991, Linus wrote that Linux ""won't be big and professional
like gnu"". 
In 2023. Linux is running on every continent, on every ocean, on billions
of devices, in orbit, and on Mars. Not bad for what started as two threads,
writing streams of As and Bs on the screen.  The early days of LinuxPosted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]Thank you, Lars!  Wonderful article!The early days of LinuxPosted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses)Thank you Lars. The story is very interesting and inspiring. It is funny that Linux started as hobby project. :)The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing.The early days of LinuxPosted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link]Typical Lars article - self deprecating and quiet. Thanks for the Linux Documentation Project - I've a huge book next door which is basically all the HOWTOs from about 1994 which got me using Linux properly using first Slackware, then Linux-FT and finally Debian.You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.The early days of LinuxPosted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]Are you sure he was playing Quake and not Doom? Quake didn't come out until well after Linux was finished and wouldn't need memory management stress tested any more, but Doom got ported around the right time, I think.The early days of LinuxPosted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link]Heh... Yes, this was a nice read. But I'm going to add something. I wasn't involved in free software, programming or anything back then. Sure, I bought Dr Dobbs every month, but my major was Comparative and Historical Linguistics of the Sino-Tibetan languages.But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!The early days of LinuxPosted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link]> While this was happening, I was taking a nap, and I recommend this method of installing Linux: napping, while Linus does the hard work.I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history!The early days of LinuxPosted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses)Who has two thumbs and still misses Usenet?............The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)The early days of LinuxPosted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]Pleasure to know you Lars. You inspired me and many others!The early days of LinuxPosted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link]What a nice piece of history. Thanks Lars!Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!The early days of LinuxPosted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]Thank you Lars!The early days of LinuxPosted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]Priceless! :D Thank you for sharing!The early days of LinuxPosted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses)Very nice article. I first encountered Linux in 1994, where some nerds in my grad school had already set up a PC running Linux 1.1.3 (I think) to be the department's mail relay. And within a year or two there were Linux desktops around the theoretical physics group, with X11 and all. I remember the desktop viewport was larger than the screen resolution (640x480 in those days) and you had to pan around with a mouse, but it seemed better than Windows95 on the same machine.The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;)The early days of LinuxPosted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses)Great article, but one minor correction - the first Linux distribution was MCC Interim Linux. SLS didn't arrive until slightly later. I'd been using HJ Lu's boot/root disks, but switched to MCC when it was released.The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(The early days of LinuxPosted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]Wow. Simply one of the best pieces I've read about Linux in a few years. No jokes. Thanks for taking the time to share this.The early days of LinuxPosted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses)Awesome article that brings back lots of memories from that era. I discovered Linux in 1994 with SLS and kernel 1.0.4 or 1.0.5 and by then it looked like like a hack, I remember that it was enough to let ""ping"" run for a few tens of seconds to completely lose network connectivity on that NE2000 card. But everything was fun in it. When you booted from floppies to that white-on-black ""login:"" prompt, you really felt that the computer was waiting for you to be creative today.Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) )The early days of LinuxPosted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]Just gonna point out that Maddog helped arrange the Alpha, which he felt would be a huge boon both for Linux (cross architecture!) and DEC (running Linux!). This, after having Linus come speak at DEC in the US.The early days of LinuxPosted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]Please don't tell Linus that Prince of Persia apparently runs well on Wine:https://appdb.winehq.org/objectManager.php?sClass=applica..., lest Linux development get horribly delayed.The early days of LinuxPosted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]Lars, thanks for this article! Your name, Lars Wirzenius, was ringing a bell. I did not know about your role in the early days of Linux. Then I realized that I knew your name from the Linux Documentation Project. I hope that you still meet Linus from time to time.The early days of LinuxPosted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link]Dear Mr. Wirzenius,Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.The early days of LinuxPosted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link]I saved this article to read on a lazy Sunday afternoon, and indeed what a well written story. I got involved much later (being from a place where computers were rare and terribly expensive that early), and I remember fighting with Slackware <some version> and a Linux 1.2 kernel and X until, much later, I started understanding that my Trident card was not actually supported beyond VGA mode at that time. If I concentrate, I probably could remember the card number, that's how much I was fighting with all these new and wonderful things.And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated!The early days of LinuxPosted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link]That anecdote about Linux fubaring his master boot sector hits very close to home: back in 1993 I had a SunOS system at work and I would create floppies to take home to install Linux on my personal 486.  One day I got distracted and run dd to /dev/sd0 instead of /dev/fd0 and that was the end of that installation: time to reinstall SunOS from scratch and it took a day or two.I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).The early days of LinuxPosted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link]It's amazing how after all these years I still remember Lars's E-mail address — from USENET and the mailing list, I guess. It seems to be burned into my brain :-)Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]I got involved when I was working for Sun in late 1991. I had acquired a Sun 2, and was using it to pull news and email from a local university. When they switched UUCP protocols, I had to find another distro, as an upgrade to the latest Sun release was way outside my budget.I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!The early days of LinuxPosted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]great! thanks to linus and you.HN discussionPosted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]https://news.ycombinator.com/item?id=43225686 Posted Apr 12, 2023 17:27 UTC (Wed)
                               byccchips(subscriber, #3222)
                              [Link]  Posted Apr 12, 2023 18:04 UTC (Wed)
                               byNdjenks(guest, #164469)
                              [Link] (3 responses) The early days of LinuxPosted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses)And a good reminder that it's fine even if you start with something akin to a fidget toy and make major screw-ups along the way — something impressive can come out of it if you keep at it long enough.The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right.The early days of LinuxPosted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And a hobby project that attracted a very talented group of individuals from around the world to create something that has changed the face of computing. Posted Apr 12, 2023 18:29 UTC (Wed)
                               byflussence(guest, #85566)
                              [Link] (1 responses) The early days of LinuxPosted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]You are right. Posted Apr 14, 2023 21:02 UTC (Fri)
                               byNdjenks(guest, #164469)
                              [Link]  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]  Posted Apr 12, 2023 18:39 UTC (Wed)
                               byamacater(subscriber, #790)
                              [Link] You could run an ISP using only those HOWTOs - I know because myself and a good friend basically built a small ISP using only that documentation until we could get on the Internet. Thanks Lars for sacrificing a machine to someone else's pet student project and kickstarting everyone else.  Posted Apr 12, 2023 20:02 UTC (Wed)
                               byiabervon(subscriber, #722)
                              [Link]  Posted Apr 12, 2023 20:11 UTC (Wed)
                               byhalla(subscriber, #14185)
                              [Link] But got together with my wife (and now I'm, her wife, all of thirty years later) and we got a stack of SLS floppies from our neighbour in the converted-to-appartments-convent-school appartment where we lived, back then (every school room was converted into a living/kitching + bedroom + bathroom).He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! He gave my wife those 3.5"" disks because she liked nethack, and knew I was being re-schooled into an Oracle 4GL programmer, at the time. Our first child was coming up...We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much! We managed to boot up and install Linux back then in 1993, and never looked back. Well, I'm looking back now, but whatevs. Linux Journal. gui toolkits, windowmaker, fvwm, wordperfect for Unix, there's been so much!  Posted Apr 12, 2023 22:09 UTC (Wed)
                               byleromarinvit(subscriber, #56850)
                              [Link] I'd like to nominate this gem as QOTW. It also reminds me of another recent article here, which quoted Rebecca Giblin saying (about Cory Doctorow) ""if he were here, he would say 'please don't do that'"". I imagine that might be applicable here as well...Thank you, Lars, for this wonderfully written piece of history! Thank you, Lars, for this wonderfully written piece of history!  Posted Apr 12, 2023 23:21 UTC (Wed)
                               bycsigler(subscriber, #1224)
                              [Link] (5 responses) The early days of LinuxPosted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses)I miss alt.religion.kibology, which was the way better talk.bizarre.I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it...The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't.The early days of LinuxPosted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)I have always suspected the name of the 'rn' program was carefully chosen to be very similar to the often used 'rm'.The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type.The early days of LinuxPosted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link]Don't tell anybody -- but, it's still there, and actually quite usable nowadays.The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-) Posted Apr 13, 2023 5:53 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] (1 responses) I miss Andy Glew and Terje Mathisen penning hyper insightful articles on processor architecture and optimisations, on comp.archThat's about it... That's about it... The early days of LinuxPosted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]Just resubscribe to comp.arch.  Terje Mathisen is still active, although Andy Glew isn't. Posted Apr 13, 2023 16:30 UTC (Thu)
                               byanton(subscriber, #25547)
                              [Link]  Posted Apr 13, 2023 6:11 UTC (Thu)
                               byjem(subscriber, #24231)
                              [Link] (1 responses)  The early days of LinuxPosted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]It was a replacement for the then-ubiquitous ""readnews"" program, whose name took far too long to type. Posted Apr 13, 2023 20:20 UTC (Thu)
                               byklossner(subscriber, #30046)
                              [Link]   Posted Apr 13, 2023 23:29 UTC (Thu)
                               byjschrod(subscriber, #1646)
                              [Link] The flamers and AOL me-toos have gone, to Facebook, Twitter and other venues.Currently it's similar to the late 80s, and I like that. ;-)  Posted Apr 13, 2023 2:09 UTC (Thu)
                               byhendry(guest, #50859)
                              [Link]  Posted Apr 13, 2023 6:25 UTC (Thu)
                               bybof(subscriber, #110741)
                              [Link] Still remember creating SLS install disks, downloaded through the University FTP access, getting them to work and then just cloning and compiling for a few years what came up afterwards, who needs distros actually... Incredible that you could do all that back then on a 386 with just iirc 8 MB of RAM, and half a Gig or so of disk.Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! Never looked back to the previous OSses I ran, because - why?P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then! P.S.: Lars, remember our CeBIT booth backroom hacking on Kannel, early 00s? It was such a joy working with you on that back then!    Posted Apr 13, 2023 6:27 UTC (Thu)
                               byccezar(subscriber, #2749)
                              [Link]  Posted Apr 13, 2023 8:14 UTC (Thu)
                               bypmatilai(subscriber, #15420)
                              [Link]  Posted Apr 13, 2023 8:36 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (9 responses) The early days of LinuxPosted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses)PS - reading Linus's interview (1992 (his style seems to have changed later (just a little))), I wonder if he was ever interested in Lisp programming (just joking (I think)).corbet is clearly a C programmer; his love of semicolons indicates that.SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 8:39 UTC (Thu)
                               byrsidd(subscriber, #2582)
                              [Link] (8 responses) corbet is clearly a C programmer; his love of semicolons indicates that. SemicolonsPosted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses)The first high-level language I learned was actually Pascal, which was the future according to my university's CS department.  I suspect that warped my mind in a number of ways...SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :)The early days of LinuxPosted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]I see what you did there ;) Posted Apr 13, 2023 13:31 UTC (Thu)
                               bycorbet(editor, #1)
                              [Link] (6 responses) SemicolonsPosted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link]:-)FORTRAN - and I can tell it's warped my mind :-)Cheers,WolSemicolonsPosted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses)> The first high-level language I learned was actually PascalSince we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life""SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch*SemicolonsPosted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]Not Algol? :) Posted Apr 13, 2023 14:42 UTC (Thu)
                               byWol(subscriber, #4433)
                              [Link] FORTRAN - and I can tell it's warped my mind :-)Cheers,Wol Cheers,Wol  Posted Apr 14, 2023 1:54 UTC (Fri)
                               byCyberax(✭ supporter ✭, #52523)
                              [Link] (3 responses) Since we're quoting Linus today: ""Yeah, yeah, most _practical_ versions of Pascal ended up having all the stuff necessary to break structure, but as you may be able to tell, I was one of the unwashed masses who had to write in ""standard Pascal"" in my youth. I'm scarred for life"" SemicolonsPosted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses)My first language was BASIC, on a TRS-80 and a BBC Micro. Then came Pascal (Turbo Pascal on MS-DOS). I learned C in grad school in a physics department. I successfully avoided learning Fortran ever. But I'm told Fortran 2003 onwards is not so bad.SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it.SemicolonsPosted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link]When I was at uni in the late '90s, we had a course ""Programming for Physicists"" that used Standard Pascal. I was already familiar with a handful of other languages (including TurboPascal™) and finding out that there was such a thing as a ""high level"" language with no concept of ""strings"" was mind blowing (and not in a good way)!Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Posted Apr 14, 2023 4:27 UTC (Fri)
                               byrsidd(subscriber, #2582)
                              [Link] (1 responses) SemicolonsPosted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]Fortran certainly is interesting these days. Many modifiers are available to fine tune exactly what you mean (instead of C's ""eh, something sane or UB"" coin flip). Of course, discovering the modifiers is troublesome at times and with umpteen ways to spell things, the ""best"" way feels more like C++ subsetting arguments if one cared enough to argue about it. Posted Apr 17, 2023 1:02 UTC (Mon)
                               bymathstuf(subscriber, #69389)
                              [Link]  Posted May 15, 2023 5:44 UTC (Mon)
                               bysammythesnake(guest, #17693)
                              [Link] Thankfully, I understand that my cohort was the last year before they switched to using that new fangled ""Java"" thing, which despite valid criticisms was at least a great language for learning how to ""do OOP right"".Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* Standard Pascal is not, IMNSHO a ""great language"" for really anything other than being less brain damaging than my later experience having to use that prehistoric dinosaur FORTRAN - and because it was in the context of stick-in-the-mud physicists at a university, it was a pretty early flavour, too (which at least *had strings* as of FORTRAN77 in '78, 2 decades before that Pascal course!)I'm mostly Ok now, though. *twitch* I'm mostly Ok now, though. *twitch*  Posted Apr 20, 2023 7:58 UTC (Thu)
                               byfest3er(guest, #60379)
                              [Link]  Posted Apr 13, 2023 14:44 UTC (Thu)
                               byKlaasjan(subscriber, #4951)
                              [Link]   Posted Apr 13, 2023 14:00 UTC (Thu)
                               byTet(subscriber, #5433)
                              [Link] (1 responses) The early days of LinuxPosted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link]And it was flexible enough so that it could be modified to create your own distro, at a time when a Linux distro could be created on a few floppies. Fun times!I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :( Posted Jul 3, 2023 19:04 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I did a distro that was elm/uucp-centric, but lost all of my old software and archives in Hurricane Ivan. :(  Posted Apr 13, 2023 17:10 UTC (Thu)
                               bykarim(subscriber, #114)
                              [Link]  Posted Apr 14, 2023 6:24 UTC (Fri)
                               bywtarreau(subscriber, #51152)
                              [Link] (1 responses) Your story about multi-tasking with ""A"" and ""B"" is excellent. I did something comparable when trying to turn an XT motherboard to SMP. I noticed the 8087 and 8088 almost had the same pinout, and using a pair of 74LSxx chips solder on top of it with the 8088 pins bent, I managed to run a second 8088 inserted into the 8087 socket. I had to invert its A19 pin so that it could boot to a RAM address that I could control (just below 512kB) before I released the RST pin. I had zero experience with SMP by then and figured nothing in my MS-DOS was designed to support this. I remember thinking ""if at least it could format floppies in the background"" (yes by then that was a common and extremely boring task). So I managed to make this second CPU blink the floppy drive's LED by writing to 3F2 IIRC, and could confirm that it continued to do so while I was starting a graphics game on the main CPU. Then I tried other stuff such as switching the CGA text attributes in the frame buffer to change colors on screen. That was totally useless but it felt absolutely awesome to me to imagine that this second CPU with 8 ot 10 pins bent and soldered to mollested 74LSxx was actually working fine there and sharing bus access with the primary CPU. So I can definitely understand the joy you and Linus experienced when seeing this A/B on screen!  The early days of LinuxPosted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link]That story about bending an 8088 to your, er, will is probably the single most terrifying hardware-hacking story not involving high voltage I have ever heard. I'm astonished that it worked at all, but knowing how the 8088's bus-arbitration works it probably did work, simply because the 8087 looked so very much like another 8088 to the 8088 that it was probably happy to coexist with an *actual* 8088 that wasn't trying to do floating point anything. (After all, the 8087 and 8088 more or less had duplicate instruction decoders, etc.)(... I'm also jealous that you could afford to risk a whole 8088 like that :) ) Posted Jun 25, 2023 20:27 UTC (Sun)
                               bynix(subscriber, #2304)
                              [Link] (... I'm also jealous that you could afford to risk a whole 8088 like that :) )   Posted Apr 14, 2023 10:34 UTC (Fri)
                               bykena(subscriber, #2735)
                              [Link]  Posted Apr 14, 2023 13:55 UTC (Fri)
                               byNightMonkey(subscriber, #23051)
                              [Link]  Posted Apr 14, 2023 14:45 UTC (Fri)
                               byPhilippReisner(subscriber, #153492)
                              [Link]  Posted Apr 15, 2023 14:21 UTC (Sat)
                               byermo(subscriber, #86690)
                              [Link] Thank you for the wonderful article and thank you for the time and effort you (and your peers of the same persuasion) put into Linux and its documentation over the years.It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back. It is primarily thanks to said efforts that yours truly got hooked on Linux and never looked back.  Posted Apr 16, 2023 17:27 UTC (Sun)
                               byiustin(subscriber, #102433)
                              [Link] And yes, the Linux Documentation Project was awesome. Like another commenter, I also built small ISPs based on nothing more than one or two Linux machines, lots of serial port expanders, and reading many, many man pages.Thank you for the trip down the memory lane, much appreciated! Thank you for the trip down the memory lane, much appreciated!  Posted Apr 23, 2023 14:55 UTC (Sun)
                               bymadscientist(subscriber, #16861)
                              [Link] I'm not sure I ever fessed up to my bosses exactly how my system got corrupted :).  Posted Apr 27, 2023 13:45 UTC (Thu)
                               byjwr(guest, #164834)
                              [Link] Thank you Lars for this nice writeup, it brings back memories of installing Slackware 1.0 in 1993.  Posted Jul 3, 2023 20:15 UTC (Mon)
                               byn7ekg(guest, #165912)
                              [Link] I tried 386/BSD, but I wasn't happy with having to wait 6 months for releases, so when I saw Linus' famous ""free OS"" email in late 1991 (October, as I recall), I jumped on board. Haven't looked back since - I've even been thinking about reviving my UUCP-over-SSH stuff. :)I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! I ported a lot of software, even ported my own version of curses that I had written years before for MS-DOS, but ncurses won the day. It's been a fun ride, seeing all these distributions, the freedom of choice, the different windowing managers, and lots and lots of apps! From one of my first Linux boxes running 0.99+ on a 486DX4-100 with 16 MB of RAM (the motherboard was sitting on top of a pizza box), to today's monster servers and laptops, it's been an awfully fun journey!Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment! Thanks to Lars, Ted, Remy, Peter, Ian, Linus, and all the rest whose names I can't recall at the moment!  Posted Oct 31, 2023 3:35 UTC (Tue)
                               bylouisdaoren(guest, #167736)
                              [Link]  Posted Mar 2, 2025 4:45 UTC (Sun)
                               bypabs(subscriber, #43278)
                              [Link]  Copyright © 2023, Eklektix, Inc.Comments and public postings are copyrighted by their creators.Linux  is a registered trademark of Linus Torvalds"
"The most unhinged video wall, made out of Chromebooks",https://varun.ch/posts/videowall/,Hacker News,2025-03-01T12:54:55,Hacker News,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"This is the story of our three year long journey to turn a fleet of laptops into what can only be described as the world’s most unhinged video wall. This project was a collaboration with my friendAksel Salmi. I was responsible for the software, and he designed the incredible hardware, seehis blogto learn about the unexpectedly complex hardware needed to mount these dismantled computers[1].   About three years ago, my Design teacher (The amazing Mr. Bush) came to us with an idea - our school was about to dispose of its fleet of Chromebooks, and he was wondering if we could build anything with them.  The Lenovo ThinkPad 11e could very well be the world’s worst laptop. It is also the standard-issue school laptop that reinforced eight-year-old me’s interest in computers. We used this school-issued laptop through primary and the start of middle school. This is me in 5th grade using a school laptop while working on my PYP Exhibition project (a game on Scratch)[2]. Despite my emotional connection to them, today these devices are, for all intents and purposes, junk. And for that reason, my school began the process of replacing them (with marginally less junky laptops)  These things don’t receive software updates from Google anymore, they struggle loading most webpages and to top it off, they’re tied to some long forgotten Enterprise Enrolment system, so they can’t even be used without a school Google account. A video wall is a large display made up of multiple screens arranged together to create a single, seamless display across all the screens. In the case of our project, we decided to try reusing the laptop screens to build a video wall. Our first idea was to harvest just the laptop display panels and somehow drive them using a powerful computer that could power the 10 screens simultaneously. We did not go this route (due to the fact that we had no idea what we were doing, and a quick estimate of the time and costs involved scared us away). Since the screens were attached to perfectly functional laptops, it was quickly apparent that we’d probably be better off letting each screen be driven independently by their own laptop motherboards. At this point, there were still many questions (eg. how were we going to do that onChromebooks), so we put aside that challenge to focus on the new issue this brings up: Can we synchronise a single video across multiple computers? Our experiments brought us to the school’s computer lab, where we experimented with VLC’s streaming abilities to get a stream synchronised across devices on a single network, but this posed two challenges: This system is not designed for videos beingperfectlyin sync, nor was it designed for two clients to receive different video inputs (because the whole point of the video wall is to display one loooooong video across the screens, not 10 repeat copies of the same video). We were stuck here until my ““breakthrough””. For context, the story is currently in 2022. Two years earlier, I had been locked up in my room due to the COVID lockdown, and in this time, I had loads of fun building random realtime web apps, like a chat app and multiplayer drawing game. These apps worked thanks tosocket.io, a (primarily) WebSocket based library that allows for low-latency, bi-directional communication. Screenshot of a chat site I made to pass the time during the 2020 lockdown I realised that my best bet to get videos synchronised would be by using a web page that usedsocket.ioto sync the video playback across clients. Yes, there are better approaches, but simply doing something like this worked unreasonably well, all things considered. I named this ExpressJS server/client systemc-sync[3]. Thanks to c-sync (and tons of tinkering), after some time we had decently synchronised videos across computer screens through a webpage (or at least it seemed like it, testing on these desktop computers)  As it turns out, in reality, the Chromebooks are too slow for this to be a reliable approach to synchronising playback, and tiny discrepancies in loading times + latency + system clocks etc. lead to videos not being synchronised. Now, I’m not entirely surewhythis works so well, but I came up with a ridiculous solution by accident. When videos reach the end of playback, each client emits the start event. This means that the slowest computers hold back the fastest computers, and get the chance to load the videos. This also means looping can be a very slightly jittery process (with each screen receiving 10 ‘start’ events), but as long as the first couple frames of the video are identical, nobody would even notice. Using this method, we have nearly perfectly synchronised video playback, and can play any video on any screen (meaning we can split a wide video into 10 segments, and each computer displays its respective part, all in sync with eachother) A disassembled Chromebook open to a test video We reached this stage within a month or two. Believe it or not, this project still had three years of work ahead of us. The biggest issue was Chromebook software. At this point, we had a website that we could manually open on each laptop to display a fullscreen synchronised video. Ideally, we would want this to be entirely automated, so that as soon as a Chromebook receives power, it boots up automagically to the c-sync client page. Unfortunately, right now, booting the Chromebook would just take you to a Google login page (and one that was locked to our school domainto boot). Also, just to add insult to injury, when batteries are removed, the laptops don’t turn themselves on when they receive power (you have to hold down the power button) This meant that our next step would have to be to replace ChromeOS with something else. The‘ChromeOS Firmware Recovery Script’is a magical piece of technology that somehow supports many different Chromebook motherboards. Ours was called ‘GLIMMER’. We just had to enter the built-in ‘Recovery Mode’, enable ‘Developer Mode’ and use the ChromeOS Shell to run the script. Now we’re basically on the home stretch. All we needed to do was pick up some stable Linux distro, write a hacky startup script that loads up Chromium and simulates the keystrokes to fullscreen the video and we’re done!  We ran in to two main issues: Some Chromebooks (roughly half of our working laptops) would refuse to enter developer mode due to the enterprise enrolment, and while we were able to get the other half onto a Linux distro, video playback would consistently freeze after some time (actually they would lock up entirely). It took us several months of on-and-off experimentation to figure out what to do. Essentially, the solution was to overwrite the entire default firmware withcoreboot(which is also possible using MrChromebox’s script). We just needed to remove the ‘Write Protection’ screw from each laptop motherboard, and this seemed to bypass the enrolment too. Lenovo’s handy Write Protection screw diagram Doing this for 20+ computers was slow and tedious. We only really needed the WiFi, motherboard and screen in working condition, but we decided to be (mostly) gentle and keep the laptops looking like laptops so that we had a keyboard and mouse for the rest of the installation steps. By the end, we got quite efficient at removing the write protection screw After ‘corebooting’ the Chromebooks, we were also pleasantly surprised to find out that ‘Wake on AC’ was a feature of the firmware, and that video playback no longer randomly breaks. By this point we had enough non-bricked Chromebooks left over for a line of 10 screens and a handful of spares. Now we’re really on the final stretch. Aksel worked on the mounting hardware, which you canread about on his blog, while I worked on figuring out a less flaky way to ‘boot to a webpage’ than the keystroke simulation and startup script I bodged together. I previously usedthe aptly named ‘FullPageOS’for a different project (which I briefly mention inmy TED talk, which you should watch), but it doesn’t run on x86 hardware. I landed on using ‘Porteus Kiosk’, which is just a minimal Linux distro that opens a fullscreen Chromium browser with all the correct flags for hands-off usage (eg. allowing video playback without user interaction)  This honestly worked totally fine, but left me unsatisfied for two reasons. Firstly, I didn’t like how we couldn’t customise the splash screen, so our project would be forever stamped with the Porteus logo on every startup (which would be every morning). And secondly, in search of a better issue to justify the extra work, I realised we can’t remotely do anything to the installations (eg. changing the page URL) without re-doing them, which would be definitely a problem once these get mounted on the wall. For those good reasons, I embarked on the journey of building ‘my own distro’ that we could install on the laptops. The system should start with something minimal (no desktop environment), and have an elegant script to autostart a kiosk mode Chromium instance. I first tried NixOS before quickly realising there was no way it would work with the tiny amount of storage on these Chromebooks (and it failed to install with every single attempt). Then I gave up, started with a Debian minimal install and just wrote a script that would provision a client (generate a ‘KIOSK_ID’, set its hostname tocsync-client-$KIOSK_ID, connect to the school’s WiFi, create users/permissions and set upopenboxto autostart a fullscreen kiosk mode Chromium). Then after attempting to repeat this on a second machine, I realised I would be wasting so much time (installing Debian is very ‘hands-on’ - you need to press lots of buttons), and I discovered ‘FAI - Fully Automatic Installation’ and the webFAI.metool. To cut a long story short[4], after redoing everything for the millionth time, I had a single USB that I can plug in to any ‘corebooted’ Chromebook which provisions it as a c-sync client. Woohoo! I also built out a ‘controller’ for c-sync which lets us manage connected clients and assign them videos.   After a successful three day stress test where the playback remained butter-smooth (and I sacrificed my ability to sleep for the greater good of testing with the backlight on), we were ready to mount these laptops on the wall.  The mounting is mostly Aksel’s thing, soI implore you to read his blog, but here are some cool photos from the process. (also aren’t our cable splices so pretty and not terrifying?? 😁❤️) An early iteration of the mounting backplate using a laser cut acrylic piece Aksel designed a pretty awesome looking backplate to mount the motherboard, which hangs on cleats on the walls. The displays are then held in place with clampy things. This is black magic to me. Everything laid out Preparing some displays and motherboards We decided to splice together power cables so that each power supply could power two computers. Send any complaints tothe pager on my contact page      Nearly there! After we painstakingly mounted everything, I realised something sort-of important. Computers generate heat. Somewhere along the way of wiping away the firmwares, the laptop fans stopped spinning, which meant things get quite hot quite quickly. I had to figure out a way to get those working again before we could comfortably leave this up 24/7 (well, actually 12/7). You can apparently interface with the‘ChromeOS Embedded Controller’using a tool calledectool, which should allow you to manually set fan speeds (among other things). The online documentation for this is lacking, and there’s apparently a slightly differentectoolfromcorebootand from Google directly. None of this made much sense at all to me, and no builtectoolbinary I could find would work. At some point, I found a dead link, butthanks to the magic of the Wayback Machine, I was able to get my hands on something that wouldn’t immediately crash. By some miracle, this version of the tool actually works perfectly fine at setting fan speeds, and after some testing, I found some goldilocks values that balance noise and temperature. As it turns out, making such a wide video is actually not easy. Each display has a resolution of 1366× 768, and very few pieces of software will let you edit a 13660 × 768 video. Final Cut Pro and Blender are the only programs we were able to do anything this wide in. Blender is one of the greatest pieces of software ever created (alongside c-sync) Then it’s just a matter of rendering the wide video and splitting it into 10 segments. Our video wall is imperfect. TN panel viewing angles suck, and the screens vary in colours and stuff. Yes, the synchronisation isn’tperfect, and yes, I’m sure there were better alternatives for nearly every decision we made along the way. Yet I love our video wall, despite how absurdly weird it is. It’s a perfect representation of the iterative design process and a true testament to teamwork and collaboration. We turned E-Waste into something interesting. And maybe, just maybe, the real video wall was the friends we made along the way.  This project was made possible by the incredible work of so many people. Aside from my collaborator Aksel Salmi, our Design teacher Daniel Bush played a huge role in guiding us through the project. Additionally, I wanted to thank thecoreboot projectand Matt ‘MrChromebox’ DeVillier for putting togetherthe firmware and toolsthat allowed any of this to work. I would also like to thank Thomas Lange of theFAI projectfor his help in building the FAI.me based automated installer that saved us so many many many hours, as well as his support over email. As silly as it sounds, this project was a backbone in my high-school experience. We hacked away at it every Monday for the past few years, and we grew up along the way too.","This is the story of our three year long journey to turn a fleet of laptops into what can only be described as the world’s most unhinged video wall. This project was a collaboration with my friendAksel Salmi. I was responsible for the software, and he designed the incredible hardware, seehis blogto learn about the unexpectedly complex hardware needed to mount these dismantled computers[1].   About three years ago, my Design teacher (The amazing Mr. Bush) came to us with an idea - our school was about to dispose of its fleet of Chromebooks, and he was wondering if we could build anything with them.  The Lenovo ThinkPad 11e could very well be the world’s worst laptop. It is also the standard-issue school laptop that reinforced eight-year-old me’s interest in computers. We used this school-issued laptop through primary and the start of middle school. This is me in 5th grade using a school laptop while working on my PYP Exhibition project (a game on Scratch)[2]. Despite my emotional connection to them, today these devices are, for all intents and purposes, junk. And for that reason, my school began the process of replacing them (with marginally less junky laptops)  These things don’t receive software updates from Google anymore, they struggle loading most webpages and to top it off, they’re tied to some long forgotten Enterprise Enrolment system, so they can’t even be used without a school Google account. A video wall is a large display made up of multiple screens arranged together to create a single, seamless display across all the screens. In the case of our project, we decided to try reusing the laptop screens to build a video wall. Our first idea was to harvest just the laptop display panels and somehow drive them using a powerful computer that could power the 10 screens simultaneously. We did not go this route (due to the fact that we had no idea what we were doing, and a quick estimate of the time and costs involved scared us away). Since the screens were attached to perfectly functional laptops, it was quickly apparent that we’d probably be better off letting each screen be driven independently by their own laptop motherboards. At this point, there were still many questions (eg. how were we going to do that onChromebooks), so we put aside that challenge to focus on the new issue this brings up: Can we synchronise a single video across multiple computers? Our experiments brought us to the school’s computer lab, where we experimented with VLC’s streaming abilities to get a stream synchronised across devices on a single network, but this posed two challenges: This system is not designed for videos beingperfectlyin sync, nor was it designed for two clients to receive different video inputs (because the whole point of the video wall is to display one loooooong video across the screens, not 10 repeat copies of the same video). We were stuck here until my ““breakthrough””. For context, the story is currently in 2022. Two years earlier, I had been locked up in my room due to the COVID lockdown, and in this time, I had loads of fun building random realtime web apps, like a chat app and multiplayer drawing game. These apps worked thanks tosocket.io, a (primarily) WebSocket based library that allows for low-latency, bi-directional communication. Screenshot of a chat site I made to pass the time during the 2020 lockdown I realised that my best bet to get videos synchronised would be by using a web page that usedsocket.ioto sync the video playback across clients. Yes, there are better approaches, but simply doing something like this worked unreasonably well, all things considered. I named this ExpressJS server/client systemc-sync[3]. Thanks to c-sync (and tons of tinkering), after some time we had decently synchronised videos across computer screens through a webpage (or at least it seemed like it, testing on these desktop computers)  As it turns out, in reality, the Chromebooks are too slow for this to be a reliable approach to synchronising playback, and tiny discrepancies in loading times + latency + system clocks etc. lead to videos not being synchronised. Now, I’m not entirely surewhythis works so well, but I came up with a ridiculous solution by accident. When videos reach the end of playback, each client emits the start event. This means that the slowest computers hold back the fastest computers, and get the chance to load the videos. This also means looping can be a very slightly jittery process (with each screen receiving 10 ‘start’ events), but as long as the first couple frames of the video are identical, nobody would even notice. Using this method, we have nearly perfectly synchronised video playback, and can play any video on any screen (meaning we can split a wide video into 10 segments, and each computer displays its respective part, all in sync with eachother) A disassembled Chromebook open to a test video We reached this stage within a month or two. Believe it or not, this project still had three years of work ahead of us. The biggest issue was Chromebook software. At this point, we had a website that we could manually open on each laptop to display a fullscreen synchronised video. Ideally, we would want this to be entirely automated, so that as soon as a Chromebook receives power, it boots up automagically to the c-sync client page. Unfortunately, right now, booting the Chromebook would just take you to a Google login page (and one that was locked to our school domainto boot). Also, just to add insult to injury, when batteries are removed, the laptops don’t turn themselves on when they receive power (you have to hold down the power button) This meant that our next step would have to be to replace ChromeOS with something else. The‘ChromeOS Firmware Recovery Script’is a magical piece of technology that somehow supports many different Chromebook motherboards. Ours was called ‘GLIMMER’. We just had to enter the built-in ‘Recovery Mode’, enable ‘Developer Mode’ and use the ChromeOS Shell to run the script. Now we’re basically on the home stretch. All we needed to do was pick up some stable Linux distro, write a hacky startup script that loads up Chromium and simulates the keystrokes to fullscreen the video and we’re done!  We ran in to two main issues: Some Chromebooks (roughly half of our working laptops) would refuse to enter developer mode due to the enterprise enrolment, and while we were able to get the other half onto a Linux distro, video playback would consistently freeze after some time (actually they would lock up entirely). It took us several months of on-and-off experimentation to figure out what to do. Essentially, the solution was to overwrite the entire default firmware withcoreboot(which is also possible using MrChromebox’s script). We just needed to remove the ‘Write Protection’ screw from each laptop motherboard, and this seemed to bypass the enrolment too. Lenovo’s handy Write Protection screw diagram Doing this for 20+ computers was slow and tedious. We only really needed the WiFi, motherboard and screen in working condition, but we decided to be (mostly) gentle and keep the laptops looking like laptops so that we had a keyboard and mouse for the rest of the installation steps. By the end, we got quite efficient at removing the write protection screw After ‘corebooting’ the Chromebooks, we were also pleasantly surprised to find out that ‘Wake on AC’ was a feature of the firmware, and that video playback no longer randomly breaks. By this point we had enough non-bricked Chromebooks left over for a line of 10 screens and a handful of spares. Now we’re really on the final stretch. Aksel worked on the mounting hardware, which you canread about on his blog, while I worked on figuring out a less flaky way to ‘boot to a webpage’ than the keystroke simulation and startup script I bodged together. I previously usedthe aptly named ‘FullPageOS’for a different project (which I briefly mention inmy TED talk, which you should watch), but it doesn’t run on x86 hardware. I landed on using ‘Porteus Kiosk’, which is just a minimal Linux distro that opens a fullscreen Chromium browser with all the correct flags for hands-off usage (eg. allowing video playback without user interaction)  This honestly worked totally fine, but left me unsatisfied for two reasons. Firstly, I didn’t like how we couldn’t customise the splash screen, so our project would be forever stamped with the Porteus logo on every startup (which would be every morning). And secondly, in search of a better issue to justify the extra work, I realised we can’t remotely do anything to the installations (eg. changing the page URL) without re-doing them, which would be definitely a problem once these get mounted on the wall. For those good reasons, I embarked on the journey of building ‘my own distro’ that we could install on the laptops. The system should start with something minimal (no desktop environment), and have an elegant script to autostart a kiosk mode Chromium instance. I first tried NixOS before quickly realising there was no way it would work with the tiny amount of storage on these Chromebooks (and it failed to install with every single attempt). Then I gave up, started with a Debian minimal install and just wrote a script that would provision a client (generate a ‘KIOSK_ID’, set its hostname tocsync-client-$KIOSK_ID, connect to the school’s WiFi, create users/permissions and set upopenboxto autostart a fullscreen kiosk mode Chromium). Then after attempting to repeat this on a second machine, I realised I would be wasting so much time (installing Debian is very ‘hands-on’ - you need to press lots of buttons), and I discovered ‘FAI - Fully Automatic Installation’ and the webFAI.metool. To cut a long story short[4], after redoing everything for the millionth time, I had a single USB that I can plug in to any ‘corebooted’ Chromebook which provisions it as a c-sync client. Woohoo! I also built out a ‘controller’ for c-sync which lets us manage connected clients and assign them videos.   After a successful three day stress test where the playback remained butter-smooth (and I sacrificed my ability to sleep for the greater good of testing with the backlight on), we were ready to mount these laptops on the wall.  The mounting is mostly Aksel’s thing, soI implore you to read his blog, but here are some cool photos from the process. (also aren’t our cable splices so pretty and not terrifying?? 😁❤️) An early iteration of the mounting backplate using a laser cut acrylic piece Aksel designed a pretty awesome looking backplate to mount the motherboard, which hangs on cleats on the walls. The displays are then held in place with clampy things. This is black magic to me. Everything laid out Preparing some displays and motherboards We decided to splice together power cables so that each power supply could power two computers. Send any complaints tothe pager on my contact page      Nearly there! After we painstakingly mounted everything, I realised something sort-of important. Computers generate heat. Somewhere along the way of wiping away the firmwares, the laptop fans stopped spinning, which meant things get quite hot quite quickly. I had to figure out a way to get those working again before we could comfortably leave this up 24/7 (well, actually 12/7). You can apparently interface with the‘ChromeOS Embedded Controller’using a tool calledectool, which should allow you to manually set fan speeds (among other things). The online documentation for this is lacking, and there’s apparently a slightly differentectoolfromcorebootand from Google directly. None of this made much sense at all to me, and no builtectoolbinary I could find would work. At some point, I found a dead link, butthanks to the magic of the Wayback Machine, I was able to get my hands on something that wouldn’t immediately crash. By some miracle, this version of the tool actually works perfectly fine at setting fan speeds, and after some testing, I found some goldilocks values that balance noise and temperature. As it turns out, making such a wide video is actually not easy. Each display has a resolution of 1366× 768, and very few pieces of software will let you edit a 13660 × 768 video. Final Cut Pro and Blender are the only programs we were able to do anything this wide in. Blender is one of the greatest pieces of software ever created (alongside c-sync) Then it’s just a matter of rendering the wide video and splitting it into 10 segments. Our video wall is imperfect. TN panel viewing angles suck, and the screens vary in colours and stuff. Yes, the synchronisation isn’tperfect, and yes, I’m sure there were better alternatives for nearly every decision we made along the way. Yet I love our video wall, despite how absurdly weird it is. It’s a perfect representation of the iterative design process and a true testament to teamwork and collaboration. We turned E-Waste into something interesting. And maybe, just maybe, the real video wall was the friends we made along the way.  This project was made possible by the incredible work of so many people. Aside from my collaborator Aksel Salmi, our Design teacher Daniel Bush played a huge role in guiding us through the project. Additionally, I wanted to thank thecoreboot projectand Matt ‘MrChromebox’ DeVillier for putting togetherthe firmware and toolsthat allowed any of this to work. I would also like to thank Thomas Lange of theFAI projectfor his help in building the FAI.me based automated installer that saved us so many many many hours, as well as his support over email. As silly as it sounds, this project was a backbone in my high-school experience. We hacked away at it every Monday for the past few years, and we grew up along the way too.","The most unhinged video wall, made out of Chromebooks","

Key Points:
",Software Development,"This is the story of our three year long journey to turn a fleet of laptops into what can only be described as the world’s most unhinged video wall. This project was a collaboration with my friendAksel Salmi. I was responsible for the software, and he designed the incredible hardware, seehis blogto learn about the unexpectedly complex hardware needed to mount these dismantled computers[1].   About three years ago, my Design teacher (The amazing Mr. Bush) came to us with an idea - our school was about to dispose of its fleet of Chromebooks, and he was wondering if we could build anything with them.  The Lenovo ThinkPad 11e could very well be the world’s worst laptop. It is also the standard-issue school laptop that reinforced eight-year-old me’s interest in computers. We used this school-issued laptop through primary and the start of middle school. This is me in 5th grade using a school laptop while working on my PYP Exhibition project (a game on Scratch)[2]. Despite my emotional connection to them, today these devices are, for all intents and purposes, junk. And for that reason, my school began the process of replacing them (with marginally less junky laptops)  These things don’t receive software updates from Google anymore, they struggle loading most webpages and to top it off, they’re tied to some long forgotten Enterprise Enrolment system, so they can’t even be used without a school Google account. A video wall is a large display made up of multiple screens arranged together to create a single, seamless display across all the screens. In the case of our project, we decided to try reusing the laptop screens to build a video wall. Our first idea was to harvest just the laptop display panels and somehow drive them using a powerful computer that could power the 10 screens simultaneously. We did not go this route (due to the fact that we had no idea what we were doing, and a quick estimate of the time and costs involved scared us away). Since the screens were attached to perfectly functional laptops, it was quickly apparent that we’d probably be better off letting each screen be driven independently by their own laptop motherboards. At this point, there were still many questions (eg. how were we going to do that onChromebooks), so we put aside that challenge to focus on the new issue this brings up: Can we synchronise a single video across multiple computers? Our experiments brought us to the school’s computer lab, where we experimented with VLC’s streaming abilities to get a stream synchronised across devices on a single network, but this posed two challenges: This system is not designed for videos beingperfectlyin sync, nor was it designed for two clients to receive different video inputs (because the whole point of the video wall is to display one loooooong video across the screens, not 10 repeat copies of the same video). We were stuck here until my ““breakthrough””. For context, the story is currently in 2022. Two years earlier, I had been locked up in my room due to the COVID lockdown, and in this time, I had loads of fun building random realtime web apps, like a chat app and multiplayer drawing game. These apps worked thanks tosocket.io, a (primarily) WebSocket based library that allows for low-latency, bi-directional communication. Screenshot of a chat site I made to pass the time during the 2020 lockdown I realised that my best bet to get videos synchronised would be by using a web page that usedsocket.ioto sync the video playback across clients. Yes, there are better approaches, but simply doing something like this worked unreasonably well, all things considered. I named this ExpressJS server/client systemc-sync[3]. Thanks to c-sync (and tons of tinkering), after some time we had decently synchronised videos across computer screens through a webpage (or at least it seemed like it, testing on these desktop computers)  As it turns out, in reality, the Chromebooks are too slow for this to be a reliable approach to synchronising playback, and tiny discrepancies in loading times + latency + system clocks etc. lead to videos not being synchronised. Now, I’m not entirely surewhythis works so well, but I came up with a ridiculous solution by accident. When videos reach the end of playback, each client emits the start event. This means that the slowest computers hold back the fastest computers, and get the chance to load the videos. This also means looping can be a very slightly jittery process (with each screen receiving 10 ‘start’ events), but as long as the first couple frames of the video are identical, nobody would even notice. Using this method, we have nearly perfectly synchronised video playback, and can play any video on any screen (meaning we can split a wide video into 10 segments, and each computer displays its respective part, all in sync with eachother) A disassembled Chromebook open to a test video We reached this stage within a month or two. Believe it or not, this project still had three years of work ahead of us. The biggest issue was Chromebook software. At this point, we had a website that we could manually open on each laptop to display a fullscreen synchronised video. Ideally, we would want this to be entirely automated, so that as soon as a Chromebook receives power, it boots up automagically to the c-sync client page. Unfortunately, right now, booting the Chromebook would just take you to a Google login page (and one that was locked to our school domainto boot). Also, just to add insult to injury, when batteries are removed, the laptops don’t turn themselves on when they receive power (you have to hold down the power button) This meant that our next step would have to be to replace ChromeOS with something else. The‘ChromeOS Firmware Recovery Script’is a magical piece of technology that somehow supports many different Chromebook motherboards. Ours was called ‘GLIMMER’. We just had to enter the built-in ‘Recovery Mode’, enable ‘Developer Mode’ and use the ChromeOS Shell to run the script. Now we’re basically on the home stretch. All we needed to do was pick up some stable Linux distro, write a hacky startup script that loads up Chromium and simulates the keystrokes to fullscreen the video and we’re done!  We ran in to two main issues: Some Chromebooks (roughly half of our working laptops) would refuse to enter developer mode due to the enterprise enrolment, and while we were able to get the other half onto a Linux distro, video playback would consistently freeze after some time (actually they would lock up entirely). It took us several months of on-and-off experimentation to figure out what to do. Essentially, the solution was to overwrite the entire default firmware withcoreboot(which is also possible using MrChromebox’s script). We just needed to remove the ‘Write Protection’ screw from each laptop motherboard, and this seemed to bypass the enrolment too. Lenovo’s handy Write Protection screw diagram Doing this for 20+ computers was slow and tedious. We only really needed the WiFi, motherboard and screen in working condition, but we decided to be (mostly) gentle and keep the laptops looking like laptops so that we had a keyboard and mouse for the rest of the installation steps. By the end, we got quite efficient at removing the write protection screw After ‘corebooting’ the Chromebooks, we were also pleasantly surprised to find out that ‘Wake on AC’ was a feature of the firmware, and that video playback no longer randomly breaks. By this point we had enough non-bricked Chromebooks left over for a line of 10 screens and a handful of spares. Now we’re really on the final stretch. Aksel worked on the mounting hardware, which you canread about on his blog, while I worked on figuring out a less flaky way to ‘boot to a webpage’ than the keystroke simulation and startup script I bodged together. I previously usedthe aptly named ‘FullPageOS’for a different project (which I briefly mention inmy TED talk, which you should watch), but it doesn’t run on x86 hardware. I landed on using ‘Porteus Kiosk’, which is just a minimal Linux distro that opens a fullscreen Chromium browser with all the correct flags for hands-off usage (eg. allowing video playback without user interaction)  This honestly worked totally fine, but left me unsatisfied for two reasons. Firstly, I didn’t like how we couldn’t customise the splash screen, so our project would be forever stamped with the Porteus logo on every startup (which would be every morning). And secondly, in search of a better issue to justify the extra work, I realised we can’t remotely do anything to the installations (eg. changing the page URL) without re-doing them, which would be definitely a problem once these get mounted on the wall. For those good reasons, I embarked on the journey of building ‘my own distro’ that we could install on the laptops. The system should start with something minimal (no desktop environment), and have an elegant script to autostart a kiosk mode Chromium instance. I first tried NixOS before quickly realising there was no way it would work with the tiny amount of storage on these Chromebooks (and it failed to install with every single attempt). Then I gave up, started with a Debian minimal install and just wrote a script that would provision a client (generate a ‘KIOSK_ID’, set its hostname tocsync-client-$KIOSK_ID, connect to the school’s WiFi, create users/permissions and set upopenboxto autostart a fullscreen kiosk mode Chromium). Then after attempting to repeat this on a second machine, I realised I would be wasting so much time (installing Debian is very ‘hands-on’ - you need to press lots of buttons), and I discovered ‘FAI - Fully Automatic Installation’ and the webFAI.metool. To cut a long story short[4], after redoing everything for the millionth time, I had a single USB that I can plug in to any ‘corebooted’ Chromebook which provisions it as a c-sync client. Woohoo! I also built out a ‘controller’ for c-sync which lets us manage connected clients and assign them videos.   After a successful three day stress test where the playback remained butter-smooth (and I sacrificed my ability to sleep for the greater good of testing with the backlight on), we were ready to mount these laptops on the wall.  The mounting is mostly Aksel’s thing, soI implore you to read his blog, but here are some cool photos from the process. (also aren’t our cable splices so pretty and not terrifying?? 😁❤️) An early iteration of the mounting backplate using a laser cut acrylic piece Aksel designed a pretty awesome looking backplate to mount the motherboard, which hangs on cleats on the walls. The displays are then held in place with clampy things. This is black magic to me. Everything laid out Preparing some displays and motherboards We decided to splice together power cables so that each power supply could power two computers. Send any complaints tothe pager on my contact page      Nearly there! After we painstakingly mounted everything, I realised something sort-of important. Computers generate heat. Somewhere along the way of wiping away the firmwares, the laptop fans stopped spinning, which meant things get quite hot quite quickly. I had to figure out a way to get those working again before we could comfortably leave this up 24/7 (well, actually 12/7). You can apparently interface with the‘ChromeOS Embedded Controller’using a tool calledectool, which should allow you to manually set fan speeds (among other things). The online documentation for this is lacking, and there’s apparently a slightly differentectoolfromcorebootand from Google directly. None of this made much sense at all to me, and no builtectoolbinary I could find would work. At some point, I found a dead link, butthanks to the magic of the Wayback Machine, I was able to get my hands on something that wouldn’t immediately crash. By some miracle, this version of the tool actually works perfectly fine at setting fan speeds, and after some testing, I found some goldilocks values that balance noise and temperature. As it turns out, making such a wide video is actually not easy. Each display has a resolution of 1366× 768, and very few pieces of software will let you edit a 13660 × 768 video. Final Cut Pro and Blender are the only programs we were able to do anything this wide in. Blender is one of the greatest pieces of software ever created (alongside c-sync) Then it’s just a matter of rendering the wide video and splitting it into 10 segments. Our video wall is imperfect. TN panel viewing angles suck, and the screens vary in colours and stuff. Yes, the synchronisation isn’tperfect, and yes, I’m sure there were better alternatives for nearly every decision we made along the way. Yet I love our video wall, despite how absurdly weird it is. It’s a perfect representation of the iterative design process and a true testament to teamwork and collaboration. We turned E-Waste into something interesting. And maybe, just maybe, the real video wall was the friends we made along the way.  This project was made possible by the incredible work of so many people. Aside from my collaborator Aksel Salmi, our Design teacher Daniel Bush played a huge role in guiding us through the project. Additionally, I wanted to thank thecoreboot projectand Matt ‘MrChromebox’ DeVillier for putting togetherthe firmware and toolsthat allowed any of this to work. I would also like to thank Thomas Lange of theFAI projectfor his help in building the FAI.me based automated installer that saved us so many many many hours, as well as his support over email. As silly as it sounds, this project was a backbone in my high-school experience. We hacked away at it every Monday for the past few years, and we grew up along the way too."
Passing the Buck: The story of the 2022 Wings Over Dallas air show collision,https://admiralcloudberg.medium.com/passing-the-buck-the-story-of-the-2022-wings-over-dallas-air-show-collision-9bbe5947297b,Hacker News,2025-03-01T19:47:23,Hacker News,https://images.unsplash.com/photo-1579403124614-197f69d8187b?q=80&w=3394&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Passing the Buck: The story of the 2022 Wings Over Dallas air show collision Admiral Cloudberg · Follow 59 min read · 1 day ago -- 7 Listen Share A Boeing B-17 and a Bell P-63F explode into shrapnel a split second after colliding in midair at the Wings Over Dallas air show. (David Walsh) On the 12th of November 2022, thousands of spectators at the Wings Over Dallas air show in Dallas, Texas bore witness to a sudden tragedy, as two WWII-era warplanes collided in midair during a performance, killing 6 crewmembers. In numerous spectator photographs and videos, a single-pilot Bell P-63 fighter could be seen arcing toward the Boeing B-17 “Texas Raiders,” its belly up in a steep left turn, until the two aircraft crossed paths, and in the blink of an eye the P-63 cleaved the larger bomber in two. Shaken by the loss inflicted on their tight-knit community, the air show’s volunteer-led organizers, the Commemorative Air Force, were left wondering whether the procedures used to prevent collisions during large formation flights might have some flaw that placed the two planes on a collision course. But the National Transportation Safety Board’s final report, and interviews with those involved, make clear that the problem was not so much that the procedures were flawed, but that no procedures existed. At Wings Over Dallas, the pilots of eight airborne aircraft, flying in close proximity to one another, were placed under the control of a so-called “air boss,” a role requiring only the bare minimum of qualifications, without prior knowledge of the maneuvers that the air boss would ask them to perform. Even worse, almost everyone involved seemed to think that this was normal — a remarkable example of what sociologist Diane Vaughan termed “normalization of deviance.” What follows is therefore not only the story of the disaster at Wings Over Dallas, but also the story of how the Commemorative Air Force, the Federal Aviation Administration, and the International Council of Air Shows created the circumstances for it to occur, all without recognizing that anything was amiss. _________________________________________________________________ Note: I am an outsider who has written a detailed story that is personal to many people. Several people are mentioned by name and I am aware of the possibility that they or their families might read this article. If you were involved or knew someone who was, and you notice incorrect biographical information, please send corrections to my publicly available email address. Thank you! _________________________________________________________________ ◊◊◊ A squadron of B-17s on a bombing run over Germany. (Stock photo) During the Second World War, American factories produced tens of thousands of warplanes to fuel the ferocious air battles over Europe and the Pacific, mass producing fighters, bombers, and transports on a scale matched neither before nor since. Untold thousands of these aircraft fell in battle, spiraling aflame from hostile skies, and an equal or even greater number were lost in accidents — destroyed in hard landings, smashed against Himalayan mountaintops, or lost at sea, never to be found. And after the war was won, even more of these aircraft met an unceremonious end at boneyards around the world, torn apart for scrap just as quickly as they were put together. But a precious few survived long enough to fall into the hands of people who saw them as more than merely machines. Among those who dedicate their time to salvaging, restoring, maintaining, and flying the relics of WWII, these aircraft came to be known as warbirds . Not every historic military plane was considered a warbird, because not every war was the war, the one nobody needed to name because it was obvious. In the decades since, that definition has slipped, but the association remains. A selection of CAF warbirds, including the B-17 Texas Raiders. (Commemorative Air Force) Although the number of true warbirds, in the original sense, can only ever go down, a surprising number remain airworthy thanks to the efforts of private individuals and non-profit organizations. The largest such organization is the Commemorative Air Force, or CAF, which today operates over 180 historic military aircraft distributed between 75 local units in 6 different countries. But the CAF wasn’t always so large. It got its start in 1957 when a small group of former US Air Force pilots purchased a single P-51 Mustang for fun, only to discover that no one else was trying to preserve decommissioned warbirds for future generations. The group then set out to acquire at least one example of every American warbird, and the rest is history. At first, the Texas-based CAF branded itself the “Confederate Air Force” as a joke, but later changed its name when the tainted association with the Confederate States of America started to interfere with its loftier goals and more serious outlook. At the same time, the CAF grew from a small, ad-hoc group into a larger organization with its own operations manuals, pilot training programs, and more. As a registered non-profit run mostly by volunteers, the CAF sustains its operations by charging membership fees in exchange for the opportunity to fly its airworthy warbirds, and by collecting 9% of the revenue from events put on by each of its local chapters, known as “wings.” Each aircraft is assigned to a wing that shoulders responsibility for operating that aircraft and training new members to fly or maintain it, with day-to-day use largely left at the local unit’s discretion, as long as CAF policies and procedures are followed. For accuracy’s sake, it should be noted that the warbirds are officially owned by the American Airpower Heritage Flying Museum — a legally separate entity from the CAF, which merely operates the aircraft — but the CEO of the CAF and the CEO of the AAHFM are the same person. Fighters in formation during the CAF’s Tora! Tora! Tora! performance, which is an approved maneuvers package. (Bart Marantz) Although CAF units and individual aircraft frequently participate in air shows hosted by third parties, the Commemorative Air Force itself also organizes several air shows each year, typically in the fall. As part of the CAF’s 2022 event roster, the group hosted two back-to-back air shows in Texas that will become relevant to this story, starting with Wings Over Houston on October 29–30, and Wings Over Dallas on November 11–12, both of which featured many of the same performances, aircraft, and pilots. Among the types of activities that took place at these two air shows, three merit further discussion — namely, revenue ride flights; approved maneuvers packages; and air boss-directed performances. Revenue ride flights are pretty much what they sound like, in that members of the public pay a specified amount of money in exchange for the opportunity to ride in a warbird, usually under the Federal Aviation Administration’s Living History Flight Exemption, which allows such flights as long as the “ticket” price is structured as a charitable donation. Such flights are an important source of revenue for the CAF and are critical to its financial stability. Normally they take place during gaps between other types of performances. Next, an approved maneuvers package refers to a scripted, choreographed performance usually involving close formation flying. The pilots who participate in an approved maneuvers package have practiced every move, every turn, and every power adjustment dozens or even hundreds of times, relying on strict timing and precisely defined flight paths. The participants in such a package are able to fly the entire performance in close proximity to other aircraft without receiving continuous instructions. When most people think of an air show performance involving multiple airplanes, this is usually what they imagine — for my American readers, the Blue Angels are a great example of a group that uses maneuvers packages; or for Canadians, the Snowbirds. A group of bombers flies in trail during the air boss directed performance at Wings Over Dallas 2022. This photo was taken moments before the collision. (Jason Noyes) The third and final type of activity, and the most important one for this story, is an air boss-directed performance. An air boss is a person who gives instructions to aircraft during an air show, such as who is cleared for takeoff, who is cleared to land, who should fly where, and so on, using a common frequency for all aircraft within the air show airspace. The air boss isn’t an air traffic controller and doesn’t need to have any air traffic control background — more on that later — but the job is similar in some respects. The most difficult part of an air boss’s job, however, is directing an unscripted performance. In an air boss-directed performance, the air boss develops a desired flight path for the aircraft involved in the performance and then instructs those aircraft and pilots to follow that flight path, without the use of an approved maneuvers package. The performance is usually constructed by stringing together basic maneuvers like low passes and course reversals that are familiar to the pilots, but the exact sequence of maneuvers is not practiced beforehand, and the positioning of each aircraft relies on the air boss’s directions rather than pre-arranged timings, bank angles, and power settings. The air boss’s role in a directed performance has been compared to that of an orchestra conductor. To understand this type of performance, it’s also important to clarify the exact definition of “formation flying” and its implications. In its report, the NTSB describes a formation flight as “two or more aircraft under the command of a flight lead that are flown solely with reference to another aircraft in the formation.” Under US regulations, if the aircraft are less than 500 feet apart, the pilots must possess a special formation flying certification. Conversely, aircraft that are not part of a formation must remain more than 500 feet apart, and while a string of aircraft each separated by this distance may appear to be a formation, it is not. This distinction will become important when analyzing what went wrong at Wings Over Dallas. The layout of the air show. (Own work, map and data courtesy NTSB) With all of that having been said, let’s jump forward in time to November 12th, 2022, the second day of the Wings Over Dallas air show at Dallas Executive Airport, a general aviation field located 12 kilometers southwest of downtown Dallas. We’ll go back and look at Wings Over Houston later. The performers in the November 12th activities started their day by attending a morning briefing held by air boss Russell Royce. Under the conditions imposed by the FAA waiver authorizing the air show, the briefing was a required item, and at least one member of every flight crew had to receive the briefing or face being cut from the roster. Also in attendance were an FAA inspector assigned to monitor the air show; another inspector being trained on air show operations; and a second air boss observing Royce in order to gain warbird experience. Using a PowerPoint presentation, Royce explained the boundaries of the air show airspace, landmarks that could be used to find the boundaries, where to go in the event of an emergency, and most importantly for this story, the location of the two “show lines.” Dallas Executive Airport has two crossing runways designated 17/35 and 13/31 respectively. Runway 13/31 has a northwest-southeast orientation and is the longer of the two runways, as a result of which it was selected as the axis along which the air show demonstrations would take place. The crowd was positioned on bleachers on the apron adjacent to the east side of runway 13/31, near the intersection with 17/35. To give the crowd the best view, aircraft would make passes down runway 13/31 along one of two designated “show lines” located at 500 feet (150m) and 1,000 feet (300m) in front of the crowd, respectively, with the 500-foot show line running down the eastern, inside edge of the runway and the 1,000-foot show line running along the tree line at the western edge of the airport’s clear area, as shown above. Which show line would be used by which aircraft, and when, was outside the scope of the briefing and would be assigned by the air boss during the performance. However, the briefing did cover techniques that could be used to identify and align with the show lines. Texas Raiders at Wings Over Houston in 2019. (Alan Wilson) At the briefing, the air boss also distributed a schedule listing the expected start times for each act. One of the acts that day was the CAF’s Tora! Tora! Tora! performance, an approved maneuvers package reenacting the 1941 Japanese attack on Pearl Harbor and the American counterattack, featuring multiple warbirds that were used in filming the 1970 movie of the same name, as well as coordinated pyrotechnics. As far as I have been able to tell, this is the only approved maneuvers package used by the CAF, and they perform it at numerous air shows every year. Following Tora! Tora! Tora!, the schedule called for a “warbird parade” featuring five American bombers and three American fighters, which would make passes back and forth in front of the crowd in a series of simulated bombing runs. At the finale of the performance, the warbirds would be joined by a Boeing B-29 Superfortress, one of only two remaining airworthy examples. The five bombers taking part in the parade were to be led by the B-17 Flying Fortress “Texas Raiders,” followed by the Consolidated B-24 Liberator “Diamond Lil,” a Curtiss-Wright SB2C Helldiver, and two North American B-25s named “Devil Dog” and “Yellow Rose.” The plan was for the bombers to fly “in trail,” with the B-17 receiving instructions from the air boss while the other four aircraft followed its lead, maintaining at least 500 feet of separation because the group was not officially a formation and the pilots were not required to be formation rated. At the same time, the three fighters were led by a North American P-51 Mustang, followed by a second Mustang and a Bell P-63F Kingcobra. The fighters would fly in close formation and all three pilots were formation-rated. Effectively, this divided the parade into two groups that not only had greatly different performance characteristics, but also different rules governing minimum separation distances and pilot qualifications. The five crewmembers of Texas Raiders and the pilot of the P-63F. (Commemorative Air Force) Because the accident involved the B-17 and the P-63F, we’re going to take a closer look at those aircraft and their crews before continuing. The Boeing B-17 Flying Fortress is a four-engine piston powered bomber designed to drop large amounts of unguided ordnance onto enemy targets. The model entered service with the US Army Air Corps, the predecessor to the Air Force, in 1936 and saw widespread action in WWII, as over 12,000 B-17s dropped 640,000 tons of explosives over Nazi Germany. At the time of the accident, there were approximately seven airworthy B-17s remaining, including the airframe nicknamed “Texas Raiders.” This B-17 was license-built by Douglas in Long Beach, California in 1944 and was delivered to the US Army Air Forces in July 1945 before being transferred to the Navy, where it served as an Airborne Warning and Command System (AWACS) platform in the Korean War. The aircraft was retired from the Navy in 1957 and spent ten years flying aerial surveying missions in Alaska until the CAF acquired it in 1967. The B-17 could be flown with a bare crew compliment of three, but for Wings Over Dallas, five crewmembers had been rostered. In command was 66-year-old Leonard “Len” Root, a longtime CAF member and recently retired American Airlines pilot with over 28,000 flight hours and type ratings in 12 different airplanes, including 500 hours on the B-17. He was also the former leader of his CAF wing and was responsible for training new members. The second in command was 67-year-old Terry Barker, also a former airline pilot, with over 25,300 flight hours and experience in everything from gliders to helicopters to large passenger jets. He had 90 hours in the B-17 and was the chief maintenance officer in the same CAF wing as Len Root. The other three crewmembers consisted of 64-year-old flight engineer Curt Rowe, a 30-year veteran of the Ohio Civil Air Patrol, and two “scanners.” The purpose of the scanners was to stand by the B-17’s rear doors and keep lookout for other aircraft, but there was no requirement that the scanners have a pilot’s license, or indeed any qualification at all. Normally only one scanner would be used, but apparently Len Root requested a second one. The first scanner in this case was 42-year-old Kevin “K5” Michels, a CAF historian and tour supervisor, while the second scanner was identified as 88-year-old Dan Ragan, a former US Navy radio operator who served on Texas Raiders during the Korean War. It’s not apparent from the available evidence whether giving him the scanner role was a pretext to allow him to keep flying on his former aircraft, but it if it was, it’s kind of hard to begrudge him. The sole surviving P-63F, seen in 2019 prior to its involvement in the accident. The second aircraft involved in the accident was the Bell P-63F. The P-63 Kingcobra family of single piston-engine fighters was developed originally for the US Army Air Forces around 1943, but the type never saw combat for the United States. Instead, around 3,300 were built mainly for the Soviet Air Force. Numerous variants were also developed, many of which did not see widespread use, including the P-63F, which was distinguished from the base model by its enlarged tail and modified engine. The P-63F project was abandoned after just two aircraft had been built, and the CAF operated the only surviving example. The pilot of the P-63F was 63-year-old United Airlines pilot Craig Hutain, a veteran aviator who flew his first airplane at 10 years old and had since accumulated a jaw-dropping 34,000 flying hours, including 108 in the P-63. He was also a member of and operations officer for the Tora! Tora! Tora! demonstration team, a fighter check pilot for the CAF, and an approved formation flyer with an endorsement for aerobatics. The CAF’s chief aviation officer summed him up with just six words: “That guy knew how to fly.” ◊◊◊ Several air bosses on the air boss platform at an air show. Note: none of these individuals were involved in the accident air show; this photo is representative only. (Dave Hadfield) The B-17 crew and the P-63 pilot were to play different roles in that day’s performance, and not only because they were flying different aircraft with different performance characteristics. Because the B-17 was the lead aircraft in the group of five bombers, its crew was responsible for the real-time interpretation of the air boss’s commands, while the other four bombers simply followed the B-17 at a safe distance. On the other hand, Craig Hutain in the P-63 was not the leader of his formation and would be focused primarily on staying close to the two P-51s ahead of him. The air boss’s instructions were relevant to him only if Royce commanded a change in the shape of the formation. Having said that, we also need meet air boss Russell Royce, the last major character in this story, before we continue. Royce’s age isn’t listed in any of the available documents but based on other statements he was probably about 38 years old at the time of the accident and held a day job working at an auto body repair shop. His father was Ralph Royce, a veteran air boss widely known throughout the American air show industry, and he grew up hopping from one air show to the next, shadowing his dad. Ralph first allowed Russell to issue instructions to aircraft at the age of 14 — albeit under close supervision — and by 18 years old he was directing entire air shows by himself. That’s not to say he was a child prodigy; in fact, in his interviews Royce denies that he was born with any particular talent, but he was undeniably very experienced. At this point you might be asking how it was legal for a teenager to direct an air show, and the answer is that there was absolutely no law saying he couldn’t. At the time that the younger Royce began air bossing, there were no formal requirements for the position at all, other than those imposed by the air show organizers. Over the years, Royce did acquire a private pilot’s license, and he briefly worked in an air traffic control tower at a general aviation airport from 2008 to 2009, but his aviation experience paled in comparison to that of the pilots he was directing. Unlike them, he had never flown for an airline, for the military, or in an air show; he wasn’t rated to fly multi-engine planes; and he had no formation or aerobatics experience. He had, however, air bossed over 300 air shows. Later in his career, new regulations began requiring air bosses to receive a letter of authorization (LOA) from the FAA in order to perform the role. To receive an LOA, an air boss had to complete the air boss recognition program run by the International Council of Air Shows Inc., or ICAS, an industry standardization and advocacy body. The LOA would remain valid for three years but could be renewed by meeting a minimum experience requirement and submitting letters of recommendation. To renew the highest level of LOA — “recognized air boss, multiple venues” — he was required to have directed 8 air shows in the last three years, submitted four letters of recommendation from other air bosses or credentialed air show performers, and attended at least one ICAS Air Boss Academy workshop. There was no requirement for recurrent training or a performance evaluation. Russell Royce held the highest level of LOA, but at the time of the accident the requirement was still so new that he had yet to receive his first renewal. Although Russell Royce wasn’t a member of the CAF, his father was previously CAF chairman and many CAF members were familiar with him. For Wings Over Dallas, he was hired by the Air Show Chairman, the CAF’s Gena Linebarger, who said she always hired Ralph and/or Russell Royce if they were available. The working relationship between the two dated back about 10 years and the younger Royce had air bossed Wings Over Dallas numerous times during that period. In fact, the CAF had also hired Ralph Royce to air boss Wings Over Houston two weeks before the incident, and according to the Wings Over Houston performers, Russell was present there, too. ◊◊◊ Flight paths leading up to the accident, part 1. (Own work, map by Google, based on NTSB data) Back at the morning briefing on November 12th, Royce wrapped up the proceedings after about 45 minutes, then asked the audience if they had any questions. Although Len Root wasn’t personally present, witnesses recalled that some other members of the B-17 crew asked clarifying questions, but no one could remember what they were. Subsequently, Royce went out to the air boss station, where he would guide the air show from atop a set of air stairs on a taxiway near runway 17/35. He was joined on the air stairs by the air boss observer, while the FAA inspector and trainee inspector stood nearby. After the start of the air show at 10:50, six acts flew before it came time for the warbird parade shortly after 13:00. The five bombers took off one after another from runway 31, followed by the fighters, with the B-17 departing at 13:10 and the P-63 at 13:15. After takeoff, each aircraft was given initial maneuvering instructions while waiting for the remaining aircraft to join them in the formation or group; in the B-17’s case, the air boss instructed, “It will be a right turn. You’re looking for a thousand feet to enter from over the crowd.” The B-17 crew acknowledged and complied, making a sweeping 270-degree right turn after takeoff to come back over the field from the east, from behind the crowd. By this point the two B-25s were already in the air finishing up the previous act, so the air boss instructed them to fall in behind the B-17 and the SB2C helldiver, which was also airborne. Flight paths leading up to the accident, part 2. (Own work, map by Google, based on NTSB data) At 13:12, the fighters reported ready to go. Royce instructed the B-17 to come over the crowd then make another right 270 onto the 1,000 foot show line, then cleared three fighters for takeoff, followed by a right turn. At the same time, Royce shot instructions to the pyrotechnic team and to the pilot of a Beechcraft T-34B Mentor that was conducting a revenue ride flight during the performance. Moments later at 13:14, with the B-17 completing the right 270 to line up for its first pass down the runway, Royce instructed them to follow that pass with a right 90 degree turn, followed by a left 270 degree turn — a course reversal technique known as a “dog bone” or “duster turn.” This maneuver would be used several times during the performance in order to reverse the direction of each consecutive pass down the show lines. Turning to the fighters, Royce instructed them to “get formed up behind the crowd and I’ll bring you overhead in just a moment.” Following the lead P-51, the three fighters entered into a close formation, completing a right 180-degree turn followed by a left 270, rolling out heading west toward the back side of the crowd just as the B-17 had done a few minutes earlier. Meanwhile, the B-17 completed its first pass in front of the crowd and started the dog bone course reversal, to which Royce commented, “Perfect, and then you can come right down the runway and the fighters are going to pick you up.” He then instructed the bomber group, which was now fully in trail, to enter a left racetrack pattern — that is, flying in circles with left turns — after completing their current pass, in order to wait for the fighters, who he said would join them “overhead,” without specifying an altitude. By this point the bombers were distributed between 500 and 800 feet, while the fighters were staggered from about 900 to 1,300 feet, with the lead P-51 on top and the P-63 on the bottom. If that all sounded complicated, that’s because it was, but so far all communications were working as intended. You can use the flight path maps above and below to follow along. Flight paths leading up to the accident, part 3. (Own work, map by Google, based on NTSB data) After confirming that the lead P-51 had the bomber group in sight, Royce instructed them to join the bombers in the racetrack, then told the bombers that after their current circuit they would complete make a second “dog bone” turn with a “left 90 and right 270.” The fighters then caught up to the bombers, made a right 270 to follow them through the racetrack, and then both groups proceeded together toward the dog bone described by the air boss. At 13:19, as the bombers and fighters were completing the dog bone, preparing for a north-south pass in front of the crowd, Royce set them up for yet another course reversal at the other end, instructing, “B-17, after this pass, right 90 left 270.” “Raiders, right dog bone,” the B-17 replied. Turning to the fighter group, Royce then said, “Fighters, walk your way up to the B-17, I’m going to break y’all out after this — um, you’re going to end up breaking left.” In formation flying, a “break” occurs when each aircraft successively makes a sharp turn to exit the formation. In hindsight, what Royce wanted the fighters to do was to cut short the left 270 in the next dog bone turn by breaking hard to the left, cutting inside the bombers to get out in front. What he meant by “walk your way up” is unclear. Flight paths leading up to the accident, part 4. (Own work, map by Google, based on NTSB data) Without waiting for a response from the fighters, Royce continued, “So you’re going to follow the bombers to the right 90 out and then you’re going to roll back in left and be on the 500 foot line if y’all want to set up an echelon for a break so y’all can get in trail.” The first part of this crucial transmission was an attempt to clarify what I already explained — that after making the right 90 portion of the dog bone turn, the fighters would make a sharp left. He also added that after making the left turn, they would align with the 500 foot show line, closest to the crowd. However, the last part of the transmission might have been unclear in the moment. In formation flying, an echelon refers to a staggered formation where each aircraft is offset from the next to either the left or right, like migrating geese. An echelon formation is an ideal starting point for a break. After the break, Royce wanted the aircraft to get in trail, one behind the other. But that was a lot of information to take in at once. Clearly confused by the lengthy transmission, the lead P-51 pilot radioed, “Okay, uh — say again for the fighters. That was not clear.” But instead of repeating his instructions, Royce responded with something totally different: “Uh fighters, go uh — right, go to echelon right.” But this transmission only made matters worse, because the fighters hadn’t even straightened out for the pass in front of the crowd yet, let alone begun the next dog bone. The next turn was a hard right, but making a hard right turn while in a right echelon formation, with each aircraft offset right from the aircraft ahead of it, is risky because the lead aircraft has to turn across the face of the aircraft behind it. In hindsight, Royce was a step ahead; he wanted them to assume the right echelon in preparation for the left break after the right 90 had already been completed. But in the heat of the moment, this would have been difficult for the fighter pilots to understand, and indeed the fighters responded not by forming an echelon, but by getting in trail, one behind the other. Why it doesn’t make sense to cut the fighters inside to the 500 foot line. (Own work) Meanwhile, as the lead bomber proceeded southbound past the crowd, Royce noticed that the gap between the B-17 and the B-24 behind it was larger than he would like, so he said, “Okay B-24, if you could give me a couple of mi — uh, inches and close the gap I’d appreciate it. B-17, let’s keep a little flat for me.” It wasn’t entirely clear what Royce meant when he asked to keep the turn a little flat, but he probably wanted the B-17 to lead the bomber formation out wide, using shallow bank angles to extend the distance traveled. This would give the fighters more time to cut the corner. However, this is known only in hindsight, because Royce had not yet explained that he wanted the fighters to move in front of the bombers, and it’s doubtful that any of the pilots had so far guessed that that was his intention. In response to the transmission, one of the bombers made an unintelligible reply, and then Royce added, “Just a little bit. When you come back through you’re coming through on the thousand foot line.” This was another crucial moment in the sequence of events — perhaps even the most crucial. Royce had now set up the next pass with the bombers on the 1,000 foot show line and the fighters on the 500 foot show line. If the fighters completed the dog bone by flying outside the bombers, that would be fine, but if they made a tight turn inside the bombers, the two formations would cross paths. At this point, the bombers were distributed between 550 and 680 feet altitude, with the B-17 occupying the highest position, while the fighter formation was distributed from 960 feet to 1,100 feet, with the P-63 in the lowest position. Then, at 13:20 and 37 seconds, Royce hammered another nail into the coffin: “American fighters should be in a right turn,” he said. “You’re gonna follow the bombers out on a right 90 and then I’m going to roll you back in front of them.” Finally, Royce had articulated his intention for the fighters to undertake the bombers. As I just explained, compliance required the fighters to turn inside the bombers, which would cause the two groups’ flight paths to cross as they lined up with their assigned show lines. To make matters worse, the way the dog bone turns were typically flown was that every aircraft would climb during the right 90, level off on the outside of the turn, then descend in toward the show lines to simulate a diving run from the crowd’s perspective. This meant that the altitudes of all the aircraft were variable during the dog bone, and just because the lowest fighter was above the highest bomber at the time the instruction was given didn’t mean that that would necessarily remain the case. Furthermore, if the fighters were to overtake the bombers, then the B-17 was the primary obstacle that they needed to pass — but the older, less powerful B-17 was slower than the other bombers and thus made tighter turns in order to prevent the bombers behind from catching up. That would make it harder for the fighters to overtake the bombers on the inside, because the B-17 was already cutting inside on every turn anyway. From here on out, the NTSB’s own diagrams of the flight paths are an effective companion to the text. This diagram shows the positions of all aircraft at 13:21:08, the time that the lead P-51 said “We see the B-17.” (NTSB) At this point, the stage was nearly set for disaster. All that remained was for the deadly dance to play out. As the fighters and bombers proceeded into the dog bone maneuver, Royce turned his attention to another aircraft entirely: a Boeing-Stearman PT-17 operating a revenue ride flight. The two-seater, WWII-era training biplane, commonly referred to as the “Stearman,” was carrying one pilot and one paying passenger on approach to runway 31, the very same runway along which the warbirds were to make their next pass. But Royce judged that no conflict existed, so he transmitted, using the Stearman’s callsign, “Quebec, I need you to drop it down to the deck. Runway three one clear to land.” “Down to the deck, five eight Quebec,” the Stearman pilot acknowledged. Watching the B-17 lead the bombers through the dog bone, Royce said, “There ya go B-17, yup, gentle flat roll it around thousand foot line.” “Thousand foot line for raiders,” said the B-17. “Fighters, roll it back to the left,” Royce then added. “Lead, fighter lead, roll it back to the left and get y’all in trail.” “Okay, fighters in trail,” the P-51 acknowledged. At this point the fighters had completed the right 90 and had caught up with most of the bombers. The formation was positioned directly overhead the B-24, the second aircraft in the bomber group, but had not yet caught up with the B-17, which had already started the left 270 and was turning inbound. “Yeah, and gunfighter, look out your left side and find the B-17,” Royce instructed, using an alternate callsign for the lead P-51. Although the fighters were turning inside the bombers, the B-17 was in the left hemisphere from the fighters’ perspective because it was already inbound and the fighters were still flying outbound. “We see the 17,” said the P-51. As the fighters started a sharp left turn, Royce repeated, “Yeah there you go. Roll it back to the left. I want you to get in front of the bombers. I want you to come through on the outside edge of the runway.” This instruction might have caused yet more confusion because the outside edge of the runway, from the crowd’s perspective, was the west edge, which was in between the 500 and 1,000 foot show lines. The 500 foot show line, which was where he wanted the fighters to fly, should have been referred to as the inside edge because it was closer to the crowd. In any case, without asking for clarification, the lead P-51 replied, “Okay.” The positions of all aircraft at 13:21:45, 10 seconds before the crash and 3 seconds before the P-63 strayed to the right. (NTSB) The altitudes of the two groups now overlapped, with the fighters at 700, 340, and 520 feet respectively, while the first four bombers were at 430, 600, 450, and 670 feet respectively. Watching from the apron, Royce said, “Nice job fighters, you’re coming through first. That will work out. B-17 and all the bombers on the thousand foot line.” While the two P-51s had passed the B-17 at the time of the transmission and were indeed in front of the bombers, the P-63 was not. And at its controls, Craig Hutain was running out of time to overtake the B-17 before they were forced to cross paths to align with their respective show lines. Nor did anyone know for sure that Hutain had the B-17 in sight, because Royce never asked him — he only asked the formation lead. Just to double check, Royce asked, “B-17, you got the fighters in front of you off your left?” The B-17’s reply is described as “unintelligible” in the transcript but it appears to have been an affirmative statement. However, at the time of that transmission, only two of the three fighters were actually “in front of” the B-17; the P-63 was still technically behind. From the captain’s seat, Len Root would have had to look back over his left shoulder into his 7 or 8 o’clock position to see it. Watching the maneuver play out, Royce said, “Nice job fighters, come on through.” Moving on to the next maneuver after the current pass was complete, he then added, “Fighters will be a big pull up and to the right.” But back in the fighter formation, confusion was evident. The two lead P-51s didn’t proceed onto the 500 foot show line as instructed, but lined up with the 1,000 foot show line instead. At the same time, however, Craig Hutain’s P-63 swung farther out to the right, heading toward the 500 foot show line — and directly into the path of the B-17. Watch bystander video of the collision courtesy of Morgan Curry and NBC 6 South Florida. Caution: Viewer discretion advised. With his low-wing fighter banked in excess of 45 degrees to the left, and with the B-17 below him and to his right, there was no way for Hutain to see it coming. Moving left-to-right across the bomber’s path from behind and above, the P-63 plowed into the B-17 amidships, cleaving it in two. As hundreds of people watched in shock and alarm, the P-63 disappeared into a chaotic hail of metal while the bomber broke in half just aft of the wings, ripping away the roof and cabin walls all the way forward to the cockpit. The crew of the B-17 barely had time to brace before what remained of their aircraft pitched forward into the ground and exploded in flames. More burning pieces of both aircraft strafed the grass next to the threshold of runway 31, narrowly missing the Stearman, which had touched down just seconds earlier. Instantly, Russell Royce activated the pre-briefed emergency procedure by shouting, “Knock it off, knock it off! Roll the trucks, roll the trucks, roll the trucks, knock it off, roll the trucks!” Emergency crews on standby near the runway rushed to the scene and began spraying down the burning wreckage, but little remained of either aircraft. It was clear that everyone on both airplanes had perished. A photographer in the audience captured this series of high-definition photos of the aircraft before, during, and after the collision. (Gary Daniels via NTSB) In the air, the remaining aircraft proceeded to their pre-planned holding points, where they waited while deciding where to divert. Not all of the pilots had seen the crash, and many of those who did were left confused by what they had witnessed, but everyone knew that a tragedy had taken place. Many of the surviving pilots had been friends with the victims for years, even decades. But only after getting their own planes back on the ground could they allow the shock to set in. Len Root, Terry Barker, Craig Hutain, Curt Rowe, Kevin Michels, and Dan Fagan — in the CAF, almost everyone knew their names. Some were lifelong pilots, airline captains, war veterans; others just wanted to teach military aviation history, but all had dedicated a portion of their lives to the warbird community, and that their lives were taken by the very objects of their passion did not provide comfort, and in fact only heightened the sense of loss. And after the victims were mourned and buried, the warbird community would also quietly mourn the aircraft themselves, saying goodbye to the shattered remnants of the only extant P-63F, and ticking one more name off the short and shrinking list of airworthy B-17s. So what was it all for? What was the reason for such a waste of life and history? ◊◊◊ Another photographer captured this alternate angle of the collision. (Dylan Phelps via NTSB) In search of the answer to that question, the National Transportation Safety Board conducted numerous interviews, pored over countless pages of documentation, reviewed dozens of photos and videos, and conducted multiple studies. In addition to their final report, the NTSB assembled a docket of evidence 1,896 pages long, which you don’t have to read, because I’ve read it for you. On its most fundamental level, any mid-air collision is the result of a breakdown in the methods used to keep aircraft separated. Those methods can be visual — in other words, pilots keeping each other in sight — and they can be procedural, such as when an air traffic controller assigns two planes to different altitudes. In general, procedural separation is safer because it relies less on human perception; that is to say, the probability that a pilot will incorrectly follow directions, or that bad directions will be issued, is lower than the probability that two pilots will not see each other, especially in conditions of restricted visibility. However, if the pilots already have each other in sight, then maintaining visual separation is perfectly adequate. You’ll hear a lot more from me about the minutia of these distinctions in a couple years when I write about the Potomac River midair collision, but for today’s story, this is enough. One of the key elements required to understand the Wings Over Dallas disaster was the type of separation in use. In an approved maneuvers package, separation was ensured by following a rigorously practiced choreography, but in an air boss directed performance, that wasn’t the case. So, the NTSB did the logical thing and asked air boss Russell Royce how he normally kept airplanes apart and what tools were available for him to do so. Royce explained that he didn’t have radar or any other flight tracking software and that he judged the position of each airplane visually and based on experience. He added that as in all modes of aviation, “visual is the rule of the road,” meaning that he expected pilots to see and avoid each other if a conflict were to develop. However, he also listed three other types of separation; namely, vertical, lateral, and time-based. These are fundamental techniques for procedural separation known to every air traffic controller. But after reviewing the sequence of events, a glaring question arose: did Royce actually use them? An explosion and falling debris are visible just after the B-17 impacted the ground. (Nathaniel Ross via New York Times) After interviewing numerous people who were present at the briefing led by Royce on the morning of the accident, it was evident that there was no discussion of assigned altitudes or show lines prior to the start of the performance. In fact, the only altitude anyone could recall being mentioned was the height that would give the crowd the best photo opportunities during the photo pass at the end of the performance. Instead, interviewees stated that for an air boss directed performance, the exact choreography to be used would not normally be decided at the briefing, but would be determined by the air boss during the performance. Agreement on this matter was nearly universal among attendees, who all said that the briefing was standard, resembled other briefings, and did not appear to be missing any information that they expected to hear. Furthermore, while FAA guidance for air shows stated that a briefing had to take place, the NTSB could find no official document specifying what means of separation were to be used in an air boss directed performance, nor any document requiring that such means be discussed in the briefing. In fact, the FAA inspector in charge of monitoring the air show was at the briefing and reported nothing unusual about it. Once the performance was underway, Russell Royce could have assigned the bombers and fighters to different altitudes, but the transcript of the air boss frequency revealed that he never did so. While he did assign an altitude to the bombers during one particular pass, the assignment was intended to achieve the desired visual effect for the crowd, and not to provide separation from the fighters, which hadn’t taken off yet at the time. Throughout the remainder of the performance, the bombers generally flew at 200 to 500 feet and then climbed to altitudes generally lower than 1,000 feet; while at the same time, the fighters generally flew between 800 and 1,300 feet, except during the accident pass, where they descended to between 100 and 600 feet. However, these altitudes appeared to be chosen by each individual crew without coordinating with other crews or with the air boss. In fact, when the NTSB asked whether there were any limits on how high or low the performers could climb or descend, Royce replied that the maximum altitude was the ceiling of the air show airspace and the minimum altitude was the ground. However, Royce didn’t stop there. Despite listing altitude as a means of separation at his disposal, in his interviews he stated that as a matter of practice he rarely assigned altitudes because he wanted the pilots to look outside and maintain awareness of other aircraft rather than looking inside at their altimeters. In fact, he asserted that assigning altitudes is actively dangerous and only serves as a distraction. Nevertheless, he professed a belief that the fighters had an obligation to be “on top of the bombers” and that he expected the fighters to remain above the bombers as they maneuvered to the 500 foot show line during the accident dog bone. He did not appear to show any awareness of the fact that the lowest fighter was below the highest bomber at numerous points throughout the entire performance, nor did he explain why, if such an obligation existed, it was neither reviewed in the briefing nor specified in any regulation. Furthermore, in a separate interview he expressed an expectation that the bombers would be at 200 feet on the 1,000 foot show line, and that the fighters would be at about the same altitude on the 500 foot show line — which is exactly what both groups actually did — without explaining how this was supposed to be reconciled with his expectation that the fighters would cross over the top of the bombers. Nor did he explain how the pilots were supposed to know his intentions with regard to altitudes when he never mentioned altitudes on the radio. The inevitable conclusion to be drawn from this mess of answers is that altitude separation was not being used. But what about other types? Smoke rises from the crash site seconds after the accident. (NBC News) The next type of separation listed by Royce was lateral separation. In the context of an air boss directed performance, lateral separation could mean assigning the bombers to the 1,000 foot show line and the fighters to the 500 foot show line prior to the start of the performance as part of a racetrack pattern in which the faster fighters would fly a longer circuit around the outside of the bombers. However, Royce only assigned show lines on the fly, based on his assessment of where the two groups were. As a result, while he did send the bombers to the 1,000 foot show line and the fighters to the 500 foot show line on every pass, he did so without putting in place any means to ensure that the fighters flew outside the bombers’ circuit. If the fighters were cutting inside the circuit, then they would have to cross paths with the bombers to reach the 500 foot show line. The fact that Royce not only didn’t ensure that the fighters were orbiting outside the bombers, but actually ordered the fighters to cut inside the bombers’ orbit to reach their show line, indicates that he was not considering the use of different show lines as a means of lateral separation. Bizarrely, in his interview Royce denied that he told the two groups to cross paths, arguing that he told the fighters to go up and to the right while the bombers were flying down and to the left, and thus away from each other. But not only did he not actually tell either group to fly “up” or “down,” it should be self-evident that telling the fighters to fly right and the bombers to fly left when the fighters are left of the bombers requires them to cross paths! When confronted further on this matter, he placed all responsibility for avoiding cross-track maneuvers onto the lead pilots, simply stating, “I don’t put constraints on them.” But while it was true that the fighters were located outside the bombers’ circuit at the time he first instructed the fighters to “get in front of the bombers,” he then told the fighters to cut the corner, forcing them to cross inside the bombers’ flight path. It is unclear how the fighter pilots were supposed to comply with this directive without crossing paths with the bombers. So much for “not putting constraints on them!” An aerial view shows the B-17’s tail in the foreground, main B-17 wreckage in left background, and P-63 wreckage in right background. (NTSB) Elaborating on his views, Royce went on to explain that keeping the airplanes a certain distance apart wasn’t really his job at all, because the minimum safe separation between aircraft would depend on pilot experience, aircraft capabilities, and the moment-to-moment operating environment. However, this response was factually incorrect, because for aircraft not part of a formation, there was a legally imposed minimum separation distance of 500 feet. Any closer and a formation flying certificate would be required. In his interviews, Royce did not appear to understand that the bomber group was not a formation and that the bomber pilots were not necessarily required to have a formation rating, and thus that there was an obligation to keep the fighter formation at least 500 feet away from the bomber group at all times. In fact, throughout his interviews Royce repeatedly referred to the fighters and bombers together as a single formation, or as separate formations, even though neither was the case. At this point it should be clear that Royce didn’t view separation as his responsibility, even when a certain amount of separation was required by regulations. That’s why the NTSB pointedly asked who he thought was responsible for separation, if not the air boss? In one instance, Royce replied that the answer was “everyone.” Describing his own role, he explained that he assisted in separation “from the perspective of the crowd,” but it sounds like he meant keeping aircraft separate in the crowd’s field of view so that no aircraft was hidden behind another. Later, he explained that official documentation doesn’t address deconfliction, so any obligation to separate aircraft would depend on whether the pilots’ contract says separation has to be done a certain way, or based on common sense — “I’m in front of you, I shall go first,” as he put it. And in another instance, he said that avoiding each other was the pilots’ job — “I do a lot of assignment of responsibility.” So in essence, no systematic deconfliction practices existed; the air boss gave little thought to separation in his commands; and pilots were expected to utilize the principle of “see and avoid” when complying with his instructions. Another aerial view looking back toward the point of the collision. (NTSB) This conclusion necessarily raises two more questions: were the aircraft complying with Royce’s instructions when they collided, and was it possible for them to have seen each other? Regarding the first question, Royce seemed to think that the answer was no. During one interview, he said that crossing one group over another wasn’t an issue — “we do it all the time… it’s never a problem” — but that the P-63 wasn’t where it was supposed to be. However, in a separate interview, when asked whether he perceived the P-63 to be in the correct position before the crash, he complained that that was “not a valid question.” With no way of knowing what Craig Hutain was actually thinking as he executed the final maneuver, it isn’t possible to say whether he understood the instructions correctly or not. In general, the air boss only issued instructions to the lead aircraft in each group or formation, and the other aircraft were expected to follow them. But in interviews with the NTSB, the pilots of both P-51s stated that they believed the air boss had told them to go to the 1,000 foot show line — which is what they did — even though this was incorrect. Then, as the P-51s maneuvered to align with the 1,000 foot show line, the P-63 did not follow them, but rather swung wider, apparently toward the 500 foot show line, which is where Royce had actually told them to go. Most pilots in the performance stated that when flying in trail, their focus was on following the aircraft ahead of them, not on the air boss’s instructions, which would appear to cast doubt on the notion that Hutain broke out of the formation on purpose. But those who knew Hutain believed otherwise. When asked about Craig, his longtime friend and colleague Jim Lasche said, “ I can just imagine him, he’s told ‘be at the 500 foot line now,’ that’s where he’s going to go.” This NTSB diagram of the flight paths leading up to the point of impact shows how the P-63 swung slightly wide toward the 500 foot show line. It’s also worth noting that around the time Hutain moved toward the 500 foot show line, Royce said, “Nice job fighters, you’re coming through first, that will work out.” However, because at that moment both the B-17 and the fighters were headed directly toward him, Royce wasn’t actually in a position to judge whether the P-63 was ahead of or behind the B-17. If Hutain didn’t have the B-17 in sight, then this unfounded comment by Royce could have added to his belief that he was clear to cut over to the 500 foot show line. With no disagreement about the fact that the B-17 was right where it was supposed to be, and a reasonable likelihood that the P-63 pilot was also trying to get to his assigned show line, it appears in hindsight that both aircraft were probably complying with the air boss’s directives when they collided. Despite this, Royce insisted in his interviews that his directives were normal and that pilot compliance represented the biggest safety issue. Because there was no time for every aircraft to read back his commands in a dynamic, fast-changing environment, he had to trust that everyone understood his instructions correctly, and the only way to verify comprehension was compliance. Furthermore, in his view, because he had asked the fighters to “get in front of the bombers,” the fact that the P-63 moved to the 500 foot show line without getting in front of the B-17 amounted to non-compliance. But if we think about it in another way, that sounds an awful lot like a fancier way of saying, “If I tell you not to hit each other, and you hit each other anyway, that’s your fault because I told you not to.” It’s a complete abdication of responsibility and a violation of the pilots’ trust. In this image from the NTSB’s visibility study, it’s clear that the B-17 wasn’t easy for the P-63 pilot to see. In addition to being a copout, this philosophy is also problematic because, as I stated earlier, visual separation is less effective than procedural separation unless definitive visual contact has already been made. Also, only the lead fighter was asked to confirm the B-17 was in sight; no one asked the P-63. And when Royce asked whether the B-17 had the fighters in sight, he described the fighters as being “in front,” when the P-63 was actually behind, so when the B-17 replied in the affirmative, there was no way to know whether Len Root had looked back to find the third fighter or whether he only saw the P-51s. So could the two crews actually see each other? To find out, the NTSB conducted a visibility study using a flight simulator. What they found was that when the air boss asked the B-17 if the fighters were in sight, the P-63 was visible through the rear left side window, over the captain’s left shoulder; but after 5 seconds it became obscured by the captain’s window pillar until the collision. Under the circumstances, it was highly unlikely that either of the B-17 pilots would have seen the P-63 coming. Furthermore, it would not have been obvious that the P-63 was on a collision course until about 7 seconds before the crash, when it appeared to break formation with the other fighters. That was less than the time it would take for one of the two scanners, who were not current and qualified pilots, to spot the incoming aircraft, determine that it was a threat, tell the pilots, and for the pilots to take evasive action. Meanwhile, starting from the time that the air boss asked the fighters whether they could see the B-17, the latter was visible to the P-63 pilot for 16 continuous seconds near the lower right side of the canopy. The B-17 only became obscured by the P-63’s upturned right wing about four seconds before the crash. However, Craig Hutain’s focus at that point was probably ahead and to the left as he attempted to follow his lead plane and find his assigned show line. Furthermore, the B-17 was still wearing its original military colors — that is to say, olive drab camouflage. This paint scheme was specifically chosen in order to make it harder for enemy fighters to spot the bomber from above against a background of trees, buildings, and earth. Hutain wasn’t an enemy but the color scheme doesn’t discriminate; as far the paint knew, it was doing its job. The NTSB concluded that while it was theoretically possible for Hutain to have seen the B-17 if he focused on keeping it in sight, there were also good reasons why he might have glanced over where he thought it was, seen nothing, and decided that he was in the clear. As for the slower, less maneuverable B-17, their chances of avoiding the collision were almost nil. But these findings merely confirmed decades of NTSB research into the dangers of relying on visual separation, especially in complex maneuvering environments. So the discovery that “see and avoid” was inadequate for air show deconfliction was hardly a surprise to anyone who had been paying attention. Most of the wreckage of the P-63 came to rest in this area. The engine block and nose landing gear can be seen separated from the main portion of the aircraft. (NTSB) To summarize, then, the air boss wasn’t using procedural separation techniques to deconflict aircraft and expected pilots to avoid hitting each other no matter what directives he gave them. If this attitude strikes you as unreasonable, you wouldn’t be alone. In fact, several people who were interviewed by the NTSB expressed displeasure with Russell Royce­ — and even more with his dad. Among interviewees, there was widespread agreement that Ralph and Russell Royce possessed a shared approach to air bossing, whether they liked that approach or not. And in fact, quite a few people came out of the woodwork to mention previous close calls involving both air bosses, including just two weeks earlier at Wings Over Houston, under the direction of Royce senior. One of those people was the Vice Chairman of Wings Over Houston, a CAF pilot who flew as copilot of a bomber in an air boss directed performance during that air show. According to his recollection, Ralph Royce ordered Craig Hutain’s P-63 to cut in between his aircraft and the bomber ahead of him with no altitude separation. The pilot in command of his bomber confirmed that the incident had taken place and added that the P-63 came within 100 feet of his aircraft. According to both the Wings Over Houston Vice Chairman and CAF Chief Aviation Officer Jim Lasche, several pilots were so incensed at the incident that they met after the air show to commiserate, and one of them even confronted Ralph Royce and told him that he was “going to kill somebody if you do this again.” But the confrontation seemed to result in more ass-covering than soul-searching. In fact, according to the Wings Over Houston Vice Chairman, Royce senior tracked him down a month after the accident and asked him what he had told the NTSB about the Wings Over Houston incident, then insisted to him that the P-63 wasn’t at his altitude and that the maneuver was perfectly safe. After that conversation, the Vice Chairman said that he was bothered by the senior Royce’s dismissiveness of his concerns and his insistence that he disbelieve his own eyes. This wasn’t the only alarming incident described in the NTSB testimony either. According to the Wings Over Houston Vice Chairman, he had previously experienced an event in which Royce senior ordered him and another aircraft to turn into the same show line at the same time from opposite directions; realizing that this would put him on a collision course, he refused to comply. And in yet another incident, also at the air show in Houston two weeks before the crash, the pilot of a C-47 told the NTSB that the air boss sent a Messerschmitt Me-262 head-on into his aircraft on the show line, causing the replica German jet fighter to pass directly beneath him when he was only 200 feet above the ground. He was unaware that the Me-262 would do this until the air boss ordered it; the maneuver was not briefed beforehand; and it was not debriefed after. A closer aerial view of the B-17 main wreckage area. (NTSB) In order to get a second opinion, the NTSB interviewed several people with decades of air show experience who were not involved in Wings Over Dallas, including two air bosses who regularly directed other major air shows. Both of those air bosses stated that they normally determined the choreography for an air boss-directed performance in advance of the briefing and would assign altitudes and show lines to ensure separation between aircraft in different groups or formations. The typical minimum separation between aircraft was 200 feet and they did not normally allow aircraft with different cruising speeds to fly together. The air bosses said they were expected to carry out the pre-planned choreography without deviation. The Wings Over Houston Vice Chairman agreed that other air bosses didn’t use “freeform” directives as often as the Royces did, and said that he normally expected the fighters to remain above the bombers using pre-briefed assigned altitudes. Separately, the CAF Director of Operations said that it was a “mistake” to send the fighters past the bombers without 500 feet of separation, describing Russell Royce’s directive as “not the right way to do it.” “I would have thought it would have been intuitively obvious to the most casual observer that you don’t do that sort of thing, but apparently it’s not,” he said to the NTSB. And regarding the absence of a planned choreography, he opined that if a “new guy” were to attend the briefing, he would probably come away knowing only to “follow the guy in front of him and hope nothing happened.” Without a moment’s pause, he added, “And that to me is not right.” Finally, joining the chorus of voices, CAF Chief Aviation Officer Jim Lasche said that Royce was wrong to avoid assigning altitudes, because, as he put it, when a fighter is flying along at 250 knots and the pilot hears confusing directions, there needs to be something for them to fall back on, like an assigned altitude and show line — but there wasn’t. None of the interviewees made much mention of the fact that Royce also sometimes used unclear language and terminology, like “walk your way up” and “keep it a little flat.” Royce himself was unable to explain exactly what he meant when he used some of these phrases. However, it was agreed by both the NTSB and other experts whose commentary I reviewed that the use of this kind of language in a highly dynamic environment can and did cause confusion. When airline pilots and controllers talk on the radio, they’re expected to use certain established terminology so that the intent of a statement is as unambiguous as possible — so why shouldn’t we expect the same from an air boss? Most of the B-17 was destroyed in the impact and fire. (NBC News) And yet, despite criticism from these authoritative voices, the majority of interviewees spoke positively or neutrally of Russell Royce and his air bossing techniques. For instance, the pilot of the B-24 immediately behind the B-17 said that Royce didn’t do things particularly differently from other air bosses. The pilot of the second P-51, himself an accredited air boss and warbird safety researcher, said that Russell was the best in the business. The FAA inspector and the trainee inspector both said they observed nothing unusual or incorrect about Royce’s air bossing on the day of the accident. And the air boss observer shadowing Royce praised his ability and took his side on the question of separation, arguing that pilots always shoulder the primary responsibility for knowing where the other aircraft are, and noting that when he was in the Air Force flying jets, the obligation to maintain that awareness didn’t go away just because he was in a high G-maneuver with his belly up to someone else. But he also added that performers expect the air boss “to be ahead of the game, to understand what’s going to happen so as to avoid conflicts down the road,” and it’s not clear how to reconcile this statement with Royce’s actual performance. The most important voice of support among the interviewees was Wings Over Dallas air show Chairman Gena Linebarger, who was responsible for selecting the air boss. She described Royce as a “perfectionist,” said she liked the way he did things, and explained that she always hired the Royces if she could. Her relationship with them dated back 10 years and included numerous air shows. In fact, her confidence in Russell was so great that she told the NTSB, months after the accident, that the only reason she hadn’t hired him for her next air show was because their insurance wouldn’t let her. The clear takeaway here is that the air show community was deeply divided on the question of whether the Royce duo were cowboys. And with no consensus that their techniques were in any way abnormal, the air boss training and accreditation process needed to receive greater scrutiny. ◊◊◊ The tail of the B-17 with the Dallas Executive Airport control tower in the background, and airliners on approach to Dallas Fort Worth International Airport. (Dallas Morning News) As I mentioned earlier in this article, Russell Royce began air bossing with on the job training from his dad, before later receiving a letter of authorization (LOA) designating him as a “recognized air boss, multiple venues.” The typical training for an air boss lasted about four years and involved observing other air bosses, discussing theory, gaining radio experience under supervision, and other skill-building practices. These same type of activities are included in the International Council of Air Shows (ICAS) Air Boss Recognition Program (ABRP), which Royce underwent less than three years before the accident when the requirement was first introduced. The ABRP came with a training manual against which air boss applicants were to be judged by an ICAS-approved “air boss evaluator,” who would observe the applicant directing an air show and assesses their performance against the criteria on an evaluation form. Achieving full recognition also required a record of air show experience, letters of recommendation, completion of an “air show education data sheet,” and a score of 75% or better on a multiple choice test. Should the applicant meet these requirements, the FAA would issue an LOA on ICAS’s recommendation. The triannual renewal process for the LOA was described earlier in this article, but it bears repeating that the process did not include recurrent training or evaluations. The air boss community is small and insular. At the time the NTSB report was written, there were only about 60 to 70 ICAS-approved air bosses, and only 6 to 7 of them were air boss evaluators. Although close friends and family are forbidden from evaluating each other, it’s plausible that almost all air bosses in the United States know one another through industry connections and ICAS conferences. In such a community, outside monitoring by the FAA is probably necessary to ensure that corners don’t get cut. However, the NTSB found that FAA oversight of the process was minimal. The agency checked in on ICAS on a quarterly basis but was not closely involved in the accreditation of air bosses, nor were FAA inspectors at air shows given guidance on how to spot improper air boss behavior. In fact, their guidance was mostly aimed at ensuring that the provisions of the airspace waiver were met; that is to say, that the performers held the correct licenses, that the airplanes stayed within the designated airspace, that the briefing was held at the proper time, and so on. Since the inspectors were not experts on air show or air boss conduct, nor were they given any training or checklists related to those topics, it was unlikely that they would independently identify safety issues. The matter was considered so secondary that the FAA inspector didn’t even have a radio with which to listen to the air boss frequency. The trainee inspector did listen to the air boss frequency but was unable to identify anything out of the ordinary about the communications. NTSB investigators examine the tail section of the B-17. (NTSB) With the FAA exercising minimal oversight of the air boss training, accrediting, selecting, and monitoring process, the burden of ensuring the quality and trustworthiness of each air boss fell disproportionately onto the air show organizers themselves. In the case of Wings Over Dallas, that was the Commemorative Air Force. As this article has already established, Chairman Linebarger, a CAF employee, selected Russell Royce as the air boss because she liked his “way of doing things.” But as Linebarger herself freely admitted, she was too busy during air shows to actually monitor the air boss’s performance, and even if she could monitor it, she hadn’t received any training or guidance on what proper air bossing was supposed to look like, other than her own lived experience. In fact, she told the NTSB that the only way for her to find out about a problem with the air boss’s directives was for someone to bring it to her attention. However, as we’ve already noted, many senior CAF pilots didn’t see anything wrong with the way the Royces directed performances either. Furthermore, a small group who did have complaints about Ralph Royce after Wings Over Houston didn’t bring their concerns to the Air Show Chairman or indeed any other authority figure. It was therefore quite probable that Chairman Linebarger had little idea that the Royces’ freeform air bossing technique was controversial, or that other air bosses employed better safeguards. Even months after the accident, some interviewees didn’t appear to understand what the problem was, including Chairman Linebarger herself. In one regretful interview passage, the NTSB asked her what she would do differently if she were to organize a 2024 rendition of Wings Over Dallas, to which she replied that she wouldn’t do anything differently at all. And when further pressed on whether she had learned anything from the disaster, she said, “I learned how quickly things can change in a blink of an eye regardless of how well you’ve planned it; anything can happen. But you just have to move forward.” Is there any interpretation of that statement other than an admission that she learned nothing? I don’t want to be overly judgmental considering that some people freeze up when put on the spot, but it’s hard to imagine what would be a worse way to answer those questions. Another view of NTSB investigators examining the tail section. (Dallas Morning News) Based on extensive testimony gleaned from multiple interviews and other information sources, this attitude appears emblematic of a larger problem with the safety culture at the CAF. The Commemorative Air Force has some structural and historical barriers to achieving the kind of open, responsive, regimented safety culture that you might find at an airline. As an organization composed of volunteers, almost all of them retired pilots, the average CAF member is old and set in their ways. For instance, Chief Aviation Officer Jim Lasche described visiting a CAF wing that was having problems and discovering that everyone was over the age of 75 and the lead mechanic was 84. He said that many CAF members didn’t like it when he made changes, were uncomfortable around computers, and resented requirements being imposed upon them. Some of the units had turned into “old boys clubs,” ostensibly representing the warbird community in a major metropolitan area without adding any new members for a decade or more. And most importantly of all, everyone was there because they wanted to fly warbirds — and the implicit fear that that privilege could be taken away sometimes created a culture of silence. In a robust organization, that pressure can be alleviated by erecting clearly defined and well-understood safeguards for members who want to express safety concerns. But in a tight-knit organization full of experienced people, each of them bringing a lifetime of agendas and opinions to the table, abuse was known to occur. For instance, Lasche mentioned one incident in which a CAF member unsuccessfully propositioned a female wing mate, then falsely reported that her plane was unsafe in order to get back at her for turning him down. And while Lasche freely gave out his personal phone number as a 24/7 safety hotline, he stated that some people would call him just to rant or vent off steam, sometimes at odd hours of the night. But perhaps the most concerning story about the CAF’s internal culture was directly related to Wings Over Dallas. As you might recall, a two-seater Stearman on a revenue ride flight landed on runway 31 at approximately the same time as the collision. The NTSB was alarmed not only because the Stearman was potentially distracting for the air boss during a critical period of the show, but also because the plane was nearly struck by falling debris from the colliding aircraft, which could have injured or killed the occupants, one of whom was a paying passenger. In his interview, Jim Lasche stated that it had always been CAF policy to run revenue ride flights during breaks between warbird performances, when no other aircraft were airborne. But in recent years, air show timelines had been condensed to eliminate breaks that would cause some of the audience to leave. Because revenue ride flights are crucial to balancing the CAF’s books, this in turn resulted in pressure to conduct revenue rides during performances. On the other hand, Lasche felt that having paying passengers airborne in the same airspace as an ongoing performance was dangerous, so he came out against the move. But he “acquiesced” after receiving intense pushback from what he described as “higher up people,” including the air boss. As a compromise, he agreed to allow revenue ride flights during performances, but only as long as they didn’t take off or land while another act was flying. However, this agreement apparently wasn’t recorded in writing and wasn’t properly enforced, because the air boss authorized several revenue ride flights to take off or land during the accident performance, including the Stearman that almost became collateral damage. The decision to allow these takeoffs and landings was made without Lasche’s knowledge and he was upset to find out about them after the fact. Nevertheless, the Stearman issue made it clear that concerns such as entertainment value and revenue were sometimes taking priority over safety, and that there was no framework in place to ensure that safety won out. NTSB investigators examine the main B-17 wreckage. (NTSB) Years before the accident, Lasche had attempted to create such a framework by implementing a safety management system, or SMS. The idea behind an SMS is to provide a secure, anonymous avenue for members to report incidents and concerns, creating a stream of data that can then be analyzed to detect unsafe trends. The CAF wasn’t required to have an SMS, but implementing one was undoubtedly a good idea and it could have been very useful, in theory. Lasche explained that he was inspired to develop an SMS after he asked a group of CAF pilots where they thought the organization’s next accident would come from, only for everyone present to agree, without hesitation, that a particular named person was at risk. When that very pilot killed himself and a passenger in a crash a few weeks later, Lasche concluded that if a system had been in place to turn those concerns into action, lives might have been saved. But the SMS as actually implemented was something of a disappointment. Despite efforts to make sure all CAF members knew how it worked, the system was rarely utilized and received zero reports during the year leading up to the crash. Nevertheless, Lasche never really tried to figure out why. The NTSB rightly pointed out that receiving zero reports is usually indicative of a lack of understanding or trust in the SMS rather than a lack of safety issues to report. It was clear that some CAF members preferred to keep reporting issues using Lasche’s personal phone number, but the lack of anonymity in that approach and the absence of detailed record-keeping limited its usefulness. Nor was this problem confined to the CAF, because ICAS had its own similar safety reporting system that only received 10 reports in 15 years. In its final report, the NTSB wrote, “An organization’s culture can become unhealthy if motivational factors exert influence that turn it into something colloquially known as “don’t rock the boat” syndrome.” The investigators felt it was understandable that such a culture could develop at the CAF, where members paid for the privilege to participate and didn’t have the protections afforded to an employee. Further, the NTSB added, “ In such cases, the performance can become more about thrilling the crowd, sometimes to the detriment of aviation safety. While some air show event organizers have actively worked to apply administrative controls to preclude such performances, the circumstances of this accident suggest that, for some parts of the warbird community, it may persist.” Another view of the B-17 main wreckage. (Unknown) The NTSB notes that these circumstances created a culture where members did not speak up about deficiencies. But I would like to point out that the CAF interviews show many members didn’t recognize that safety deficiencies existed in the first place. I want to caveat this section by mentioning that I am not a CAF member, not a pilot, and not an expert on air shows. Weeks of research don’t replace or equal years of experience. But in all the testimony I read and all the guidance I reviewed, I didn’t find a credible defense of the way that the accident performance was carried out, not from Russell Royce nor from anyone else. Royce’s beliefs about the proper way to conduct an air boss directed performance were inconsistent with basic, well-established safety principles that transcend most modes of aviation. It is a fact that in a complex maneuvering environment involving multiple aircraft, especially dissimilar aircraft, visual separation is insufficient to preclude an accident, and it is a fact that the use of procedural separation in addition to visual separation is superior to visual separation alone. Of course pilots should be keeping each other in sight; that’s a basic part of flying any aircraft. But that doesn’t mean that the air boss shouldn’t take easily available measures to reinforce the imperfect substrate of see-and-avoid, and the existence of visual separation certainly doesn’t allow the air boss to wash their hands of all responsibility when two pilots inevitably and predictably lose sight of each other and collide — because saying, “that’s too bad, I guess they should have looked harder,” and then moving on to the next air show, is a surefire way to kill another 6 people next year. As seasoned aviators, often with decades of experience in the highly regimented and extremely safe world of passenger airlines, most CAF pilots and managers should theoretically have been in a position to recognize that Royce wasn’t using procedural separation, and that this represented a serious safety issue. But not only did nobody speak up about this before the accident, several prominent CAF members interviewed by the NTSB still didn’t see anything wrong with it even six or eight months later. As far as they were concerned, Russell Royce’s freeform air bossing style was just the way things were done, and the possibility that “the way things were done” was unsafe and should be improved may not have occurred to them. Looking in from the outside, as someone who hasn’t spent my life in the air show environment, and as someone who knows what the consequences of these unsafe practices turned out to be, it’s easy for me to say they should have reacted differently. But I won’t say that, because it’s a well-documented fact that human beings, existing day in and day out within a system where safety measures are not being used, will quickly become accustomed to the unsafe environment even if they know, in principle, what a safe environment ought to look like. And that’s what sociologist Diane Vaughan famously called “normalization of deviance.” Aerial view of the tail section of the B-17. (NBC News) The normalization of deviance is the process by which actors within a complex system featuring complex safety requirements unconsciously tolerate increasingly unsafe behaviors and practices as long as those practices are not met with adverse consequences. It’s really not a very difficult concept to understand, because all of us have probably experienced a basic version of it at some point in our lives. For example, I spent years writing articles using a laptop with broken keys, even though this resulted in a greatly elevated number of typographical errors, simply because I had become accustomed to my little workarounds for using the keyboard and catching mistakes. But if someone else had tried to sit down and write using my laptop, they would have been frustrated beyond belief. So while it would have been better to get my computer some professional help, I never did, even though I knew the problem existed and I could afford the repairs. The point is that when you’re inside the unsafe system, it can be hard to see the forest for the trees, and sometimes even if you do, inertia can prevent change. This was probably especially true at the CAF, an organization with a tight-knit membership, disincentives to rocking the boat, and a unique mission that requires flying 85-year-old aircraft with all the safety issues inherent to that. And when you’re up there in a formation, flying a B-17 Flying Fortress or a P-51 Mustang, living the dream in front of thousands of awed spectators, who really wants to be the one to say, “Hey, maybe we shouldn’t be doing this?” Hell, who even wants to think that? ◊◊◊ In October 2023, a memorial to the lives lost in the accident was erected in Conroe, Texas, home of the CAF unit that operated the B-17. (The Courier of Montgomery County) To its credit, the CAF made several changes to its operations after the accident without waiting for NTSB recommendations. In 2023, CAF management drafted new air show guidance banning CAF aircraft from participating in air boss directed performances that included aircraft of differing performance; multiple parades of different aircraft categories without at least 500 feet of altitude separation; or maneuvers other than racetrack patterns. Dog bone turns and crossing paths would only be allowed as part of an approved maneuvers package. Air bosses would be required to meet these stipulations before the CAF would agree to participate. At the same time, the FAA issued a Safety Alert For Operators (SAFO) recommending that air show organizers provide all pilots with a detailed written plan, including well-defined separation strategies, in advance of any performance. The SAFO also stated that maneuvers other than racetrack patterns were not recommended for air boss directed performances, and linked to a new ICAS document that outlined a strategy for using altitude blocks and lateral buffers to separate aircraft. A form that air bosses could use to assign aircraft to these blocks was also included. In its final report, the NTSB made several recommendations that went even farther. These included the following: · The FAA and ICAS should establish air show standard operating practices addressing procedural separation in air boss directed performances, risk assessments for air show operations, and a post-show debriefing with reporting of results to the FAA and ICAS. · The FAA should require recurrent evaluations upon renewing air boss letters of authorization. · ICAS should develop a set of standard terminology for air bosses to use when describing air show maneuvers. · FAA inspectors should observe air boss performance during air shows and provide feedback during the debriefing, and the FAA should provide its inspectors with evaluation guidance. · The CAF should use existing FAA guidance to develop a risk assessment and mitigation process tailored to its unique operations. So far, the effect of these reforms remains to be seen. Some changes are already visible; for example, Chief Aviation Officer Jim Lasche said he refused to allow CAF airplanes and pilots to participate in a February 2023 air show where Russell Royce was the air boss. Later, the family of Len Root sued Royce for negligence, and the CAF for hiring him. The outcome of that suit is pending as of this writing. A makeshift memorial honors the victims near Dallas Executive Airport. (Fort Worth Star-Telegram) However, one of the points I want to end on is that while there were characters in this story who came across very unfavorably, a true safety reckoning doesn’t stop at blaming those people — in fact, searching for blame isn’t really part of the process at all. Everyone in this story was a product of the environment that they operated in, while at the same time, each contributed in some unconscious way to the perpetuation of that environment. It requires vision and drive to change the culture of an organization — or it can require tragedy, lawsuits, and new FAA regulations. But both methods require that the root causes of unsafe practices be identified, and it’s clear that the practices that led to the disaster at Wings Over Dallas didn’t start with Russell Royce, or with his dad, or with the air show chairman. They probably didn’t even start with the CAF. Most likely, those practices arose from the informal origins of warbird demonstrations — a few guys getting together on weekends to pluck ex-military aircraft from boneyards to fly for fun — and those origins created attitudes that shaped the institutional character thereafter. And because of the Wings Over Dallas tragedy and other recent warbird incidents, it’s now up to the current generation of warbird enthusiasts to ensure that it remains possible to uphold the CAF’s mission — to preserve the aviation heritage of the Second World War for the education and enjoyment not only of current, but also future generations. In that mission, I wish them luck. I am not and probably never will be a warbird enthusiast, but it nevertheless fills me with a certain wonder to look up and see a B-17, its mighty radial engines turning fuel into noise, like a beast out of another age, brashly carving its way through our modern skies. I hope that some lucky few will one day get to ride on a B-17 as it celebrates its hundredth birthday. The biggest obstacle to that milestone isn’t the age of the planes, but the way that we treat them, and if nothing is learned from this latest tragedy, then not only will we lose this part of our history — we will deserve it. _______________________________________________________________ Thanks for your patience in waiting for this article! After publishing my piece on EgyptAir 804 in December, I moved half way across the country in a long, messy relocation process fraught with other struggles along the way. But here I am, and here it is. Thank you! _______________________________________________________________ Don’t forget to listen to Controlled Pod Into Terrain, my podcast (with slides!), where I discuss aerospace disasters with my cohosts Ariadne and J! Check out our channel here , and listen to our latest episode about a titanic battle between a BAC 1–11 and some wind. Alternatively, download audio-only versions via RSS.com , or look us up on Spotify! _______________________________________________________________ Join the discussion of this article on Reddit Support me on Patreon (Note: I do not earn money from views on Medium!) Follow me on Bluesky Visit r/admiralcloudberg to read and discuss over 260 similar articles Bibliography","Passing the Buck: The story of the 2022 Wings Over Dallas air show collision Admiral Cloudberg · Follow 59 min read · 1 day ago -- 7 Listen Share A Boeing B-17 and a Bell P-63F explode into shrapnel a split second after colliding in midair at the Wings Over Dallas air show. (David Walsh) On the 12th of November 2022, thousands of spectators at the Wings Over Dallas air show in Dallas, Texas bore witness to a sudden tragedy, as two WWII-era warplanes collided in midair during a performance, killing 6 crewmembers. In numerous spectator photographs and videos, a single-pilot Bell P-63 fighter could be seen arcing toward the Boeing B-17 “Texas Raiders,” its belly up in a steep left turn, until the two aircraft crossed paths, and in the blink of an eye the P-63 cleaved the larger bomber in two. Shaken by the loss inflicted on their tight-knit community, the air show’s volunteer-led organizers, the Commemorative Air Force, were left wondering whether the procedures used to prevent collisions during large formation flights might have some flaw that placed the two planes on a collision course. But the National Transportation Safety Board’s final report, and interviews with those involved, make clear that the problem was not so much that the procedures were flawed, but that no procedures existed. At Wings Over Dallas, the pilots of eight airborne aircraft, flying in close proximity to one another, were placed under the control of a so-called “air boss,” a role requiring only the bare minimum of qualifications, without prior knowledge of the maneuvers that the air boss would ask them to perform. Even worse, almost everyone involved seemed to think that this was normal — a remarkable example of what sociologist Diane Vaughan termed “normalization of deviance.” What follows is therefore not only the story of the disaster at Wings Over Dallas, but also the story of how the Commemorative Air Force, the Federal Aviation Administration, and the International Council of Air Shows created the circumstances for it to occur, all without recognizing that anything was amiss. _________________________________________________________________ Note: I am an outsider who has written a detailed story that is personal to many people. Several people are mentioned by name and I am aware of the possibility that they or their families might read this article. If you were involved or knew someone who was, and you notice incorrect biographical information, please send corrections to my publicly available email address. Thank you! _________________________________________________________________ ◊◊◊ A squadron of B-17s on a bombing run over Germany. (Stock photo) During the Second World War, American factories produced tens of thousands of warplanes to fuel the ferocious air battles over Europe and the Pacific, mass producing fighters, bombers, and transports on a scale matched neither before nor since. Untold thousands of these aircraft fell in battle, spiraling aflame from hostile skies, and an equal or even greater number were lost in accidents — destroyed in hard landings, smashed against Himalayan mountaintops, or lost at sea, never to be found. And after the war was won, even more of these aircraft met an unceremonious end at boneyards around the world, torn apart for scrap just as quickly as they were put together. But a precious few survived long enough to fall into the hands of people who saw them as more than merely machines. Among those who dedicate their time to salvaging, restoring, maintaining, and flying the relics of WWII, these aircraft came to be known as warbirds . Not every historic military plane was considered a warbird, because not every war was the war, the one nobody needed to name because it was obvious. In the decades since, that definition has slipped, but the association remains. A selection of CAF warbirds, including the B-17 Texas Raiders. (Commemorative Air Force) Although the number of true warbirds, in the original sense, can only ever go down, a surprising number remain airworthy thanks to the efforts of private individuals and non-profit organizations. The largest such organization is the Commemorative Air Force, or CAF, which today operates over 180 historic military aircraft distributed between 75 local units in 6 different countries. But the CAF wasn’t always so large. It got its start in 1957 when a small group of former US Air Force pilots purchased a single P-51 Mustang for fun, only to discover that no one else was trying to preserve decommissioned warbirds for future generations. The group then set out to acquire at least one example of every American warbird, and the rest is history. At first, the Texas-based CAF branded itself the “Confederate Air Force” as a joke, but later changed its name when the tainted association with the Confederate States of America started to interfere with its loftier goals and more serious outlook. At the same time, the CAF grew from a small, ad-hoc group into a larger organization with its own operations manuals, pilot training programs, and more. As a registered non-profit run mostly by volunteers, the CAF sustains its operations by charging membership fees in exchange for the opportunity to fly its airworthy warbirds, and by collecting 9% of the revenue from events put on by each of its local chapters, known as “wings.” Each aircraft is assigned to a wing that shoulders responsibility for operating that aircraft and training new members to fly or maintain it, with day-to-day use largely left at the local unit’s discretion, as long as CAF policies and procedures are followed. For accuracy’s sake, it should be noted that the warbirds are officially owned by the American Airpower Heritage Flying Museum — a legally separate entity from the CAF, which merely operates the aircraft — but the CEO of the CAF and the CEO of the AAHFM are the same person. Fighters in formation during the CAF’s Tora! Tora! Tora! performance, which is an approved maneuvers package. (Bart Marantz) Although CAF units and individual aircraft frequently participate in air shows hosted by third parties, the Commemorative Air Force itself also organizes several air shows each year, typically in the fall. As part of the CAF’s 2022 event roster, the group hosted two back-to-back air shows in Texas that will become relevant to this story, starting with Wings Over Houston on October 29–30, and Wings Over Dallas on November 11–12, both of which featured many of the same performances, aircraft, and pilots. Among the types of activities that took place at these two air shows, three merit further discussion — namely, revenue ride flights; approved maneuvers packages; and air boss-directed performances. Revenue ride flights are pretty much what they sound like, in that members of the public pay a specified amount of money in exchange for the opportunity to ride in a warbird, usually under the Federal Aviation Administration’s Living History Flight Exemption, which allows such flights as long as the “ticket” price is structured as a charitable donation. Such flights are an important source of revenue for the CAF and are critical to its financial stability. Normally they take place during gaps between other types of performances. Next, an approved maneuvers package refers to a scripted, choreographed performance usually involving close formation flying. The pilots who participate in an approved maneuvers package have practiced every move, every turn, and every power adjustment dozens or even hundreds of times, relying on strict timing and precisely defined flight paths. The participants in such a package are able to fly the entire performance in close proximity to other aircraft without receiving continuous instructions. When most people think of an air show performance involving multiple airplanes, this is usually what they imagine — for my American readers, the Blue Angels are a great example of a group that uses maneuvers packages; or for Canadians, the Snowbirds. A group of bombers flies in trail during the air boss directed performance at Wings Over Dallas 2022. This photo was taken moments before the collision. (Jason Noyes) The third and final type of activity, and the most important one for this story, is an air boss-directed performance. An air boss is a person who gives instructions to aircraft during an air show, such as who is cleared for takeoff, who is cleared to land, who should fly where, and so on, using a common frequency for all aircraft within the air show airspace. The air boss isn’t an air traffic controller and doesn’t need to have any air traffic control background — more on that later — but the job is similar in some respects. The most difficult part of an air boss’s job, however, is directing an unscripted performance. In an air boss-directed performance, the air boss develops a desired flight path for the aircraft involved in the performance and then instructs those aircraft and pilots to follow that flight path, without the use of an approved maneuvers package. The performance is usually constructed by stringing together basic maneuvers like low passes and course reversals that are familiar to the pilots, but the exact sequence of maneuvers is not practiced beforehand, and the positioning of each aircraft relies on the air boss’s directions rather than pre-arranged timings, bank angles, and power settings. The air boss’s role in a directed performance has been compared to that of an orchestra conductor. To understand this type of performance, it’s also important to clarify the exact definition of “formation flying” and its implications. In its report, the NTSB describes a formation flight as “two or more aircraft under the command of a flight lead that are flown solely with reference to another aircraft in the formation.” Under US regulations, if the aircraft are less than 500 feet apart, the pilots must possess a special formation flying certification. Conversely, aircraft that are not part of a formation must remain more than 500 feet apart, and while a string of aircraft each separated by this distance may appear to be a formation, it is not. This distinction will become important when analyzing what went wrong at Wings Over Dallas. The layout of the air show. (Own work, map and data courtesy NTSB) With all of that having been said, let’s jump forward in time to November 12th, 2022, the second day of the Wings Over Dallas air show at Dallas Executive Airport, a general aviation field located 12 kilometers southwest of downtown Dallas. We’ll go back and look at Wings Over Houston later. The performers in the November 12th activities started their day by attending a morning briefing held by air boss Russell Royce. Under the conditions imposed by the FAA waiver authorizing the air show, the briefing was a required item, and at least one member of every flight crew had to receive the briefing or face being cut from the roster. Also in attendance were an FAA inspector assigned to monitor the air show; another inspector being trained on air show operations; and a second air boss observing Royce in order to gain warbird experience. Using a PowerPoint presentation, Royce explained the boundaries of the air show airspace, landmarks that could be used to find the boundaries, where to go in the event of an emergency, and most importantly for this story, the location of the two “show lines.” Dallas Executive Airport has two crossing runways designated 17/35 and 13/31 respectively. Runway 13/31 has a northwest-southeast orientation and is the longer of the two runways, as a result of which it was selected as the axis along which the air show demonstrations would take place. The crowd was positioned on bleachers on the apron adjacent to the east side of runway 13/31, near the intersection with 17/35. To give the crowd the best view, aircraft would make passes down runway 13/31 along one of two designated “show lines” located at 500 feet (150m) and 1,000 feet (300m) in front of the crowd, respectively, with the 500-foot show line running down the eastern, inside edge of the runway and the 1,000-foot show line running along the tree line at the western edge of the airport’s clear area, as shown above. Which show line would be used by which aircraft, and when, was outside the scope of the briefing and would be assigned by the air boss during the performance. However, the briefing did cover techniques that could be used to identify and align with the show lines. Texas Raiders at Wings Over Houston in 2019. (Alan Wilson) At the briefing, the air boss also distributed a schedule listing the expected start times for each act. One of the acts that day was the CAF’s Tora! Tora! Tora! performance, an approved maneuvers package reenacting the 1941 Japanese attack on Pearl Harbor and the American counterattack, featuring multiple warbirds that were used in filming the 1970 movie of the same name, as well as coordinated pyrotechnics. As far as I have been able to tell, this is the only approved maneuvers package used by the CAF, and they perform it at numerous air shows every year. Following Tora! Tora! Tora!, the schedule called for a “warbird parade” featuring five American bombers and three American fighters, which would make passes back and forth in front of the crowd in a series of simulated bombing runs. At the finale of the performance, the warbirds would be joined by a Boeing B-29 Superfortress, one of only two remaining airworthy examples. The five bombers taking part in the parade were to be led by the B-17 Flying Fortress “Texas Raiders,” followed by the Consolidated B-24 Liberator “Diamond Lil,” a Curtiss-Wright SB2C Helldiver, and two North American B-25s named “Devil Dog” and “Yellow Rose.” The plan was for the bombers to fly “in trail,” with the B-17 receiving instructions from the air boss while the other four aircraft followed its lead, maintaining at least 500 feet of separation because the group was not officially a formation and the pilots were not required to be formation rated. At the same time, the three fighters were led by a North American P-51 Mustang, followed by a second Mustang and a Bell P-63F Kingcobra. The fighters would fly in close formation and all three pilots were formation-rated. Effectively, this divided the parade into two groups that not only had greatly different performance characteristics, but also different rules governing minimum separation distances and pilot qualifications. The five crewmembers of Texas Raiders and the pilot of the P-63F. (Commemorative Air Force) Because the accident involved the B-17 and the P-63F, we’re going to take a closer look at those aircraft and their crews before continuing. The Boeing B-17 Flying Fortress is a four-engine piston powered bomber designed to drop large amounts of unguided ordnance onto enemy targets. The model entered service with the US Army Air Corps, the predecessor to the Air Force, in 1936 and saw widespread action in WWII, as over 12,000 B-17s dropped 640,000 tons of explosives over Nazi Germany. At the time of the accident, there were approximately seven airworthy B-17s remaining, including the airframe nicknamed “Texas Raiders.” This B-17 was license-built by Douglas in Long Beach, California in 1944 and was delivered to the US Army Air Forces in July 1945 before being transferred to the Navy, where it served as an Airborne Warning and Command System (AWACS) platform in the Korean War. The aircraft was retired from the Navy in 1957 and spent ten years flying aerial surveying missions in Alaska until the CAF acquired it in 1967. The B-17 could be flown with a bare crew compliment of three, but for Wings Over Dallas, five crewmembers had been rostered. In command was 66-year-old Leonard “Len” Root, a longtime CAF member and recently retired American Airlines pilot with over 28,000 flight hours and type ratings in 12 different airplanes, including 500 hours on the B-17. He was also the former leader of his CAF wing and was responsible for training new members. The second in command was 67-year-old Terry Barker, also a former airline pilot, with over 25,300 flight hours and experience in everything from gliders to helicopters to large passenger jets. He had 90 hours in the B-17 and was the chief maintenance officer in the same CAF wing as Len Root. The other three crewmembers consisted of 64-year-old flight engineer Curt Rowe, a 30-year veteran of the Ohio Civil Air Patrol, and two “scanners.” The purpose of the scanners was to stand by the B-17’s rear doors and keep lookout for other aircraft, but there was no requirement that the scanners have a pilot’s license, or indeed any qualification at all. Normally only one scanner would be used, but apparently Len Root requested a second one. The first scanner in this case was 42-year-old Kevin “K5” Michels, a CAF historian and tour supervisor, while the second scanner was identified as 88-year-old Dan Ragan, a former US Navy radio operator who served on Texas Raiders during the Korean War. It’s not apparent from the available evidence whether giving him the scanner role was a pretext to allow him to keep flying on his former aircraft, but it if it was, it’s kind of hard to begrudge him. The sole surviving P-63F, seen in 2019 prior to its involvement in the accident. The second aircraft involved in the accident was the Bell P-63F. The P-63 Kingcobra family of single piston-engine fighters was developed originally for the US Army Air Forces around 1943, but the type never saw combat for the United States. Instead, around 3,300 were built mainly for the Soviet Air Force. Numerous variants were also developed, many of which did not see widespread use, including the P-63F, which was distinguished from the base model by its enlarged tail and modified engine. The P-63F project was abandoned after just two aircraft had been built, and the CAF operated the only surviving example. The pilot of the P-63F was 63-year-old United Airlines pilot Craig Hutain, a veteran aviator who flew his first airplane at 10 years old and had since accumulated a jaw-dropping 34,000 flying hours, including 108 in the P-63. He was also a member of and operations officer for the Tora! Tora! Tora! demonstration team, a fighter check pilot for the CAF, and an approved formation flyer with an endorsement for aerobatics. The CAF’s chief aviation officer summed him up with just six words: “That guy knew how to fly.” ◊◊◊ Several air bosses on the air boss platform at an air show. Note: none of these individuals were involved in the accident air show; this photo is representative only. (Dave Hadfield) The B-17 crew and the P-63 pilot were to play different roles in that day’s performance, and not only because they were flying different aircraft with different performance characteristics. Because the B-17 was the lead aircraft in the group of five bombers, its crew was responsible for the real-time interpretation of the air boss’s commands, while the other four bombers simply followed the B-17 at a safe distance. On the other hand, Craig Hutain in the P-63 was not the leader of his formation and would be focused primarily on staying close to the two P-51s ahead of him. The air boss’s instructions were relevant to him only if Royce commanded a change in the shape of the formation. Having said that, we also need meet air boss Russell Royce, the last major character in this story, before we continue. Royce’s age isn’t listed in any of the available documents but based on other statements he was probably about 38 years old at the time of the accident and held a day job working at an auto body repair shop. His father was Ralph Royce, a veteran air boss widely known throughout the American air show industry, and he grew up hopping from one air show to the next, shadowing his dad. Ralph first allowed Russell to issue instructions to aircraft at the age of 14 — albeit under close supervision — and by 18 years old he was directing entire air shows by himself. That’s not to say he was a child prodigy; in fact, in his interviews Royce denies that he was born with any particular talent, but he was undeniably very experienced. At this point you might be asking how it was legal for a teenager to direct an air show, and the answer is that there was absolutely no law saying he couldn’t. At the time that the younger Royce began air bossing, there were no formal requirements for the position at all, other than those imposed by the air show organizers. Over the years, Royce did acquire a private pilot’s license, and he briefly worked in an air traffic control tower at a general aviation airport from 2008 to 2009, but his aviation experience paled in comparison to that of the pilots he was directing. Unlike them, he had never flown for an airline, for the military, or in an air show; he wasn’t rated to fly multi-engine planes; and he had no formation or aerobatics experience. He had, however, air bossed over 300 air shows. Later in his career, new regulations began requiring air bosses to receive a letter of authorization (LOA) from the FAA in order to perform the role. To receive an LOA, an air boss had to complete the air boss recognition program run by the International Council of Air Shows Inc., or ICAS, an industry standardization and advocacy body. The LOA would remain valid for three years but could be renewed by meeting a minimum experience requirement and submitting letters of recommendation. To renew the highest level of LOA — “recognized air boss, multiple venues” — he was required to have directed 8 air shows in the last three years, submitted four letters of recommendation from other air bosses or credentialed air show performers, and attended at least one ICAS Air Boss Academy workshop. There was no requirement for recurrent training or a performance evaluation. Russell Royce held the highest level of LOA, but at the time of the accident the requirement was still so new that he had yet to receive his first renewal. Although Russell Royce wasn’t a member of the CAF, his father was previously CAF chairman and many CAF members were familiar with him. For Wings Over Dallas, he was hired by the Air Show Chairman, the CAF’s Gena Linebarger, who said she always hired Ralph and/or Russell Royce if they were available. The working relationship between the two dated back about 10 years and the younger Royce had air bossed Wings Over Dallas numerous times during that period. In fact, the CAF had also hired Ralph Royce to air boss Wings Over Houston two weeks before the incident, and according to the Wings Over Houston performers, Russell was present there, too. ◊◊◊ Flight paths leading up to the accident, part 1. (Own work, map by Google, based on NTSB data) Back at the morning briefing on November 12th, Royce wrapped up the proceedings after about 45 minutes, then asked the audience if they had any questions. Although Len Root wasn’t personally present, witnesses recalled that some other members of the B-17 crew asked clarifying questions, but no one could remember what they were. Subsequently, Royce went out to the air boss station, where he would guide the air show from atop a set of air stairs on a taxiway near runway 17/35. He was joined on the air stairs by the air boss observer, while the FAA inspector and trainee inspector stood nearby. After the start of the air show at 10:50, six acts flew before it came time for the warbird parade shortly after 13:00. The five bombers took off one after another from runway 31, followed by the fighters, with the B-17 departing at 13:10 and the P-63 at 13:15. After takeoff, each aircraft was given initial maneuvering instructions while waiting for the remaining aircraft to join them in the formation or group; in the B-17’s case, the air boss instructed, “It will be a right turn. You’re looking for a thousand feet to enter from over the crowd.” The B-17 crew acknowledged and complied, making a sweeping 270-degree right turn after takeoff to come back over the field from the east, from behind the crowd. By this point the two B-25s were already in the air finishing up the previous act, so the air boss instructed them to fall in behind the B-17 and the SB2C helldiver, which was also airborne. Flight paths leading up to the accident, part 2. (Own work, map by Google, based on NTSB data) At 13:12, the fighters reported ready to go. Royce instructed the B-17 to come over the crowd then make another right 270 onto the 1,000 foot show line, then cleared three fighters for takeoff, followed by a right turn. At the same time, Royce shot instructions to the pyrotechnic team and to the pilot of a Beechcraft T-34B Mentor that was conducting a revenue ride flight during the performance. Moments later at 13:14, with the B-17 completing the right 270 to line up for its first pass down the runway, Royce instructed them to follow that pass with a right 90 degree turn, followed by a left 270 degree turn — a course reversal technique known as a “dog bone” or “duster turn.” This maneuver would be used several times during the performance in order to reverse the direction of each consecutive pass down the show lines. Turning to the fighters, Royce instructed them to “get formed up behind the crowd and I’ll bring you overhead in just a moment.” Following the lead P-51, the three fighters entered into a close formation, completing a right 180-degree turn followed by a left 270, rolling out heading west toward the back side of the crowd just as the B-17 had done a few minutes earlier. Meanwhile, the B-17 completed its first pass in front of the crowd and started the dog bone course reversal, to which Royce commented, “Perfect, and then you can come right down the runway and the fighters are going to pick you up.” He then instructed the bomber group, which was now fully in trail, to enter a left racetrack pattern — that is, flying in circles with left turns — after completing their current pass, in order to wait for the fighters, who he said would join them “overhead,” without specifying an altitude. By this point the bombers were distributed between 500 and 800 feet, while the fighters were staggered from about 900 to 1,300 feet, with the lead P-51 on top and the P-63 on the bottom. If that all sounded complicated, that’s because it was, but so far all communications were working as intended. You can use the flight path maps above and below to follow along. Flight paths leading up to the accident, part 3. (Own work, map by Google, based on NTSB data) After confirming that the lead P-51 had the bomber group in sight, Royce instructed them to join the bombers in the racetrack, then told the bombers that after their current circuit they would complete make a second “dog bone” turn with a “left 90 and right 270.” The fighters then caught up to the bombers, made a right 270 to follow them through the racetrack, and then both groups proceeded together toward the dog bone described by the air boss. At 13:19, as the bombers and fighters were completing the dog bone, preparing for a north-south pass in front of the crowd, Royce set them up for yet another course reversal at the other end, instructing, “B-17, after this pass, right 90 left 270.” “Raiders, right dog bone,” the B-17 replied. Turning to the fighter group, Royce then said, “Fighters, walk your way up to the B-17, I’m going to break y’all out after this — um, you’re going to end up breaking left.” In formation flying, a “break” occurs when each aircraft successively makes a sharp turn to exit the formation. In hindsight, what Royce wanted the fighters to do was to cut short the left 270 in the next dog bone turn by breaking hard to the left, cutting inside the bombers to get out in front. What he meant by “walk your way up” is unclear. Flight paths leading up to the accident, part 4. (Own work, map by Google, based on NTSB data) Without waiting for a response from the fighters, Royce continued, “So you’re going to follow the bombers to the right 90 out and then you’re going to roll back in left and be on the 500 foot line if y’all want to set up an echelon for a break so y’all can get in trail.” The first part of this crucial transmission was an attempt to clarify what I already explained — that after making the right 90 portion of the dog bone turn, the fighters would make a sharp left. He also added that after making the left turn, they would align with the 500 foot show line, closest to the crowd. However, the last part of the transmission might have been unclear in the moment. In formation flying, an echelon refers to a staggered formation where each aircraft is offset from the next to either the left or right, like migrating geese. An echelon formation is an ideal starting point for a break. After the break, Royce wanted the aircraft to get in trail, one behind the other. But that was a lot of information to take in at once. Clearly confused by the lengthy transmission, the lead P-51 pilot radioed, “Okay, uh — say again for the fighters. That was not clear.” But instead of repeating his instructions, Royce responded with something totally different: “Uh fighters, go uh — right, go to echelon right.” But this transmission only made matters worse, because the fighters hadn’t even straightened out for the pass in front of the crowd yet, let alone begun the next dog bone. The next turn was a hard right, but making a hard right turn while in a right echelon formation, with each aircraft offset right from the aircraft ahead of it, is risky because the lead aircraft has to turn across the face of the aircraft behind it. In hindsight, Royce was a step ahead; he wanted them to assume the right echelon in preparation for the left break after the right 90 had already been completed. But in the heat of the moment, this would have been difficult for the fighter pilots to understand, and indeed the fighters responded not by forming an echelon, but by getting in trail, one behind the other. Why it doesn’t make sense to cut the fighters inside to the 500 foot line. (Own work) Meanwhile, as the lead bomber proceeded southbound past the crowd, Royce noticed that the gap between the B-17 and the B-24 behind it was larger than he would like, so he said, “Okay B-24, if you could give me a couple of mi — uh, inches and close the gap I’d appreciate it. B-17, let’s keep a little flat for me.” It wasn’t entirely clear what Royce meant when he asked to keep the turn a little flat, but he probably wanted the B-17 to lead the bomber formation out wide, using shallow bank angles to extend the distance traveled. This would give the fighters more time to cut the corner. However, this is known only in hindsight, because Royce had not yet explained that he wanted the fighters to move in front of the bombers, and it’s doubtful that any of the pilots had so far guessed that that was his intention. In response to the transmission, one of the bombers made an unintelligible reply, and then Royce added, “Just a little bit. When you come back through you’re coming through on the thousand foot line.” This was another crucial moment in the sequence of events — perhaps even the most crucial. Royce had now set up the next pass with the bombers on the 1,000 foot show line and the fighters on the 500 foot show line. If the fighters completed the dog bone by flying outside the bombers, that would be fine, but if they made a tight turn inside the bombers, the two formations would cross paths. At this point, the bombers were distributed between 550 and 680 feet altitude, with the B-17 occupying the highest position, while the fighter formation was distributed from 960 feet to 1,100 feet, with the P-63 in the lowest position. Then, at 13:20 and 37 seconds, Royce hammered another nail into the coffin: “American fighters should be in a right turn,” he said. “You’re gonna follow the bombers out on a right 90 and then I’m going to roll you back in front of them.” Finally, Royce had articulated his intention for the fighters to undertake the bombers. As I just explained, compliance required the fighters to turn inside the bombers, which would cause the two groups’ flight paths to cross as they lined up with their assigned show lines. To make matters worse, the way the dog bone turns were typically flown was that every aircraft would climb during the right 90, level off on the outside of the turn, then descend in toward the show lines to simulate a diving run from the crowd’s perspective. This meant that the altitudes of all the aircraft were variable during the dog bone, and just because the lowest fighter was above the highest bomber at the time the instruction was given didn’t mean that that would necessarily remain the case. Furthermore, if the fighters were to overtake the bombers, then the B-17 was the primary obstacle that they needed to pass — but the older, less powerful B-17 was slower than the other bombers and thus made tighter turns in order to prevent the bombers behind from catching up. That would make it harder for the fighters to overtake the bombers on the inside, because the B-17 was already cutting inside on every turn anyway. From here on out, the NTSB’s own diagrams of the flight paths are an effective companion to the text. This diagram shows the positions of all aircraft at 13:21:08, the time that the lead P-51 said “We see the B-17.” (NTSB) At this point, the stage was nearly set for disaster. All that remained was for the deadly dance to play out. As the fighters and bombers proceeded into the dog bone maneuver, Royce turned his attention to another aircraft entirely: a Boeing-Stearman PT-17 operating a revenue ride flight. The two-seater, WWII-era training biplane, commonly referred to as the “Stearman,” was carrying one pilot and one paying passenger on approach to runway 31, the very same runway along which the warbirds were to make their next pass. But Royce judged that no conflict existed, so he transmitted, using the Stearman’s callsign, “Quebec, I need you to drop it down to the deck. Runway three one clear to land.” “Down to the deck, five eight Quebec,” the Stearman pilot acknowledged. Watching the B-17 lead the bombers through the dog bone, Royce said, “There ya go B-17, yup, gentle flat roll it around thousand foot line.” “Thousand foot line for raiders,” said the B-17. “Fighters, roll it back to the left,” Royce then added. “Lead, fighter lead, roll it back to the left and get y’all in trail.” “Okay, fighters in trail,” the P-51 acknowledged. At this point the fighters had completed the right 90 and had caught up with most of the bombers. The formation was positioned directly overhead the B-24, the second aircraft in the bomber group, but had not yet caught up with the B-17, which had already started the left 270 and was turning inbound. “Yeah, and gunfighter, look out your left side and find the B-17,” Royce instructed, using an alternate callsign for the lead P-51. Although the fighters were turning inside the bombers, the B-17 was in the left hemisphere from the fighters’ perspective because it was already inbound and the fighters were still flying outbound. “We see the 17,” said the P-51. As the fighters started a sharp left turn, Royce repeated, “Yeah there you go. Roll it back to the left. I want you to get in front of the bombers. I want you to come through on the outside edge of the runway.” This instruction might have caused yet more confusion because the outside edge of the runway, from the crowd’s perspective, was the west edge, which was in between the 500 and 1,000 foot show lines. The 500 foot show line, which was where he wanted the fighters to fly, should have been referred to as the inside edge because it was closer to the crowd. In any case, without asking for clarification, the lead P-51 replied, “Okay.” The positions of all aircraft at 13:21:45, 10 seconds before the crash and 3 seconds before the P-63 strayed to the right. (NTSB) The altitudes of the two groups now overlapped, with the fighters at 700, 340, and 520 feet respectively, while the first four bombers were at 430, 600, 450, and 670 feet respectively. Watching from the apron, Royce said, “Nice job fighters, you’re coming through first. That will work out. B-17 and all the bombers on the thousand foot line.” While the two P-51s had passed the B-17 at the time of the transmission and were indeed in front of the bombers, the P-63 was not. And at its controls, Craig Hutain was running out of time to overtake the B-17 before they were forced to cross paths to align with their respective show lines. Nor did anyone know for sure that Hutain had the B-17 in sight, because Royce never asked him — he only asked the formation lead. Just to double check, Royce asked, “B-17, you got the fighters in front of you off your left?” The B-17’s reply is described as “unintelligible” in the transcript but it appears to have been an affirmative statement. However, at the time of that transmission, only two of the three fighters were actually “in front of” the B-17; the P-63 was still technically behind. From the captain’s seat, Len Root would have had to look back over his left shoulder into his 7 or 8 o’clock position to see it. Watching the maneuver play out, Royce said, “Nice job fighters, come on through.” Moving on to the next maneuver after the current pass was complete, he then added, “Fighters will be a big pull up and to the right.” But back in the fighter formation, confusion was evident. The two lead P-51s didn’t proceed onto the 500 foot show line as instructed, but lined up with the 1,000 foot show line instead. At the same time, however, Craig Hutain’s P-63 swung farther out to the right, heading toward the 500 foot show line — and directly into the path of the B-17. Watch bystander video of the collision courtesy of Morgan Curry and NBC 6 South Florida. Caution: Viewer discretion advised. With his low-wing fighter banked in excess of 45 degrees to the left, and with the B-17 below him and to his right, there was no way for Hutain to see it coming. Moving left-to-right across the bomber’s path from behind and above, the P-63 plowed into the B-17 amidships, cleaving it in two. As hundreds of people watched in shock and alarm, the P-63 disappeared into a chaotic hail of metal while the bomber broke in half just aft of the wings, ripping away the roof and cabin walls all the way forward to the cockpit. The crew of the B-17 barely had time to brace before what remained of their aircraft pitched forward into the ground and exploded in flames. More burning pieces of both aircraft strafed the grass next to the threshold of runway 31, narrowly missing the Stearman, which had touched down just seconds earlier. Instantly, Russell Royce activated the pre-briefed emergency procedure by shouting, “Knock it off, knock it off! Roll the trucks, roll the trucks, roll the trucks, knock it off, roll the trucks!” Emergency crews on standby near the runway rushed to the scene and began spraying down the burning wreckage, but little remained of either aircraft. It was clear that everyone on both airplanes had perished. A photographer in the audience captured this series of high-definition photos of the aircraft before, during, and after the collision. (Gary Daniels via NTSB) In the air, the remaining aircraft proceeded to their pre-planned holding points, where they waited while deciding where to divert. Not all of the pilots had seen the crash, and many of those who did were left confused by what they had witnessed, but everyone knew that a tragedy had taken place. Many of the surviving pilots had been friends with the victims for years, even decades. But only after getting their own planes back on the ground could they allow the shock to set in. Len Root, Terry Barker, Craig Hutain, Curt Rowe, Kevin Michels, and Dan Fagan — in the CAF, almost everyone knew their names. Some were lifelong pilots, airline captains, war veterans; others just wanted to teach military aviation history, but all had dedicated a portion of their lives to the warbird community, and that their lives were taken by the very objects of their passion did not provide comfort, and in fact only heightened the sense of loss. And after the victims were mourned and buried, the warbird community would also quietly mourn the aircraft themselves, saying goodbye to the shattered remnants of the only extant P-63F, and ticking one more name off the short and shrinking list of airworthy B-17s. So what was it all for? What was the reason for such a waste of life and history? ◊◊◊ Another photographer captured this alternate angle of the collision. (Dylan Phelps via NTSB) In search of the answer to that question, the National Transportation Safety Board conducted numerous interviews, pored over countless pages of documentation, reviewed dozens of photos and videos, and conducted multiple studies. In addition to their final report, the NTSB assembled a docket of evidence 1,896 pages long, which you don’t have to read, because I’ve read it for you. On its most fundamental level, any mid-air collision is the result of a breakdown in the methods used to keep aircraft separated. Those methods can be visual — in other words, pilots keeping each other in sight — and they can be procedural, such as when an air traffic controller assigns two planes to different altitudes. In general, procedural separation is safer because it relies less on human perception; that is to say, the probability that a pilot will incorrectly follow directions, or that bad directions will be issued, is lower than the probability that two pilots will not see each other, especially in conditions of restricted visibility. However, if the pilots already have each other in sight, then maintaining visual separation is perfectly adequate. You’ll hear a lot more from me about the minutia of these distinctions in a couple years when I write about the Potomac River midair collision, but for today’s story, this is enough. One of the key elements required to understand the Wings Over Dallas disaster was the type of separation in use. In an approved maneuvers package, separation was ensured by following a rigorously practiced choreography, but in an air boss directed performance, that wasn’t the case. So, the NTSB did the logical thing and asked air boss Russell Royce how he normally kept airplanes apart and what tools were available for him to do so. Royce explained that he didn’t have radar or any other flight tracking software and that he judged the position of each airplane visually and based on experience. He added that as in all modes of aviation, “visual is the rule of the road,” meaning that he expected pilots to see and avoid each other if a conflict were to develop. However, he also listed three other types of separation; namely, vertical, lateral, and time-based. These are fundamental techniques for procedural separation known to every air traffic controller. But after reviewing the sequence of events, a glaring question arose: did Royce actually use them? An explosion and falling debris are visible just after the B-17 impacted the ground. (Nathaniel Ross via New York Times) After interviewing numerous people who were present at the briefing led by Royce on the morning of the accident, it was evident that there was no discussion of assigned altitudes or show lines prior to the start of the performance. In fact, the only altitude anyone could recall being mentioned was the height that would give the crowd the best photo opportunities during the photo pass at the end of the performance. Instead, interviewees stated that for an air boss directed performance, the exact choreography to be used would not normally be decided at the briefing, but would be determined by the air boss during the performance. Agreement on this matter was nearly universal among attendees, who all said that the briefing was standard, resembled other briefings, and did not appear to be missing any information that they expected to hear. Furthermore, while FAA guidance for air shows stated that a briefing had to take place, the NTSB could find no official document specifying what means of separation were to be used in an air boss directed performance, nor any document requiring that such means be discussed in the briefing. In fact, the FAA inspector in charge of monitoring the air show was at the briefing and reported nothing unusual about it. Once the performance was underway, Russell Royce could have assigned the bombers and fighters to different altitudes, but the transcript of the air boss frequency revealed that he never did so. While he did assign an altitude to the bombers during one particular pass, the assignment was intended to achieve the desired visual effect for the crowd, and not to provide separation from the fighters, which hadn’t taken off yet at the time. Throughout the remainder of the performance, the bombers generally flew at 200 to 500 feet and then climbed to altitudes generally lower than 1,000 feet; while at the same time, the fighters generally flew between 800 and 1,300 feet, except during the accident pass, where they descended to between 100 and 600 feet. However, these altitudes appeared to be chosen by each individual crew without coordinating with other crews or with the air boss. In fact, when the NTSB asked whether there were any limits on how high or low the performers could climb or descend, Royce replied that the maximum altitude was the ceiling of the air show airspace and the minimum altitude was the ground. However, Royce didn’t stop there. Despite listing altitude as a means of separation at his disposal, in his interviews he stated that as a matter of practice he rarely assigned altitudes because he wanted the pilots to look outside and maintain awareness of other aircraft rather than looking inside at their altimeters. In fact, he asserted that assigning altitudes is actively dangerous and only serves as a distraction. Nevertheless, he professed a belief that the fighters had an obligation to be “on top of the bombers” and that he expected the fighters to remain above the bombers as they maneuvered to the 500 foot show line during the accident dog bone. He did not appear to show any awareness of the fact that the lowest fighter was below the highest bomber at numerous points throughout the entire performance, nor did he explain why, if such an obligation existed, it was neither reviewed in the briefing nor specified in any regulation. Furthermore, in a separate interview he expressed an expectation that the bombers would be at 200 feet on the 1,000 foot show line, and that the fighters would be at about the same altitude on the 500 foot show line — which is exactly what both groups actually did — without explaining how this was supposed to be reconciled with his expectation that the fighters would cross over the top of the bombers. Nor did he explain how the pilots were supposed to know his intentions with regard to altitudes when he never mentioned altitudes on the radio. The inevitable conclusion to be drawn from this mess of answers is that altitude separation was not being used. But what about other types? Smoke rises from the crash site seconds after the accident. (NBC News) The next type of separation listed by Royce was lateral separation. In the context of an air boss directed performance, lateral separation could mean assigning the bombers to the 1,000 foot show line and the fighters to the 500 foot show line prior to the start of the performance as part of a racetrack pattern in which the faster fighters would fly a longer circuit around the outside of the bombers. However, Royce only assigned show lines on the fly, based on his assessment of where the two groups were. As a result, while he did send the bombers to the 1,000 foot show line and the fighters to the 500 foot show line on every pass, he did so without putting in place any means to ensure that the fighters flew outside the bombers’ circuit. If the fighters were cutting inside the circuit, then they would have to cross paths with the bombers to reach the 500 foot show line. The fact that Royce not only didn’t ensure that the fighters were orbiting outside the bombers, but actually ordered the fighters to cut inside the bombers’ orbit to reach their show line, indicates that he was not considering the use of different show lines as a means of lateral separation. Bizarrely, in his interview Royce denied that he told the two groups to cross paths, arguing that he told the fighters to go up and to the right while the bombers were flying down and to the left, and thus away from each other. But not only did he not actually tell either group to fly “up” or “down,” it should be self-evident that telling the fighters to fly right and the bombers to fly left when the fighters are left of the bombers requires them to cross paths! When confronted further on this matter, he placed all responsibility for avoiding cross-track maneuvers onto the lead pilots, simply stating, “I don’t put constraints on them.” But while it was true that the fighters were located outside the bombers’ circuit at the time he first instructed the fighters to “get in front of the bombers,” he then told the fighters to cut the corner, forcing them to cross inside the bombers’ flight path. It is unclear how the fighter pilots were supposed to comply with this directive without crossing paths with the bombers. So much for “not putting constraints on them!” An aerial view shows the B-17’s tail in the foreground, main B-17 wreckage in left background, and P-63 wreckage in right background. (NTSB) Elaborating on his views, Royce went on to explain that keeping the airplanes a certain distance apart wasn’t really his job at all, because the minimum safe separation between aircraft would depend on pilot experience, aircraft capabilities, and the moment-to-moment operating environment. However, this response was factually incorrect, because for aircraft not part of a formation, there was a legally imposed minimum separation distance of 500 feet. Any closer and a formation flying certificate would be required. In his interviews, Royce did not appear to understand that the bomber group was not a formation and that the bomber pilots were not necessarily required to have a formation rating, and thus that there was an obligation to keep the fighter formation at least 500 feet away from the bomber group at all times. In fact, throughout his interviews Royce repeatedly referred to the fighters and bombers together as a single formation, or as separate formations, even though neither was the case. At this point it should be clear that Royce didn’t view separation as his responsibility, even when a certain amount of separation was required by regulations. That’s why the NTSB pointedly asked who he thought was responsible for separation, if not the air boss? In one instance, Royce replied that the answer was “everyone.” Describing his own role, he explained that he assisted in separation “from the perspective of the crowd,” but it sounds like he meant keeping aircraft separate in the crowd’s field of view so that no aircraft was hidden behind another. Later, he explained that official documentation doesn’t address deconfliction, so any obligation to separate aircraft would depend on whether the pilots’ contract says separation has to be done a certain way, or based on common sense — “I’m in front of you, I shall go first,” as he put it. And in another instance, he said that avoiding each other was the pilots’ job — “I do a lot of assignment of responsibility.” So in essence, no systematic deconfliction practices existed; the air boss gave little thought to separation in his commands; and pilots were expected to utilize the principle of “see and avoid” when complying with his instructions. Another aerial view looking back toward the point of the collision. (NTSB) This conclusion necessarily raises two more questions: were the aircraft complying with Royce’s instructions when they collided, and was it possible for them to have seen each other? Regarding the first question, Royce seemed to think that the answer was no. During one interview, he said that crossing one group over another wasn’t an issue — “we do it all the time… it’s never a problem” — but that the P-63 wasn’t where it was supposed to be. However, in a separate interview, when asked whether he perceived the P-63 to be in the correct position before the crash, he complained that that was “not a valid question.” With no way of knowing what Craig Hutain was actually thinking as he executed the final maneuver, it isn’t possible to say whether he understood the instructions correctly or not. In general, the air boss only issued instructions to the lead aircraft in each group or formation, and the other aircraft were expected to follow them. But in interviews with the NTSB, the pilots of both P-51s stated that they believed the air boss had told them to go to the 1,000 foot show line — which is what they did — even though this was incorrect. Then, as the P-51s maneuvered to align with the 1,000 foot show line, the P-63 did not follow them, but rather swung wider, apparently toward the 500 foot show line, which is where Royce had actually told them to go. Most pilots in the performance stated that when flying in trail, their focus was on following the aircraft ahead of them, not on the air boss’s instructions, which would appear to cast doubt on the notion that Hutain broke out of the formation on purpose. But those who knew Hutain believed otherwise. When asked about Craig, his longtime friend and colleague Jim Lasche said, “ I can just imagine him, he’s told ‘be at the 500 foot line now,’ that’s where he’s going to go.” This NTSB diagram of the flight paths leading up to the point of impact shows how the P-63 swung slightly wide toward the 500 foot show line. It’s also worth noting that around the time Hutain moved toward the 500 foot show line, Royce said, “Nice job fighters, you’re coming through first, that will work out.” However, because at that moment both the B-17 and the fighters were headed directly toward him, Royce wasn’t actually in a position to judge whether the P-63 was ahead of or behind the B-17. If Hutain didn’t have the B-17 in sight, then this unfounded comment by Royce could have added to his belief that he was clear to cut over to the 500 foot show line. With no disagreement about the fact that the B-17 was right where it was supposed to be, and a reasonable likelihood that the P-63 pilot was also trying to get to his assigned show line, it appears in hindsight that both aircraft were probably complying with the air boss’s directives when they collided. Despite this, Royce insisted in his interviews that his directives were normal and that pilot compliance represented the biggest safety issue. Because there was no time for every aircraft to read back his commands in a dynamic, fast-changing environment, he had to trust that everyone understood his instructions correctly, and the only way to verify comprehension was compliance. Furthermore, in his view, because he had asked the fighters to “get in front of the bombers,” the fact that the P-63 moved to the 500 foot show line without getting in front of the B-17 amounted to non-compliance. But if we think about it in another way, that sounds an awful lot like a fancier way of saying, “If I tell you not to hit each other, and you hit each other anyway, that’s your fault because I told you not to.” It’s a complete abdication of responsibility and a violation of the pilots’ trust. In this image from the NTSB’s visibility study, it’s clear that the B-17 wasn’t easy for the P-63 pilot to see. In addition to being a copout, this philosophy is also problematic because, as I stated earlier, visual separation is less effective than procedural separation unless definitive visual contact has already been made. Also, only the lead fighter was asked to confirm the B-17 was in sight; no one asked the P-63. And when Royce asked whether the B-17 had the fighters in sight, he described the fighters as being “in front,” when the P-63 was actually behind, so when the B-17 replied in the affirmative, there was no way to know whether Len Root had looked back to find the third fighter or whether he only saw the P-51s. So could the two crews actually see each other? To find out, the NTSB conducted a visibility study using a flight simulator. What they found was that when the air boss asked the B-17 if the fighters were in sight, the P-63 was visible through the rear left side window, over the captain’s left shoulder; but after 5 seconds it became obscured by the captain’s window pillar until the collision. Under the circumstances, it was highly unlikely that either of the B-17 pilots would have seen the P-63 coming. Furthermore, it would not have been obvious that the P-63 was on a collision course until about 7 seconds before the crash, when it appeared to break formation with the other fighters. That was less than the time it would take for one of the two scanners, who were not current and qualified pilots, to spot the incoming aircraft, determine that it was a threat, tell the pilots, and for the pilots to take evasive action. Meanwhile, starting from the time that the air boss asked the fighters whether they could see the B-17, the latter was visible to the P-63 pilot for 16 continuous seconds near the lower right side of the canopy. The B-17 only became obscured by the P-63’s upturned right wing about four seconds before the crash. However, Craig Hutain’s focus at that point was probably ahead and to the left as he attempted to follow his lead plane and find his assigned show line. Furthermore, the B-17 was still wearing its original military colors — that is to say, olive drab camouflage. This paint scheme was specifically chosen in order to make it harder for enemy fighters to spot the bomber from above against a background of trees, buildings, and earth. Hutain wasn’t an enemy but the color scheme doesn’t discriminate; as far the paint knew, it was doing its job. The NTSB concluded that while it was theoretically possible for Hutain to have seen the B-17 if he focused on keeping it in sight, there were also good reasons why he might have glanced over where he thought it was, seen nothing, and decided that he was in the clear. As for the slower, less maneuverable B-17, their chances of avoiding the collision were almost nil. But these findings merely confirmed decades of NTSB research into the dangers of relying on visual separation, especially in complex maneuvering environments. So the discovery that “see and avoid” was inadequate for air show deconfliction was hardly a surprise to anyone who had been paying attention. Most of the wreckage of the P-63 came to rest in this area. The engine block and nose landing gear can be seen separated from the main portion of the aircraft. (NTSB) To summarize, then, the air boss wasn’t using procedural separation techniques to deconflict aircraft and expected pilots to avoid hitting each other no matter what directives he gave them. If this attitude strikes you as unreasonable, you wouldn’t be alone. In fact, several people who were interviewed by the NTSB expressed displeasure with Russell Royce­ — and even more with his dad. Among interviewees, there was widespread agreement that Ralph and Russell Royce possessed a shared approach to air bossing, whether they liked that approach or not. And in fact, quite a few people came out of the woodwork to mention previous close calls involving both air bosses, including just two weeks earlier at Wings Over Houston, under the direction of Royce senior. One of those people was the Vice Chairman of Wings Over Houston, a CAF pilot who flew as copilot of a bomber in an air boss directed performance during that air show. According to his recollection, Ralph Royce ordered Craig Hutain’s P-63 to cut in between his aircraft and the bomber ahead of him with no altitude separation. The pilot in command of his bomber confirmed that the incident had taken place and added that the P-63 came within 100 feet of his aircraft. According to both the Wings Over Houston Vice Chairman and CAF Chief Aviation Officer Jim Lasche, several pilots were so incensed at the incident that they met after the air show to commiserate, and one of them even confronted Ralph Royce and told him that he was “going to kill somebody if you do this again.” But the confrontation seemed to result in more ass-covering than soul-searching. In fact, according to the Wings Over Houston Vice Chairman, Royce senior tracked him down a month after the accident and asked him what he had told the NTSB about the Wings Over Houston incident, then insisted to him that the P-63 wasn’t at his altitude and that the maneuver was perfectly safe. After that conversation, the Vice Chairman said that he was bothered by the senior Royce’s dismissiveness of his concerns and his insistence that he disbelieve his own eyes. This wasn’t the only alarming incident described in the NTSB testimony either. According to the Wings Over Houston Vice Chairman, he had previously experienced an event in which Royce senior ordered him and another aircraft to turn into the same show line at the same time from opposite directions; realizing that this would put him on a collision course, he refused to comply. And in yet another incident, also at the air show in Houston two weeks before the crash, the pilot of a C-47 told the NTSB that the air boss sent a Messerschmitt Me-262 head-on into his aircraft on the show line, causing the replica German jet fighter to pass directly beneath him when he was only 200 feet above the ground. He was unaware that the Me-262 would do this until the air boss ordered it; the maneuver was not briefed beforehand; and it was not debriefed after. A closer aerial view of the B-17 main wreckage area. (NTSB) In order to get a second opinion, the NTSB interviewed several people with decades of air show experience who were not involved in Wings Over Dallas, including two air bosses who regularly directed other major air shows. Both of those air bosses stated that they normally determined the choreography for an air boss-directed performance in advance of the briefing and would assign altitudes and show lines to ensure separation between aircraft in different groups or formations. The typical minimum separation between aircraft was 200 feet and they did not normally allow aircraft with different cruising speeds to fly together. The air bosses said they were expected to carry out the pre-planned choreography without deviation. The Wings Over Houston Vice Chairman agreed that other air bosses didn’t use “freeform” directives as often as the Royces did, and said that he normally expected the fighters to remain above the bombers using pre-briefed assigned altitudes. Separately, the CAF Director of Operations said that it was a “mistake” to send the fighters past the bombers without 500 feet of separation, describing Russell Royce’s directive as “not the right way to do it.” “I would have thought it would have been intuitively obvious to the most casual observer that you don’t do that sort of thing, but apparently it’s not,” he said to the NTSB. And regarding the absence of a planned choreography, he opined that if a “new guy” were to attend the briefing, he would probably come away knowing only to “follow the guy in front of him and hope nothing happened.” Without a moment’s pause, he added, “And that to me is not right.” Finally, joining the chorus of voices, CAF Chief Aviation Officer Jim Lasche said that Royce was wrong to avoid assigning altitudes, because, as he put it, when a fighter is flying along at 250 knots and the pilot hears confusing directions, there needs to be something for them to fall back on, like an assigned altitude and show line — but there wasn’t. None of the interviewees made much mention of the fact that Royce also sometimes used unclear language and terminology, like “walk your way up” and “keep it a little flat.” Royce himself was unable to explain exactly what he meant when he used some of these phrases. However, it was agreed by both the NTSB and other experts whose commentary I reviewed that the use of this kind of language in a highly dynamic environment can and did cause confusion. When airline pilots and controllers talk on the radio, they’re expected to use certain established terminology so that the intent of a statement is as unambiguous as possible — so why shouldn’t we expect the same from an air boss? Most of the B-17 was destroyed in the impact and fire. (NBC News) And yet, despite criticism from these authoritative voices, the majority of interviewees spoke positively or neutrally of Russell Royce and his air bossing techniques. For instance, the pilot of the B-24 immediately behind the B-17 said that Royce didn’t do things particularly differently from other air bosses. The pilot of the second P-51, himself an accredited air boss and warbird safety researcher, said that Russell was the best in the business. The FAA inspector and the trainee inspector both said they observed nothing unusual or incorrect about Royce’s air bossing on the day of the accident. And the air boss observer shadowing Royce praised his ability and took his side on the question of separation, arguing that pilots always shoulder the primary responsibility for knowing where the other aircraft are, and noting that when he was in the Air Force flying jets, the obligation to maintain that awareness didn’t go away just because he was in a high G-maneuver with his belly up to someone else. But he also added that performers expect the air boss “to be ahead of the game, to understand what’s going to happen so as to avoid conflicts down the road,” and it’s not clear how to reconcile this statement with Royce’s actual performance. The most important voice of support among the interviewees was Wings Over Dallas air show Chairman Gena Linebarger, who was responsible for selecting the air boss. She described Royce as a “perfectionist,” said she liked the way he did things, and explained that she always hired the Royces if she could. Her relationship with them dated back 10 years and included numerous air shows. In fact, her confidence in Russell was so great that she told the NTSB, months after the accident, that the only reason she hadn’t hired him for her next air show was because their insurance wouldn’t let her. The clear takeaway here is that the air show community was deeply divided on the question of whether the Royce duo were cowboys. And with no consensus that their techniques were in any way abnormal, the air boss training and accreditation process needed to receive greater scrutiny. ◊◊◊ The tail of the B-17 with the Dallas Executive Airport control tower in the background, and airliners on approach to Dallas Fort Worth International Airport. (Dallas Morning News) As I mentioned earlier in this article, Russell Royce began air bossing with on the job training from his dad, before later receiving a letter of authorization (LOA) designating him as a “recognized air boss, multiple venues.” The typical training for an air boss lasted about four years and involved observing other air bosses, discussing theory, gaining radio experience under supervision, and other skill-building practices. These same type of activities are included in the International Council of Air Shows (ICAS) Air Boss Recognition Program (ABRP), which Royce underwent less than three years before the accident when the requirement was first introduced. The ABRP came with a training manual against which air boss applicants were to be judged by an ICAS-approved “air boss evaluator,” who would observe the applicant directing an air show and assesses their performance against the criteria on an evaluation form. Achieving full recognition also required a record of air show experience, letters of recommendation, completion of an “air show education data sheet,” and a score of 75% or better on a multiple choice test. Should the applicant meet these requirements, the FAA would issue an LOA on ICAS’s recommendation. The triannual renewal process for the LOA was described earlier in this article, but it bears repeating that the process did not include recurrent training or evaluations. The air boss community is small and insular. At the time the NTSB report was written, there were only about 60 to 70 ICAS-approved air bosses, and only 6 to 7 of them were air boss evaluators. Although close friends and family are forbidden from evaluating each other, it’s plausible that almost all air bosses in the United States know one another through industry connections and ICAS conferences. In such a community, outside monitoring by the FAA is probably necessary to ensure that corners don’t get cut. However, the NTSB found that FAA oversight of the process was minimal. The agency checked in on ICAS on a quarterly basis but was not closely involved in the accreditation of air bosses, nor were FAA inspectors at air shows given guidance on how to spot improper air boss behavior. In fact, their guidance was mostly aimed at ensuring that the provisions of the airspace waiver were met; that is to say, that the performers held the correct licenses, that the airplanes stayed within the designated airspace, that the briefing was held at the proper time, and so on. Since the inspectors were not experts on air show or air boss conduct, nor were they given any training or checklists related to those topics, it was unlikely that they would independently identify safety issues. The matter was considered so secondary that the FAA inspector didn’t even have a radio with which to listen to the air boss frequency. The trainee inspector did listen to the air boss frequency but was unable to identify anything out of the ordinary about the communications. NTSB investigators examine the tail section of the B-17. (NTSB) With the FAA exercising minimal oversight of the air boss training, accrediting, selecting, and monitoring process, the burden of ensuring the quality and trustworthiness of each air boss fell disproportionately onto the air show organizers themselves. In the case of Wings Over Dallas, that was the Commemorative Air Force. As this article has already established, Chairman Linebarger, a CAF employee, selected Russell Royce as the air boss because she liked his “way of doing things.” But as Linebarger herself freely admitted, she was too busy during air shows to actually monitor the air boss’s performance, and even if she could monitor it, she hadn’t received any training or guidance on what proper air bossing was supposed to look like, other than her own lived experience. In fact, she told the NTSB that the only way for her to find out about a problem with the air boss’s directives was for someone to bring it to her attention. However, as we’ve already noted, many senior CAF pilots didn’t see anything wrong with the way the Royces directed performances either. Furthermore, a small group who did have complaints about Ralph Royce after Wings Over Houston didn’t bring their concerns to the Air Show Chairman or indeed any other authority figure. It was therefore quite probable that Chairman Linebarger had little idea that the Royces’ freeform air bossing technique was controversial, or that other air bosses employed better safeguards. Even months after the accident, some interviewees didn’t appear to understand what the problem was, including Chairman Linebarger herself. In one regretful interview passage, the NTSB asked her what she would do differently if she were to organize a 2024 rendition of Wings Over Dallas, to which she replied that she wouldn’t do anything differently at all. And when further pressed on whether she had learned anything from the disaster, she said, “I learned how quickly things can change in a blink of an eye regardless of how well you’ve planned it; anything can happen. But you just have to move forward.” Is there any interpretation of that statement other than an admission that she learned nothing? I don’t want to be overly judgmental considering that some people freeze up when put on the spot, but it’s hard to imagine what would be a worse way to answer those questions. Another view of NTSB investigators examining the tail section. (Dallas Morning News) Based on extensive testimony gleaned from multiple interviews and other information sources, this attitude appears emblematic of a larger problem with the safety culture at the CAF. The Commemorative Air Force has some structural and historical barriers to achieving the kind of open, responsive, regimented safety culture that you might find at an airline. As an organization composed of volunteers, almost all of them retired pilots, the average CAF member is old and set in their ways. For instance, Chief Aviation Officer Jim Lasche described visiting a CAF wing that was having problems and discovering that everyone was over the age of 75 and the lead mechanic was 84. He said that many CAF members didn’t like it when he made changes, were uncomfortable around computers, and resented requirements being imposed upon them. Some of the units had turned into “old boys clubs,” ostensibly representing the warbird community in a major metropolitan area without adding any new members for a decade or more. And most importantly of all, everyone was there because they wanted to fly warbirds — and the implicit fear that that privilege could be taken away sometimes created a culture of silence. In a robust organization, that pressure can be alleviated by erecting clearly defined and well-understood safeguards for members who want to express safety concerns. But in a tight-knit organization full of experienced people, each of them bringing a lifetime of agendas and opinions to the table, abuse was known to occur. For instance, Lasche mentioned one incident in which a CAF member unsuccessfully propositioned a female wing mate, then falsely reported that her plane was unsafe in order to get back at her for turning him down. And while Lasche freely gave out his personal phone number as a 24/7 safety hotline, he stated that some people would call him just to rant or vent off steam, sometimes at odd hours of the night. But perhaps the most concerning story about the CAF’s internal culture was directly related to Wings Over Dallas. As you might recall, a two-seater Stearman on a revenue ride flight landed on runway 31 at approximately the same time as the collision. The NTSB was alarmed not only because the Stearman was potentially distracting for the air boss during a critical period of the show, but also because the plane was nearly struck by falling debris from the colliding aircraft, which could have injured or killed the occupants, one of whom was a paying passenger. In his interview, Jim Lasche stated that it had always been CAF policy to run revenue ride flights during breaks between warbird performances, when no other aircraft were airborne. But in recent years, air show timelines had been condensed to eliminate breaks that would cause some of the audience to leave. Because revenue ride flights are crucial to balancing the CAF’s books, this in turn resulted in pressure to conduct revenue rides during performances. On the other hand, Lasche felt that having paying passengers airborne in the same airspace as an ongoing performance was dangerous, so he came out against the move. But he “acquiesced” after receiving intense pushback from what he described as “higher up people,” including the air boss. As a compromise, he agreed to allow revenue ride flights during performances, but only as long as they didn’t take off or land while another act was flying. However, this agreement apparently wasn’t recorded in writing and wasn’t properly enforced, because the air boss authorized several revenue ride flights to take off or land during the accident performance, including the Stearman that almost became collateral damage. The decision to allow these takeoffs and landings was made without Lasche’s knowledge and he was upset to find out about them after the fact. Nevertheless, the Stearman issue made it clear that concerns such as entertainment value and revenue were sometimes taking priority over safety, and that there was no framework in place to ensure that safety won out. NTSB investigators examine the main B-17 wreckage. (NTSB) Years before the accident, Lasche had attempted to create such a framework by implementing a safety management system, or SMS. The idea behind an SMS is to provide a secure, anonymous avenue for members to report incidents and concerns, creating a stream of data that can then be analyzed to detect unsafe trends. The CAF wasn’t required to have an SMS, but implementing one was undoubtedly a good idea and it could have been very useful, in theory. Lasche explained that he was inspired to develop an SMS after he asked a group of CAF pilots where they thought the organization’s next accident would come from, only for everyone present to agree, without hesitation, that a particular named person was at risk. When that very pilot killed himself and a passenger in a crash a few weeks later, Lasche concluded that if a system had been in place to turn those concerns into action, lives might have been saved. But the SMS as actually implemented was something of a disappointment. Despite efforts to make sure all CAF members knew how it worked, the system was rarely utilized and received zero reports during the year leading up to the crash. Nevertheless, Lasche never really tried to figure out why. The NTSB rightly pointed out that receiving zero reports is usually indicative of a lack of understanding or trust in the SMS rather than a lack of safety issues to report. It was clear that some CAF members preferred to keep reporting issues using Lasche’s personal phone number, but the lack of anonymity in that approach and the absence of detailed record-keeping limited its usefulness. Nor was this problem confined to the CAF, because ICAS had its own similar safety reporting system that only received 10 reports in 15 years. In its final report, the NTSB wrote, “An organization’s culture can become unhealthy if motivational factors exert influence that turn it into something colloquially known as “don’t rock the boat” syndrome.” The investigators felt it was understandable that such a culture could develop at the CAF, where members paid for the privilege to participate and didn’t have the protections afforded to an employee. Further, the NTSB added, “ In such cases, the performance can become more about thrilling the crowd, sometimes to the detriment of aviation safety. While some air show event organizers have actively worked to apply administrative controls to preclude such performances, the circumstances of this accident suggest that, for some parts of the warbird community, it may persist.” Another view of the B-17 main wreckage. (Unknown) The NTSB notes that these circumstances created a culture where members did not speak up about deficiencies. But I would like to point out that the CAF interviews show many members didn’t recognize that safety deficiencies existed in the first place. I want to caveat this section by mentioning that I am not a CAF member, not a pilot, and not an expert on air shows. Weeks of research don’t replace or equal years of experience. But in all the testimony I read and all the guidance I reviewed, I didn’t find a credible defense of the way that the accident performance was carried out, not from Russell Royce nor from anyone else. Royce’s beliefs about the proper way to conduct an air boss directed performance were inconsistent with basic, well-established safety principles that transcend most modes of aviation. It is a fact that in a complex maneuvering environment involving multiple aircraft, especially dissimilar aircraft, visual separation is insufficient to preclude an accident, and it is a fact that the use of procedural separation in addition to visual separation is superior to visual separation alone. Of course pilots should be keeping each other in sight; that’s a basic part of flying any aircraft. But that doesn’t mean that the air boss shouldn’t take easily available measures to reinforce the imperfect substrate of see-and-avoid, and the existence of visual separation certainly doesn’t allow the air boss to wash their hands of all responsibility when two pilots inevitably and predictably lose sight of each other and collide — because saying, “that’s too bad, I guess they should have looked harder,” and then moving on to the next air show, is a surefire way to kill another 6 people next year. As seasoned aviators, often with decades of experience in the highly regimented and extremely safe world of passenger airlines, most CAF pilots and managers should theoretically have been in a position to recognize that Royce wasn’t using procedural separation, and that this represented a serious safety issue. But not only did nobody speak up about this before the accident, several prominent CAF members interviewed by the NTSB still didn’t see anything wrong with it even six or eight months later. As far as they were concerned, Russell Royce’s freeform air bossing style was just the way things were done, and the possibility that “the way things were done” was unsafe and should be improved may not have occurred to them. Looking in from the outside, as someone who hasn’t spent my life in the air show environment, and as someone who knows what the consequences of these unsafe practices turned out to be, it’s easy for me to say they should have reacted differently. But I won’t say that, because it’s a well-documented fact that human beings, existing day in and day out within a system where safety measures are not being used, will quickly become accustomed to the unsafe environment even if they know, in principle, what a safe environment ought to look like. And that’s what sociologist Diane Vaughan famously called “normalization of deviance.” Aerial view of the tail section of the B-17. (NBC News) The normalization of deviance is the process by which actors within a complex system featuring complex safety requirements unconsciously tolerate increasingly unsafe behaviors and practices as long as those practices are not met with adverse consequences. It’s really not a very difficult concept to understand, because all of us have probably experienced a basic version of it at some point in our lives. For example, I spent years writing articles using a laptop with broken keys, even though this resulted in a greatly elevated number of typographical errors, simply because I had become accustomed to my little workarounds for using the keyboard and catching mistakes. But if someone else had tried to sit down and write using my laptop, they would have been frustrated beyond belief. So while it would have been better to get my computer some professional help, I never did, even though I knew the problem existed and I could afford the repairs. The point is that when you’re inside the unsafe system, it can be hard to see the forest for the trees, and sometimes even if you do, inertia can prevent change. This was probably especially true at the CAF, an organization with a tight-knit membership, disincentives to rocking the boat, and a unique mission that requires flying 85-year-old aircraft with all the safety issues inherent to that. And when you’re up there in a formation, flying a B-17 Flying Fortress or a P-51 Mustang, living the dream in front of thousands of awed spectators, who really wants to be the one to say, “Hey, maybe we shouldn’t be doing this?” Hell, who even wants to think that? ◊◊◊ In October 2023, a memorial to the lives lost in the accident was erected in Conroe, Texas, home of the CAF unit that operated the B-17. (The Courier of Montgomery County) To its credit, the CAF made several changes to its operations after the accident without waiting for NTSB recommendations. In 2023, CAF management drafted new air show guidance banning CAF aircraft from participating in air boss directed performances that included aircraft of differing performance; multiple parades of different aircraft categories without at least 500 feet of altitude separation; or maneuvers other than racetrack patterns. Dog bone turns and crossing paths would only be allowed as part of an approved maneuvers package. Air bosses would be required to meet these stipulations before the CAF would agree to participate. At the same time, the FAA issued a Safety Alert For Operators (SAFO) recommending that air show organizers provide all pilots with a detailed written plan, including well-defined separation strategies, in advance of any performance. The SAFO also stated that maneuvers other than racetrack patterns were not recommended for air boss directed performances, and linked to a new ICAS document that outlined a strategy for using altitude blocks and lateral buffers to separate aircraft. A form that air bosses could use to assign aircraft to these blocks was also included. In its final report, the NTSB made several recommendations that went even farther. These included the following: · The FAA and ICAS should establish air show standard operating practices addressing procedural separation in air boss directed performances, risk assessments for air show operations, and a post-show debriefing with reporting of results to the FAA and ICAS. · The FAA should require recurrent evaluations upon renewing air boss letters of authorization. · ICAS should develop a set of standard terminology for air bosses to use when describing air show maneuvers. · FAA inspectors should observe air boss performance during air shows and provide feedback during the debriefing, and the FAA should provide its inspectors with evaluation guidance. · The CAF should use existing FAA guidance to develop a risk assessment and mitigation process tailored to its unique operations. So far, the effect of these reforms remains to be seen. Some changes are already visible; for example, Chief Aviation Officer Jim Lasche said he refused to allow CAF airplanes and pilots to participate in a February 2023 air show where Russell Royce was the air boss. Later, the family of Len Root sued Royce for negligence, and the CAF for hiring him. The outcome of that suit is pending as of this writing. A makeshift memorial honors the victims near Dallas Executive Airport. (Fort Worth Star-Telegram) However, one of the points I want to end on is that while there were characters in this story who came across very unfavorably, a true safety reckoning doesn’t stop at blaming those people — in fact, searching for blame isn’t really part of the process at all. Everyone in this story was a product of the environment that they operated in, while at the same time, each contributed in some unconscious way to the perpetuation of that environment. It requires vision and drive to change the culture of an organization — or it can require tragedy, lawsuits, and new FAA regulations. But both methods require that the root causes of unsafe practices be identified, and it’s clear that the practices that led to the disaster at Wings Over Dallas didn’t start with Russell Royce, or with his dad, or with the air show chairman. They probably didn’t even start with the CAF. Most likely, those practices arose from the informal origins of warbird demonstrations — a few guys getting together on weekends to pluck ex-military aircraft from boneyards to fly for fun — and those origins created attitudes that shaped the institutional character thereafter. And because of the Wings Over Dallas tragedy and other recent warbird incidents, it’s now up to the current generation of warbird enthusiasts to ensure that it remains possible to uphold the CAF’s mission — to preserve the aviation heritage of the Second World War for the education and enjoyment not only of current, but also future generations. In that mission, I wish them luck. I am not and probably never will be a warbird enthusiast, but it nevertheless fills me with a certain wonder to look up and see a B-17, its mighty radial engines turning fuel into noise, like a beast out of another age, brashly carving its way through our modern skies. I hope that some lucky few will one day get to ride on a B-17 as it celebrates its hundredth birthday. The biggest obstacle to that milestone isn’t the age of the planes, but the way that we treat them, and if nothing is learned from this latest tragedy, then not only will we lose this part of our history — we will deserve it. _______________________________________________________________ Thanks for your patience in waiting for this article! After publishing my piece on EgyptAir 804 in December, I moved half way across the country in a long, messy relocation process fraught with other struggles along the way. But here I am, and here it is. Thank you! _______________________________________________________________ Don’t forget to listen to Controlled Pod Into Terrain, my podcast (with slides!), where I discuss aerospace disasters with my cohosts Ariadne and J! Check out our channel here , and listen to our latest episode about a titanic battle between a BAC 1–11 and some wind. Alternatively, download audio-only versions via RSS.com , or look us up on Spotify! _______________________________________________________________ Join the discussion of this article on Reddit Support me on Patreon (Note: I do not earn money from views on Medium!) Follow me on Bluesky Visit r/admiralcloudberg to read and discuss over 260 similar articles Bibliography",Passing the Buck: The story of the 2022 Wings Over Dallas air show collision,"

Key Points:
",Software Development,"Passing the Buck: The story of the 2022 Wings Over Dallas air show collision Admiral Cloudberg · Follow 59 min read · 1 day ago -- 7 Listen Share A Boeing B-17 and a Bell P-63F explode into shrapnel a split second after colliding in midair at the Wings Over Dallas air show. (David Walsh) On the 12th of November 2022, thousands of spectators at the Wings Over Dallas air show in Dallas, Texas bore witness to a sudden tragedy, as two WWII-era warplanes collided in midair during a performance, killing 6 crewmembers. In numerous spectator photographs and videos, a single-pilot Bell P-63 fighter could be seen arcing toward the Boeing B-17 “Texas Raiders,” its belly up in a steep left turn, until the two aircraft crossed paths, and in the blink of an eye the P-63 cleaved the larger bomber in two. Shaken by the loss inflicted on their tight-knit community, the air show’s volunteer-led organizers, the Commemorative Air Force, were left wondering whether the procedures used to prevent collisions during large formation flights might have some flaw that placed the two planes on a collision course. But the National Transportation Safety Board’s final report, and interviews with those involved, make clear that the problem was not so much that the procedures were flawed, but that no procedures existed. At Wings Over Dallas, the pilots of eight airborne aircraft, flying in close proximity to one another, were placed under the control of a so-called “air boss,” a role requiring only the bare minimum of qualifications, without prior knowledge of the maneuvers that the air boss would ask them to perform. Even worse, almost everyone involved seemed to think that this was normal — a remarkable example of what sociologist Diane Vaughan termed “normalization of deviance.” What follows is therefore not only the story of the disaster at Wings Over Dallas, but also the story of how the Commemorative Air Force, the Federal Aviation Administration, and the International Council of Air Shows created the circumstances for it to occur, all without recognizing that anything was amiss. _________________________________________________________________ Note: I am an outsider who has written a detailed story that is personal to many people. Several people are mentioned by name and I am aware of the possibility that they or their families might read this article. If you were involved or knew someone who was, and you notice incorrect biographical information, please send corrections to my publicly available email address. Thank you! _________________________________________________________________ ◊◊◊ A squadron of B-17s on a bombing run over Germany. (Stock photo) During the Second World War, American factories produced tens of thousands of warplanes to fuel the ferocious air battles over Europe and the Pacific, mass producing fighters, bombers, and transports on a scale matched neither before nor since. Untold thousands of these aircraft fell in battle, spiraling aflame from hostile skies, and an equal or even greater number were lost in accidents — destroyed in hard landings, smashed against Himalayan mountaintops, or lost at sea, never to be found. And after the war was won, even more of these aircraft met an unceremonious end at boneyards around the world, torn apart for scrap just as quickly as they were put together. But a precious few survived long enough to fall into the hands of people who saw them as more than merely machines. Among those who dedicate their time to salvaging, restoring, maintaining, and flying the relics of WWII, these aircraft came to be known as warbirds . Not every historic military plane was considered a warbird, because not every war was the war, the one nobody needed to name because it was obvious. In the decades since, that definition has slipped, but the association remains. A selection of CAF warbirds, including the B-17 Texas Raiders. (Commemorative Air Force) Although the number of true warbirds, in the original sense, can only ever go down, a surprising number remain airworthy thanks to the efforts of private individuals and non-profit organizations. The largest such organization is the Commemorative Air Force, or CAF, which today operates over 180 historic military aircraft distributed between 75 local units in 6 different countries. But the CAF wasn’t always so large. It got its start in 1957 when a small group of former US Air Force pilots purchased a single P-51 Mustang for fun, only to discover that no one else was trying to preserve decommissioned warbirds for future generations. The group then set out to acquire at least one example of every American warbird, and the rest is history. At first, the Texas-based CAF branded itself the “Confederate Air Force” as a joke, but later changed its name when the tainted association with the Confederate States of America started to interfere with its loftier goals and more serious outlook. At the same time, the CAF grew from a small, ad-hoc group into a larger organization with its own operations manuals, pilot training programs, and more. As a registered non-profit run mostly by volunteers, the CAF sustains its operations by charging membership fees in exchange for the opportunity to fly its airworthy warbirds, and by collecting 9% of the revenue from events put on by each of its local chapters, known as “wings.” Each aircraft is assigned to a wing that shoulders responsibility for operating that aircraft and training new members to fly or maintain it, with day-to-day use largely left at the local unit’s discretion, as long as CAF policies and procedures are followed. For accuracy’s sake, it should be noted that the warbirds are officially owned by the American Airpower Heritage Flying Museum — a legally separate entity from the CAF, which merely operates the aircraft — but the CEO of the CAF and the CEO of the AAHFM are the same person. Fighters in formation during the CAF’s Tora! Tora! Tora! performance, which is an approved maneuvers package. (Bart Marantz) Although CAF units and individual aircraft frequently participate in air shows hosted by third parties, the Commemorative Air Force itself also organizes several air shows each year, typically in the fall. As part of the CAF’s 2022 event roster, the group hosted two back-to-back air shows in Texas that will become relevant to this story, starting with Wings Over Houston on October 29–30, and Wings Over Dallas on November 11–12, both of which featured many of the same performances, aircraft, and pilots. Among the types of activities that took place at these two air shows, three merit further discussion — namely, revenue ride flights; approved maneuvers packages; and air boss-directed performances. Revenue ride flights are pretty much what they sound like, in that members of the public pay a specified amount of money in exchange for the opportunity to ride in a warbird, usually under the Federal Aviation Administration’s Living History Flight Exemption, which allows such flights as long as the “ticket” price is structured as a charitable donation. Such flights are an important source of revenue for the CAF and are critical to its financial stability. Normally they take place during gaps between other types of performances. Next, an approved maneuvers package refers to a scripted, choreographed performance usually involving close formation flying. The pilots who participate in an approved maneuvers package have practiced every move, every turn, and every power adjustment dozens or even hundreds of times, relying on strict timing and precisely defined flight paths. The participants in such a package are able to fly the entire performance in close proximity to other aircraft without receiving continuous instructions. When most people think of an air show performance involving multiple airplanes, this is usually what they imagine — for my American readers, the Blue Angels are a great example of a group that uses maneuvers packages; or for Canadians, the Snowbirds. A group of bombers flies in trail during the air boss directed performance at Wings Over Dallas 2022. This photo was taken moments before the collision. (Jason Noyes) The third and final type of activity, and the most important one for this story, is an air boss-directed performance. An air boss is a person who gives instructions to aircraft during an air show, such as who is cleared for takeoff, who is cleared to land, who should fly where, and so on, using a common frequency for all aircraft within the air show airspace. The air boss isn’t an air traffic controller and doesn’t need to have any air traffic control background — more on that later — but the job is similar in some respects. The most difficult part of an air boss’s job, however, is directing an unscripted performance. In an air boss-directed performance, the air boss develops a desired flight path for the aircraft involved in the performance and then instructs those aircraft and pilots to follow that flight path, without the use of an approved maneuvers package. The performance is usually constructed by stringing together basic maneuvers like low passes and course reversals that are familiar to the pilots, but the exact sequence of maneuvers is not practiced beforehand, and the positioning of each aircraft relies on the air boss’s directions rather than pre-arranged timings, bank angles, and power settings. The air boss’s role in a directed performance has been compared to that of an orchestra conductor. To understand this type of performance, it’s also important to clarify the exact definition of “formation flying” and its implications. In its report, the NTSB describes a formation flight as “two or more aircraft under the command of a flight lead that are flown solely with reference to another aircraft in the formation.” Under US regulations, if the aircraft are less than 500 feet apart, the pilots must possess a special formation flying certification. Conversely, aircraft that are not part of a formation must remain more than 500 feet apart, and while a string of aircraft each separated by this distance may appear to be a formation, it is not. This distinction will become important when analyzing what went wrong at Wings Over Dallas. The layout of the air show. (Own work, map and data courtesy NTSB) With all of that having been said, let’s jump forward in time to November 12th, 2022, the second day of the Wings Over Dallas air show at Dallas Executive Airport, a general aviation field located 12 kilometers southwest of downtown Dallas. We’ll go back and look at Wings Over Houston later. The performers in the November 12th activities started their day by attending a morning briefing held by air boss Russell Royce. Under the conditions imposed by the FAA waiver authorizing the air show, the briefing was a required item, and at least one member of every flight crew had to receive the briefing or face being cut from the roster. Also in attendance were an FAA inspector assigned to monitor the air show; another inspector being trained on air show operations; and a second air boss observing Royce in order to gain warbird experience. Using a PowerPoint presentation, Royce explained the boundaries of the air show airspace, landmarks that could be used to find the boundaries, where to go in the event of an emergency, and most importantly for this story, the location of the two “show lines.” Dallas Executive Airport has two crossing runways designated 17/35 and 13/31 respectively. Runway 13/31 has a northwest-southeast orientation and is the longer of the two runways, as a result of which it was selected as the axis along which the air show demonstrations would take place. The crowd was positioned on bleachers on the apron adjacent to the east side of runway 13/31, near the intersection with 17/35. To give the crowd the best view, aircraft would make passes down runway 13/31 along one of two designated “show lines” located at 500 feet (150m) and 1,000 feet (300m) in front of the crowd, respectively, with the 500-foot show line running down the eastern, inside edge of the runway and the 1,000-foot show line running along the tree line at the western edge of the airport’s clear area, as shown above. Which show line would be used by which aircraft, and when, was outside the scope of the briefing and would be assigned by the air boss during the performance. However, the briefing did cover techniques that could be used to identify and align with the show lines. Texas Raiders at Wings Over Houston in 2019. (Alan Wilson) At the briefing, the air boss also distributed a schedule listing the expected start times for each act. One of the acts that day was the CAF’s Tora! Tora! Tora! performance, an approved maneuvers package reenacting the 1941 Japanese attack on Pearl Harbor and the American counterattack, featuring multiple warbirds that were used in filming the 1970 movie of the same name, as well as coordinated pyrotechnics. As far as I have been able to tell, this is the only approved maneuvers package used by the CAF, and they perform it at numerous air shows every year. Following Tora! Tora! Tora!, the schedule called for a “warbird parade” featuring five American bombers and three American fighters, which would make passes back and forth in front of the crowd in a series of simulated bombing runs. At the finale of the performance, the warbirds would be joined by a Boeing B-29 Superfortress, one of only two remaining airworthy examples. The five bombers taking part in the parade were to be led by the B-17 Flying Fortress “Texas Raiders,” followed by the Consolidated B-24 Liberator “Diamond Lil,” a Curtiss-Wright SB2C Helldiver, and two North American B-25s named “Devil Dog” and “Yellow Rose.” The plan was for the bombers to fly “in trail,” with the B-17 receiving instructions from the air boss while the other four aircraft followed its lead, maintaining at least 500 feet of separation because the group was not officially a formation and the pilots were not required to be formation rated. At the same time, the three fighters were led by a North American P-51 Mustang, followed by a second Mustang and a Bell P-63F Kingcobra. The fighters would fly in close formation and all three pilots were formation-rated. Effectively, this divided the parade into two groups that not only had greatly different performance characteristics, but also different rules governing minimum separation distances and pilot qualifications. The five crewmembers of Texas Raiders and the pilot of the P-63F. (Commemorative Air Force) Because the accident involved the B-17 and the P-63F, we’re going to take a closer look at those aircraft and their crews before continuing. The Boeing B-17 Flying Fortress is a four-engine piston powered bomber designed to drop large amounts of unguided ordnance onto enemy targets. The model entered service with the US Army Air Corps, the predecessor to the Air Force, in 1936 and saw widespread action in WWII, as over 12,000 B-17s dropped 640,000 tons of explosives over Nazi Germany. At the time of the accident, there were approximately seven airworthy B-17s remaining, including the airframe nicknamed “Texas Raiders.” This B-17 was license-built by Douglas in Long Beach, California in 1944 and was delivered to the US Army Air Forces in July 1945 before being transferred to the Navy, where it served as an Airborne Warning and Command System (AWACS) platform in the Korean War. The aircraft was retired from the Navy in 1957 and spent ten years flying aerial surveying missions in Alaska until the CAF acquired it in 1967. The B-17 could be flown with a bare crew compliment of three, but for Wings Over Dallas, five crewmembers had been rostered. In command was 66-year-old Leonard “Len” Root, a longtime CAF member and recently retired American Airlines pilot with over 28,000 flight hours and type ratings in 12 different airplanes, including 500 hours on the B-17. He was also the former leader of his CAF wing and was responsible for training new members. The second in command was 67-year-old Terry Barker, also a former airline pilot, with over 25,300 flight hours and experience in everything from gliders to helicopters to large passenger jets. He had 90 hours in the B-17 and was the chief maintenance officer in the same CAF wing as Len Root. The other three crewmembers consisted of 64-year-old flight engineer Curt Rowe, a 30-year veteran of the Ohio Civil Air Patrol, and two “scanners.” The purpose of the scanners was to stand by the B-17’s rear doors and keep lookout for other aircraft, but there was no requirement that the scanners have a pilot’s license, or indeed any qualification at all. Normally only one scanner would be used, but apparently Len Root requested a second one. The first scanner in this case was 42-year-old Kevin “K5” Michels, a CAF historian and tour supervisor, while the second scanner was identified as 88-year-old Dan Ragan, a former US Navy radio operator who served on Texas Raiders during the Korean War. It’s not apparent from the available evidence whether giving him the scanner role was a pretext to allow him to keep flying on his former aircraft, but it if it was, it’s kind of hard to begrudge him. The sole surviving P-63F, seen in 2019 prior to its involvement in the accident. The second aircraft involved in the accident was the Bell P-63F. The P-63 Kingcobra family of single piston-engine fighters was developed originally for the US Army Air Forces around 1943, but the type never saw combat for the United States. Instead, around 3,300 were built mainly for the Soviet Air Force. Numerous variants were also developed, many of which did not see widespread use, including the P-63F, which was distinguished from the base model by its enlarged tail and modified engine. The P-63F project was abandoned after just two aircraft had been built, and the CAF operated the only surviving example. The pilot of the P-63F was 63-year-old United Airlines pilot Craig Hutain, a veteran aviator who flew his first airplane at 10 years old and had since accumulated a jaw-dropping 34,000 flying hours, including 108 in the P-63. He was also a member of and operations officer for the Tora! Tora! Tora! demonstration team, a fighter check pilot for the CAF, and an approved formation flyer with an endorsement for aerobatics. The CAF’s chief aviation officer summed him up with just six words: “That guy knew how to fly.” ◊◊◊ Several air bosses on the air boss platform at an air show. Note: none of these individuals were involved in the accident air show; this photo is representative only. (Dave Hadfield) The B-17 crew and the P-63 pilot were to play different roles in that day’s performance, and not only because they were flying different aircraft with different performance characteristics. Because the B-17 was the lead aircraft in the group of five bombers, its crew was responsible for the real-time interpretation of the air boss’s commands, while the other four bombers simply followed the B-17 at a safe distance. On the other hand, Craig Hutain in the P-63 was not the leader of his formation and would be focused primarily on staying close to the two P-51s ahead of him. The air boss’s instructions were relevant to him only if Royce commanded a change in the shape of the formation. Having said that, we also need meet air boss Russell Royce, the last major character in this story, before we continue. Royce’s age isn’t listed in any of the available documents but based on other statements he was probably about 38 years old at the time of the accident and held a day job working at an auto body repair shop. His father was Ralph Royce, a veteran air boss widely known throughout the American air show industry, and he grew up hopping from one air show to the next, shadowing his dad. Ralph first allowed Russell to issue instructions to aircraft at the age of 14 — albeit under close supervision — and by 18 years old he was directing entire air shows by himself. That’s not to say he was a child prodigy; in fact, in his interviews Royce denies that he was born with any particular talent, but he was undeniably very experienced. At this point you might be asking how it was legal for a teenager to direct an air show, and the answer is that there was absolutely no law saying he couldn’t. At the time that the younger Royce began air bossing, there were no formal requirements for the position at all, other than those imposed by the air show organizers. Over the years, Royce did acquire a private pilot’s license, and he briefly worked in an air traffic control tower at a general aviation airport from 2008 to 2009, but his aviation experience paled in comparison to that of the pilots he was directing. Unlike them, he had never flown for an airline, for the military, or in an air show; he wasn’t rated to fly multi-engine planes; and he had no formation or aerobatics experience. He had, however, air bossed over 300 air shows. Later in his career, new regulations began requiring air bosses to receive a letter of authorization (LOA) from the FAA in order to perform the role. To receive an LOA, an air boss had to complete the air boss recognition program run by the International Council of Air Shows Inc., or ICAS, an industry standardization and advocacy body. The LOA would remain valid for three years but could be renewed by meeting a minimum experience requirement and submitting letters of recommendation. To renew the highest level of LOA — “recognized air boss, multiple venues” — he was required to have directed 8 air shows in the last three years, submitted four letters of recommendation from other air bosses or credentialed air show performers, and attended at least one ICAS Air Boss Academy workshop. There was no requirement for recurrent training or a performance evaluation. Russell Royce held the highest level of LOA, but at the time of the accident the requirement was still so new that he had yet to receive his first renewal. Although Russell Royce wasn’t a member of the CAF, his father was previously CAF chairman and many CAF members were familiar with him. For Wings Over Dallas, he was hired by the Air Show Chairman, the CAF’s Gena Linebarger, who said she always hired Ralph and/or Russell Royce if they were available. The working relationship between the two dated back about 10 years and the younger Royce had air bossed Wings Over Dallas numerous times during that period. In fact, the CAF had also hired Ralph Royce to air boss Wings Over Houston two weeks before the incident, and according to the Wings Over Houston performers, Russell was present there, too. ◊◊◊ Flight paths leading up to the accident, part 1. (Own work, map by Google, based on NTSB data) Back at the morning briefing on November 12th, Royce wrapped up the proceedings after about 45 minutes, then asked the audience if they had any questions. Although Len Root wasn’t personally present, witnesses recalled that some other members of the B-17 crew asked clarifying questions, but no one could remember what they were. Subsequently, Royce went out to the air boss station, where he would guide the air show from atop a set of air stairs on a taxiway near runway 17/35. He was joined on the air stairs by the air boss observer, while the FAA inspector and trainee inspector stood nearby. After the start of the air show at 10:50, six acts flew before it came time for the warbird parade shortly after 13:00. The five bombers took off one after another from runway 31, followed by the fighters, with the B-17 departing at 13:10 and the P-63 at 13:15. After takeoff, each aircraft was given initial maneuvering instructions while waiting for the remaining aircraft to join them in the formation or group; in the B-17’s case, the air boss instructed, “It will be a right turn. You’re looking for a thousand feet to enter from over the crowd.” The B-17 crew acknowledged and complied, making a sweeping 270-degree right turn after takeoff to come back over the field from the east, from behind the crowd. By this point the two B-25s were already in the air finishing up the previous act, so the air boss instructed them to fall in behind the B-17 and the SB2C helldiver, which was also airborne. Flight paths leading up to the accident, part 2. (Own work, map by Google, based on NTSB data) At 13:12, the fighters reported ready to go. Royce instructed the B-17 to come over the crowd then make another right 270 onto the 1,000 foot show line, then cleared three fighters for takeoff, followed by a right turn. At the same time, Royce shot instructions to the pyrotechnic team and to the pilot of a Beechcraft T-34B Mentor that was conducting a revenue ride flight during the performance. Moments later at 13:14, with the B-17 completing the right 270 to line up for its first pass down the runway, Royce instructed them to follow that pass with a right 90 degree turn, followed by a left 270 degree turn — a course reversal technique known as a “dog bone” or “duster turn.” This maneuver would be used several times during the performance in order to reverse the direction of each consecutive pass down the show lines. Turning to the fighters, Royce instructed them to “get formed up behind the crowd and I’ll bring you overhead in just a moment.” Following the lead P-51, the three fighters entered into a close formation, completing a right 180-degree turn followed by a left 270, rolling out heading west toward the back side of the crowd just as the B-17 had done a few minutes earlier. Meanwhile, the B-17 completed its first pass in front of the crowd and started the dog bone course reversal, to which Royce commented, “Perfect, and then you can come right down the runway and the fighters are going to pick you up.” He then instructed the bomber group, which was now fully in trail, to enter a left racetrack pattern — that is, flying in circles with left turns — after completing their current pass, in order to wait for the fighters, who he said would join them “overhead,” without specifying an altitude. By this point the bombers were distributed between 500 and 800 feet, while the fighters were staggered from about 900 to 1,300 feet, with the lead P-51 on top and the P-63 on the bottom. If that all sounded complicated, that’s because it was, but so far all communications were working as intended. You can use the flight path maps above and below to follow along. Flight paths leading up to the accident, part 3. (Own work, map by Google, based on NTSB data) After confirming that the lead P-51 had the bomber group in sight, Royce instructed them to join the bombers in the racetrack, then told the bombers that after their current circuit they would complete make a second “dog bone” turn with a “left 90 and right 270.” The fighters then caught up to the bombers, made a right 270 to follow them through the racetrack, and then both groups proceeded together toward the dog bone described by the air boss. At 13:19, as the bombers and fighters were completing the dog bone, preparing for a north-south pass in front of the crowd, Royce set them up for yet another course reversal at the other end, instructing, “B-17, after this pass, right 90 left 270.” “Raiders, right dog bone,” the B-17 replied. Turning to the fighter group, Royce then said, “Fighters, walk your way up to the B-17, I’m going to break y’all out after this — um, you’re going to end up breaking left.” In formation flying, a “break” occurs when each aircraft successively makes a sharp turn to exit the formation. In hindsight, what Royce wanted the fighters to do was to cut short the left 270 in the next dog bone turn by breaking hard to the left, cutting inside the bombers to get out in front. What he meant by “walk your way up” is unclear. Flight paths leading up to the accident, part 4. (Own work, map by Google, based on NTSB data) Without waiting for a response from the fighters, Royce continued, “So you’re going to follow the bombers to the right 90 out and then you’re going to roll back in left and be on the 500 foot line if y’all want to set up an echelon for a break so y’all can get in trail.” The first part of this crucial transmission was an attempt to clarify what I already explained — that after making the right 90 portion of the dog bone turn, the fighters would make a sharp left. He also added that after making the left turn, they would align with the 500 foot show line, closest to the crowd. However, the last part of the transmission might have been unclear in the moment. In formation flying, an echelon refers to a staggered formation where each aircraft is offset from the next to either the left or right, like migrating geese. An echelon formation is an ideal starting point for a break. After the break, Royce wanted the aircraft to get in trail, one behind the other. But that was a lot of information to take in at once. Clearly confused by the lengthy transmission, the lead P-51 pilot radioed, “Okay, uh — say again for the fighters. That was not clear.” But instead of repeating his instructions, Royce responded with something totally different: “Uh fighters, go uh — right, go to echelon right.” But this transmission only made matters worse, because the fighters hadn’t even straightened out for the pass in front of the crowd yet, let alone begun the next dog bone. The next turn was a hard right, but making a hard right turn while in a right echelon formation, with each aircraft offset right from the aircraft ahead of it, is risky because the lead aircraft has to turn across the face of the aircraft behind it. In hindsight, Royce was a step ahead; he wanted them to assume the right echelon in preparation for the left break after the right 90 had already been completed. But in the heat of the moment, this would have been difficult for the fighter pilots to understand, and indeed the fighters responded not by forming an echelon, but by getting in trail, one behind the other. Why it doesn’t make sense to cut the fighters inside to the 500 foot line. (Own work) Meanwhile, as the lead bomber proceeded southbound past the crowd, Royce noticed that the gap between the B-17 and the B-24 behind it was larger than he would like, so he said, “Okay B-24, if you could give me a couple of mi — uh, inches and close the gap I’d appreciate it. B-17, let’s keep a little flat for me.” It wasn’t entirely clear what Royce meant when he asked to keep the turn a little flat, but he probably wanted the B-17 to lead the bomber formation out wide, using shallow bank angles to extend the distance traveled. This would give the fighters more time to cut the corner. However, this is known only in hindsight, because Royce had not yet explained that he wanted the fighters to move in front of the bombers, and it’s doubtful that any of the pilots had so far guessed that that was his intention. In response to the transmission, one of the bombers made an unintelligible reply, and then Royce added, “Just a little bit. When you come back through you’re coming through on the thousand foot line.” This was another crucial moment in the sequence of events — perhaps even the most crucial. Royce had now set up the next pass with the bombers on the 1,000 foot show line and the fighters on the 500 foot show line. If the fighters completed the dog bone by flying outside the bombers, that would be fine, but if they made a tight turn inside the bombers, the two formations would cross paths. At this point, the bombers were distributed between 550 and 680 feet altitude, with the B-17 occupying the highest position, while the fighter formation was distributed from 960 feet to 1,100 feet, with the P-63 in the lowest position. Then, at 13:20 and 37 seconds, Royce hammered another nail into the coffin: “American fighters should be in a right turn,” he said. “You’re gonna follow the bombers out on a right 90 and then I’m going to roll you back in front of them.” Finally, Royce had articulated his intention for the fighters to undertake the bombers. As I just explained, compliance required the fighters to turn inside the bombers, which would cause the two groups’ flight paths to cross as they lined up with their assigned show lines. To make matters worse, the way the dog bone turns were typically flown was that every aircraft would climb during the right 90, level off on the outside of the turn, then descend in toward the show lines to simulate a diving run from the crowd’s perspective. This meant that the altitudes of all the aircraft were variable during the dog bone, and just because the lowest fighter was above the highest bomber at the time the instruction was given didn’t mean that that would necessarily remain the case. Furthermore, if the fighters were to overtake the bombers, then the B-17 was the primary obstacle that they needed to pass — but the older, less powerful B-17 was slower than the other bombers and thus made tighter turns in order to prevent the bombers behind from catching up. That would make it harder for the fighters to overtake the bombers on the inside, because the B-17 was already cutting inside on every turn anyway. From here on out, the NTSB’s own diagrams of the flight paths are an effective companion to the text. This diagram shows the positions of all aircraft at 13:21:08, the time that the lead P-51 said “We see the B-17.” (NTSB) At this point, the stage was nearly set for disaster. All that remained was for the deadly dance to play out. As the fighters and bombers proceeded into the dog bone maneuver, Royce turned his attention to another aircraft entirely: a Boeing-Stearman PT-17 operating a revenue ride flight. The two-seater, WWII-era training biplane, commonly referred to as the “Stearman,” was carrying one pilot and one paying passenger on approach to runway 31, the very same runway along which the warbirds were to make their next pass. But Royce judged that no conflict existed, so he transmitted, using the Stearman’s callsign, “Quebec, I need you to drop it down to the deck. Runway three one clear to land.” “Down to the deck, five eight Quebec,” the Stearman pilot acknowledged. Watching the B-17 lead the bombers through the dog bone, Royce said, “There ya go B-17, yup, gentle flat roll it around thousand foot line.” “Thousand foot line for raiders,” said the B-17. “Fighters, roll it back to the left,” Royce then added. “Lead, fighter lead, roll it back to the left and get y’all in trail.” “Okay, fighters in trail,” the P-51 acknowledged. At this point the fighters had completed the right 90 and had caught up with most of the bombers. The formation was positioned directly overhead the B-24, the second aircraft in the bomber group, but had not yet caught up with the B-17, which had already started the left 270 and was turning inbound. “Yeah, and gunfighter, look out your left side and find the B-17,” Royce instructed, using an alternate callsign for the lead P-51. Although the fighters were turning inside the bombers, the B-17 was in the left hemisphere from the fighters’ perspective because it was already inbound and the fighters were still flying outbound. “We see the 17,” said the P-51. As the fighters started a sharp left turn, Royce repeated, “Yeah there you go. Roll it back to the left. I want you to get in front of the bombers. I want you to come through on the outside edge of the runway.” This instruction might have caused yet more confusion because the outside edge of the runway, from the crowd’s perspective, was the west edge, which was in between the 500 and 1,000 foot show lines. The 500 foot show line, which was where he wanted the fighters to fly, should have been referred to as the inside edge because it was closer to the crowd. In any case, without asking for clarification, the lead P-51 replied, “Okay.” The positions of all aircraft at 13:21:45, 10 seconds before the crash and 3 seconds before the P-63 strayed to the right. (NTSB) The altitudes of the two groups now overlapped, with the fighters at 700, 340, and 520 feet respectively, while the first four bombers were at 430, 600, 450, and 670 feet respectively. Watching from the apron, Royce said, “Nice job fighters, you’re coming through first. That will work out. B-17 and all the bombers on the thousand foot line.” While the two P-51s had passed the B-17 at the time of the transmission and were indeed in front of the bombers, the P-63 was not. And at its controls, Craig Hutain was running out of time to overtake the B-17 before they were forced to cross paths to align with their respective show lines. Nor did anyone know for sure that Hutain had the B-17 in sight, because Royce never asked him — he only asked the formation lead. Just to double check, Royce asked, “B-17, you got the fighters in front of you off your left?” The B-17’s reply is described as “unintelligible” in the transcript but it appears to have been an affirmative statement. However, at the time of that transmission, only two of the three fighters were actually “in front of” the B-17; the P-63 was still technically behind. From the captain’s seat, Len Root would have had to look back over his left shoulder into his 7 or 8 o’clock position to see it. Watching the maneuver play out, Royce said, “Nice job fighters, come on through.” Moving on to the next maneuver after the current pass was complete, he then added, “Fighters will be a big pull up and to the right.” But back in the fighter formation, confusion was evident. The two lead P-51s didn’t proceed onto the 500 foot show line as instructed, but lined up with the 1,000 foot show line instead. At the same time, however, Craig Hutain’s P-63 swung farther out to the right, heading toward the 500 foot show line — and directly into the path of the B-17. Watch bystander video of the collision courtesy of Morgan Curry and NBC 6 South Florida. Caution: Viewer discretion advised. With his low-wing fighter banked in excess of 45 degrees to the left, and with the B-17 below him and to his right, there was no way for Hutain to see it coming. Moving left-to-right across the bomber’s path from behind and above, the P-63 plowed into the B-17 amidships, cleaving it in two. As hundreds of people watched in shock and alarm, the P-63 disappeared into a chaotic hail of metal while the bomber broke in half just aft of the wings, ripping away the roof and cabin walls all the way forward to the cockpit. The crew of the B-17 barely had time to brace before what remained of their aircraft pitched forward into the ground and exploded in flames. More burning pieces of both aircraft strafed the grass next to the threshold of runway 31, narrowly missing the Stearman, which had touched down just seconds earlier. Instantly, Russell Royce activated the pre-briefed emergency procedure by shouting, “Knock it off, knock it off! Roll the trucks, roll the trucks, roll the trucks, knock it off, roll the trucks!” Emergency crews on standby near the runway rushed to the scene and began spraying down the burning wreckage, but little remained of either aircraft. It was clear that everyone on both airplanes had perished. A photographer in the audience captured this series of high-definition photos of the aircraft before, during, and after the collision. (Gary Daniels via NTSB) In the air, the remaining aircraft proceeded to their pre-planned holding points, where they waited while deciding where to divert. Not all of the pilots had seen the crash, and many of those who did were left confused by what they had witnessed, but everyone knew that a tragedy had taken place. Many of the surviving pilots had been friends with the victims for years, even decades. But only after getting their own planes back on the ground could they allow the shock to set in. Len Root, Terry Barker, Craig Hutain, Curt Rowe, Kevin Michels, and Dan Fagan — in the CAF, almost everyone knew their names. Some were lifelong pilots, airline captains, war veterans; others just wanted to teach military aviation history, but all had dedicated a portion of their lives to the warbird community, and that their lives were taken by the very objects of their passion did not provide comfort, and in fact only heightened the sense of loss. And after the victims were mourned and buried, the warbird community would also quietly mourn the aircraft themselves, saying goodbye to the shattered remnants of the only extant P-63F, and ticking one more name off the short and shrinking list of airworthy B-17s. So what was it all for? What was the reason for such a waste of life and history? ◊◊◊ Another photographer captured this alternate angle of the collision. (Dylan Phelps via NTSB) In search of the answer to that question, the National Transportation Safety Board conducted numerous interviews, pored over countless pages of documentation, reviewed dozens of photos and videos, and conducted multiple studies. In addition to their final report, the NTSB assembled a docket of evidence 1,896 pages long, which you don’t have to read, because I’ve read it for you. On its most fundamental level, any mid-air collision is the result of a breakdown in the methods used to keep aircraft separated. Those methods can be visual — in other words, pilots keeping each other in sight — and they can be procedural, such as when an air traffic controller assigns two planes to different altitudes. In general, procedural separation is safer because it relies less on human perception; that is to say, the probability that a pilot will incorrectly follow directions, or that bad directions will be issued, is lower than the probability that two pilots will not see each other, especially in conditions of restricted visibility. However, if the pilots already have each other in sight, then maintaining visual separation is perfectly adequate. You’ll hear a lot more from me about the minutia of these distinctions in a couple years when I write about the Potomac River midair collision, but for today’s story, this is enough. One of the key elements required to understand the Wings Over Dallas disaster was the type of separation in use. In an approved maneuvers package, separation was ensured by following a rigorously practiced choreography, but in an air boss directed performance, that wasn’t the case. So, the NTSB did the logical thing and asked air boss Russell Royce how he normally kept airplanes apart and what tools were available for him to do so. Royce explained that he didn’t have radar or any other flight tracking software and that he judged the position of each airplane visually and based on experience. He added that as in all modes of aviation, “visual is the rule of the road,” meaning that he expected pilots to see and avoid each other if a conflict were to develop. However, he also listed three other types of separation; namely, vertical, lateral, and time-based. These are fundamental techniques for procedural separation known to every air traffic controller. But after reviewing the sequence of events, a glaring question arose: did Royce actually use them? An explosion and falling debris are visible just after the B-17 impacted the ground. (Nathaniel Ross via New York Times) After interviewing numerous people who were present at the briefing led by Royce on the morning of the accident, it was evident that there was no discussion of assigned altitudes or show lines prior to the start of the performance. In fact, the only altitude anyone could recall being mentioned was the height that would give the crowd the best photo opportunities during the photo pass at the end of the performance. Instead, interviewees stated that for an air boss directed performance, the exact choreography to be used would not normally be decided at the briefing, but would be determined by the air boss during the performance. Agreement on this matter was nearly universal among attendees, who all said that the briefing was standard, resembled other briefings, and did not appear to be missing any information that they expected to hear. Furthermore, while FAA guidance for air shows stated that a briefing had to take place, the NTSB could find no official document specifying what means of separation were to be used in an air boss directed performance, nor any document requiring that such means be discussed in the briefing. In fact, the FAA inspector in charge of monitoring the air show was at the briefing and reported nothing unusual about it. Once the performance was underway, Russell Royce could have assigned the bombers and fighters to different altitudes, but the transcript of the air boss frequency revealed that he never did so. While he did assign an altitude to the bombers during one particular pass, the assignment was intended to achieve the desired visual effect for the crowd, and not to provide separation from the fighters, which hadn’t taken off yet at the time. Throughout the remainder of the performance, the bombers generally flew at 200 to 500 feet and then climbed to altitudes generally lower than 1,000 feet; while at the same time, the fighters generally flew between 800 and 1,300 feet, except during the accident pass, where they descended to between 100 and 600 feet. However, these altitudes appeared to be chosen by each individual crew without coordinating with other crews or with the air boss. In fact, when the NTSB asked whether there were any limits on how high or low the performers could climb or descend, Royce replied that the maximum altitude was the ceiling of the air show airspace and the minimum altitude was the ground. However, Royce didn’t stop there. Despite listing altitude as a means of separation at his disposal, in his interviews he stated that as a matter of practice he rarely assigned altitudes because he wanted the pilots to look outside and maintain awareness of other aircraft rather than looking inside at their altimeters. In fact, he asserted that assigning altitudes is actively dangerous and only serves as a distraction. Nevertheless, he professed a belief that the fighters had an obligation to be “on top of the bombers” and that he expected the fighters to remain above the bombers as they maneuvered to the 500 foot show line during the accident dog bone. He did not appear to show any awareness of the fact that the lowest fighter was below the highest bomber at numerous points throughout the entire performance, nor did he explain why, if such an obligation existed, it was neither reviewed in the briefing nor specified in any regulation. Furthermore, in a separate interview he expressed an expectation that the bombers would be at 200 feet on the 1,000 foot show line, and that the fighters would be at about the same altitude on the 500 foot show line — which is exactly what both groups actually did — without explaining how this was supposed to be reconciled with his expectation that the fighters would cross over the top of the bombers. Nor did he explain how the pilots were supposed to know his intentions with regard to altitudes when he never mentioned altitudes on the radio. The inevitable conclusion to be drawn from this mess of answers is that altitude separation was not being used. But what about other types? Smoke rises from the crash site seconds after the accident. (NBC News) The next type of separation listed by Royce was lateral separation. In the context of an air boss directed performance, lateral separation could mean assigning the bombers to the 1,000 foot show line and the fighters to the 500 foot show line prior to the start of the performance as part of a racetrack pattern in which the faster fighters would fly a longer circuit around the outside of the bombers. However, Royce only assigned show lines on the fly, based on his assessment of where the two groups were. As a result, while he did send the bombers to the 1,000 foot show line and the fighters to the 500 foot show line on every pass, he did so without putting in place any means to ensure that the fighters flew outside the bombers’ circuit. If the fighters were cutting inside the circuit, then they would have to cross paths with the bombers to reach the 500 foot show line. The fact that Royce not only didn’t ensure that the fighters were orbiting outside the bombers, but actually ordered the fighters to cut inside the bombers’ orbit to reach their show line, indicates that he was not considering the use of different show lines as a means of lateral separation. Bizarrely, in his interview Royce denied that he told the two groups to cross paths, arguing that he told the fighters to go up and to the right while the bombers were flying down and to the left, and thus away from each other. But not only did he not actually tell either group to fly “up” or “down,” it should be self-evident that telling the fighters to fly right and the bombers to fly left when the fighters are left of the bombers requires them to cross paths! When confronted further on this matter, he placed all responsibility for avoiding cross-track maneuvers onto the lead pilots, simply stating, “I don’t put constraints on them.” But while it was true that the fighters were located outside the bombers’ circuit at the time he first instructed the fighters to “get in front of the bombers,” he then told the fighters to cut the corner, forcing them to cross inside the bombers’ flight path. It is unclear how the fighter pilots were supposed to comply with this directive without crossing paths with the bombers. So much for “not putting constraints on them!” An aerial view shows the B-17’s tail in the foreground, main B-17 wreckage in left background, and P-63 wreckage in right background. (NTSB) Elaborating on his views, Royce went on to explain that keeping the airplanes a certain distance apart wasn’t really his job at all, because the minimum safe separation between aircraft would depend on pilot experience, aircraft capabilities, and the moment-to-moment operating environment. However, this response was factually incorrect, because for aircraft not part of a formation, there was a legally imposed minimum separation distance of 500 feet. Any closer and a formation flying certificate would be required. In his interviews, Royce did not appear to understand that the bomber group was not a formation and that the bomber pilots were not necessarily required to have a formation rating, and thus that there was an obligation to keep the fighter formation at least 500 feet away from the bomber group at all times. In fact, throughout his interviews Royce repeatedly referred to the fighters and bombers together as a single formation, or as separate formations, even though neither was the case. At this point it should be clear that Royce didn’t view separation as his responsibility, even when a certain amount of separation was required by regulations. That’s why the NTSB pointedly asked who he thought was responsible for separation, if not the air boss? In one instance, Royce replied that the answer was “everyone.” Describing his own role, he explained that he assisted in separation “from the perspective of the crowd,” but it sounds like he meant keeping aircraft separate in the crowd’s field of view so that no aircraft was hidden behind another. Later, he explained that official documentation doesn’t address deconfliction, so any obligation to separate aircraft would depend on whether the pilots’ contract says separation has to be done a certain way, or based on common sense — “I’m in front of you, I shall go first,” as he put it. And in another instance, he said that avoiding each other was the pilots’ job — “I do a lot of assignment of responsibility.” So in essence, no systematic deconfliction practices existed; the air boss gave little thought to separation in his commands; and pilots were expected to utilize the principle of “see and avoid” when complying with his instructions. Another aerial view looking back toward the point of the collision. (NTSB) This conclusion necessarily raises two more questions: were the aircraft complying with Royce’s instructions when they collided, and was it possible for them to have seen each other? Regarding the first question, Royce seemed to think that the answer was no. During one interview, he said that crossing one group over another wasn’t an issue — “we do it all the time… it’s never a problem” — but that the P-63 wasn’t where it was supposed to be. However, in a separate interview, when asked whether he perceived the P-63 to be in the correct position before the crash, he complained that that was “not a valid question.” With no way of knowing what Craig Hutain was actually thinking as he executed the final maneuver, it isn’t possible to say whether he understood the instructions correctly or not. In general, the air boss only issued instructions to the lead aircraft in each group or formation, and the other aircraft were expected to follow them. But in interviews with the NTSB, the pilots of both P-51s stated that they believed the air boss had told them to go to the 1,000 foot show line — which is what they did — even though this was incorrect. Then, as the P-51s maneuvered to align with the 1,000 foot show line, the P-63 did not follow them, but rather swung wider, apparently toward the 500 foot show line, which is where Royce had actually told them to go. Most pilots in the performance stated that when flying in trail, their focus was on following the aircraft ahead of them, not on the air boss’s instructions, which would appear to cast doubt on the notion that Hutain broke out of the formation on purpose. But those who knew Hutain believed otherwise. When asked about Craig, his longtime friend and colleague Jim Lasche said, “ I can just imagine him, he’s told ‘be at the 500 foot line now,’ that’s where he’s going to go.” This NTSB diagram of the flight paths leading up to the point of impact shows how the P-63 swung slightly wide toward the 500 foot show line. It’s also worth noting that around the time Hutain moved toward the 500 foot show line, Royce said, “Nice job fighters, you’re coming through first, that will work out.” However, because at that moment both the B-17 and the fighters were headed directly toward him, Royce wasn’t actually in a position to judge whether the P-63 was ahead of or behind the B-17. If Hutain didn’t have the B-17 in sight, then this unfounded comment by Royce could have added to his belief that he was clear to cut over to the 500 foot show line. With no disagreement about the fact that the B-17 was right where it was supposed to be, and a reasonable likelihood that the P-63 pilot was also trying to get to his assigned show line, it appears in hindsight that both aircraft were probably complying with the air boss’s directives when they collided. Despite this, Royce insisted in his interviews that his directives were normal and that pilot compliance represented the biggest safety issue. Because there was no time for every aircraft to read back his commands in a dynamic, fast-changing environment, he had to trust that everyone understood his instructions correctly, and the only way to verify comprehension was compliance. Furthermore, in his view, because he had asked the fighters to “get in front of the bombers,” the fact that the P-63 moved to the 500 foot show line without getting in front of the B-17 amounted to non-compliance. But if we think about it in another way, that sounds an awful lot like a fancier way of saying, “If I tell you not to hit each other, and you hit each other anyway, that’s your fault because I told you not to.” It’s a complete abdication of responsibility and a violation of the pilots’ trust. In this image from the NTSB’s visibility study, it’s clear that the B-17 wasn’t easy for the P-63 pilot to see. In addition to being a copout, this philosophy is also problematic because, as I stated earlier, visual separation is less effective than procedural separation unless definitive visual contact has already been made. Also, only the lead fighter was asked to confirm the B-17 was in sight; no one asked the P-63. And when Royce asked whether the B-17 had the fighters in sight, he described the fighters as being “in front,” when the P-63 was actually behind, so when the B-17 replied in the affirmative, there was no way to know whether Len Root had looked back to find the third fighter or whether he only saw the P-51s. So could the two crews actually see each other? To find out, the NTSB conducted a visibility study using a flight simulator. What they found was that when the air boss asked the B-17 if the fighters were in sight, the P-63 was visible through the rear left side window, over the captain’s left shoulder; but after 5 seconds it became obscured by the captain’s window pillar until the collision. Under the circumstances, it was highly unlikely that either of the B-17 pilots would have seen the P-63 coming. Furthermore, it would not have been obvious that the P-63 was on a collision course until about 7 seconds before the crash, when it appeared to break formation with the other fighters. That was less than the time it would take for one of the two scanners, who were not current and qualified pilots, to spot the incoming aircraft, determine that it was a threat, tell the pilots, and for the pilots to take evasive action. Meanwhile, starting from the time that the air boss asked the fighters whether they could see the B-17, the latter was visible to the P-63 pilot for 16 continuous seconds near the lower right side of the canopy. The B-17 only became obscured by the P-63’s upturned right wing about four seconds before the crash. However, Craig Hutain’s focus at that point was probably ahead and to the left as he attempted to follow his lead plane and find his assigned show line. Furthermore, the B-17 was still wearing its original military colors — that is to say, olive drab camouflage. This paint scheme was specifically chosen in order to make it harder for enemy fighters to spot the bomber from above against a background of trees, buildings, and earth. Hutain wasn’t an enemy but the color scheme doesn’t discriminate; as far the paint knew, it was doing its job. The NTSB concluded that while it was theoretically possible for Hutain to have seen the B-17 if he focused on keeping it in sight, there were also good reasons why he might have glanced over where he thought it was, seen nothing, and decided that he was in the clear. As for the slower, less maneuverable B-17, their chances of avoiding the collision were almost nil. But these findings merely confirmed decades of NTSB research into the dangers of relying on visual separation, especially in complex maneuvering environments. So the discovery that “see and avoid” was inadequate for air show deconfliction was hardly a surprise to anyone who had been paying attention. Most of the wreckage of the P-63 came to rest in this area. The engine block and nose landing gear can be seen separated from the main portion of the aircraft. (NTSB) To summarize, then, the air boss wasn’t using procedural separation techniques to deconflict aircraft and expected pilots to avoid hitting each other no matter what directives he gave them. If this attitude strikes you as unreasonable, you wouldn’t be alone. In fact, several people who were interviewed by the NTSB expressed displeasure with Russell Royce­ — and even more with his dad. Among interviewees, there was widespread agreement that Ralph and Russell Royce possessed a shared approach to air bossing, whether they liked that approach or not. And in fact, quite a few people came out of the woodwork to mention previous close calls involving both air bosses, including just two weeks earlier at Wings Over Houston, under the direction of Royce senior. One of those people was the Vice Chairman of Wings Over Houston, a CAF pilot who flew as copilot of a bomber in an air boss directed performance during that air show. According to his recollection, Ralph Royce ordered Craig Hutain’s P-63 to cut in between his aircraft and the bomber ahead of him with no altitude separation. The pilot in command of his bomber confirmed that the incident had taken place and added that the P-63 came within 100 feet of his aircraft. According to both the Wings Over Houston Vice Chairman and CAF Chief Aviation Officer Jim Lasche, several pilots were so incensed at the incident that they met after the air show to commiserate, and one of them even confronted Ralph Royce and told him that he was “going to kill somebody if you do this again.” But the confrontation seemed to result in more ass-covering than soul-searching. In fact, according to the Wings Over Houston Vice Chairman, Royce senior tracked him down a month after the accident and asked him what he had told the NTSB about the Wings Over Houston incident, then insisted to him that the P-63 wasn’t at his altitude and that the maneuver was perfectly safe. After that conversation, the Vice Chairman said that he was bothered by the senior Royce’s dismissiveness of his concerns and his insistence that he disbelieve his own eyes. This wasn’t the only alarming incident described in the NTSB testimony either. According to the Wings Over Houston Vice Chairman, he had previously experienced an event in which Royce senior ordered him and another aircraft to turn into the same show line at the same time from opposite directions; realizing that this would put him on a collision course, he refused to comply. And in yet another incident, also at the air show in Houston two weeks before the crash, the pilot of a C-47 told the NTSB that the air boss sent a Messerschmitt Me-262 head-on into his aircraft on the show line, causing the replica German jet fighter to pass directly beneath him when he was only 200 feet above the ground. He was unaware that the Me-262 would do this until the air boss ordered it; the maneuver was not briefed beforehand; and it was not debriefed after. A closer aerial view of the B-17 main wreckage area. (NTSB) In order to get a second opinion, the NTSB interviewed several people with decades of air show experience who were not involved in Wings Over Dallas, including two air bosses who regularly directed other major air shows. Both of those air bosses stated that they normally determined the choreography for an air boss-directed performance in advance of the briefing and would assign altitudes and show lines to ensure separation between aircraft in different groups or formations. The typical minimum separation between aircraft was 200 feet and they did not normally allow aircraft with different cruising speeds to fly together. The air bosses said they were expected to carry out the pre-planned choreography without deviation. The Wings Over Houston Vice Chairman agreed that other air bosses didn’t use “freeform” directives as often as the Royces did, and said that he normally expected the fighters to remain above the bombers using pre-briefed assigned altitudes. Separately, the CAF Director of Operations said that it was a “mistake” to send the fighters past the bombers without 500 feet of separation, describing Russell Royce’s directive as “not the right way to do it.” “I would have thought it would have been intuitively obvious to the most casual observer that you don’t do that sort of thing, but apparently it’s not,” he said to the NTSB. And regarding the absence of a planned choreography, he opined that if a “new guy” were to attend the briefing, he would probably come away knowing only to “follow the guy in front of him and hope nothing happened.” Without a moment’s pause, he added, “And that to me is not right.” Finally, joining the chorus of voices, CAF Chief Aviation Officer Jim Lasche said that Royce was wrong to avoid assigning altitudes, because, as he put it, when a fighter is flying along at 250 knots and the pilot hears confusing directions, there needs to be something for them to fall back on, like an assigned altitude and show line — but there wasn’t. None of the interviewees made much mention of the fact that Royce also sometimes used unclear language and terminology, like “walk your way up” and “keep it a little flat.” Royce himself was unable to explain exactly what he meant when he used some of these phrases. However, it was agreed by both the NTSB and other experts whose commentary I reviewed that the use of this kind of language in a highly dynamic environment can and did cause confusion. When airline pilots and controllers talk on the radio, they’re expected to use certain established terminology so that the intent of a statement is as unambiguous as possible — so why shouldn’t we expect the same from an air boss? Most of the B-17 was destroyed in the impact and fire. (NBC News) And yet, despite criticism from these authoritative voices, the majority of interviewees spoke positively or neutrally of Russell Royce and his air bossing techniques. For instance, the pilot of the B-24 immediately behind the B-17 said that Royce didn’t do things particularly differently from other air bosses. The pilot of the second P-51, himself an accredited air boss and warbird safety researcher, said that Russell was the best in the business. The FAA inspector and the trainee inspector both said they observed nothing unusual or incorrect about Royce’s air bossing on the day of the accident. And the air boss observer shadowing Royce praised his ability and took his side on the question of separation, arguing that pilots always shoulder the primary responsibility for knowing where the other aircraft are, and noting that when he was in the Air Force flying jets, the obligation to maintain that awareness didn’t go away just because he was in a high G-maneuver with his belly up to someone else. But he also added that performers expect the air boss “to be ahead of the game, to understand what’s going to happen so as to avoid conflicts down the road,” and it’s not clear how to reconcile this statement with Royce’s actual performance. The most important voice of support among the interviewees was Wings Over Dallas air show Chairman Gena Linebarger, who was responsible for selecting the air boss. She described Royce as a “perfectionist,” said she liked the way he did things, and explained that she always hired the Royces if she could. Her relationship with them dated back 10 years and included numerous air shows. In fact, her confidence in Russell was so great that she told the NTSB, months after the accident, that the only reason she hadn’t hired him for her next air show was because their insurance wouldn’t let her. The clear takeaway here is that the air show community was deeply divided on the question of whether the Royce duo were cowboys. And with no consensus that their techniques were in any way abnormal, the air boss training and accreditation process needed to receive greater scrutiny. ◊◊◊ The tail of the B-17 with the Dallas Executive Airport control tower in the background, and airliners on approach to Dallas Fort Worth International Airport. (Dallas Morning News) As I mentioned earlier in this article, Russell Royce began air bossing with on the job training from his dad, before later receiving a letter of authorization (LOA) designating him as a “recognized air boss, multiple venues.” The typical training for an air boss lasted about four years and involved observing other air bosses, discussing theory, gaining radio experience under supervision, and other skill-building practices. These same type of activities are included in the International Council of Air Shows (ICAS) Air Boss Recognition Program (ABRP), which Royce underwent less than three years before the accident when the requirement was first introduced. The ABRP came with a training manual against which air boss applicants were to be judged by an ICAS-approved “air boss evaluator,” who would observe the applicant directing an air show and assesses their performance against the criteria on an evaluation form. Achieving full recognition also required a record of air show experience, letters of recommendation, completion of an “air show education data sheet,” and a score of 75% or better on a multiple choice test. Should the applicant meet these requirements, the FAA would issue an LOA on ICAS’s recommendation. The triannual renewal process for the LOA was described earlier in this article, but it bears repeating that the process did not include recurrent training or evaluations. The air boss community is small and insular. At the time the NTSB report was written, there were only about 60 to 70 ICAS-approved air bosses, and only 6 to 7 of them were air boss evaluators. Although close friends and family are forbidden from evaluating each other, it’s plausible that almost all air bosses in the United States know one another through industry connections and ICAS conferences. In such a community, outside monitoring by the FAA is probably necessary to ensure that corners don’t get cut. However, the NTSB found that FAA oversight of the process was minimal. The agency checked in on ICAS on a quarterly basis but was not closely involved in the accreditation of air bosses, nor were FAA inspectors at air shows given guidance on how to spot improper air boss behavior. In fact, their guidance was mostly aimed at ensuring that the provisions of the airspace waiver were met; that is to say, that the performers held the correct licenses, that the airplanes stayed within the designated airspace, that the briefing was held at the proper time, and so on. Since the inspectors were not experts on air show or air boss conduct, nor were they given any training or checklists related to those topics, it was unlikely that they would independently identify safety issues. The matter was considered so secondary that the FAA inspector didn’t even have a radio with which to listen to the air boss frequency. The trainee inspector did listen to the air boss frequency but was unable to identify anything out of the ordinary about the communications. NTSB investigators examine the tail section of the B-17. (NTSB) With the FAA exercising minimal oversight of the air boss training, accrediting, selecting, and monitoring process, the burden of ensuring the quality and trustworthiness of each air boss fell disproportionately onto the air show organizers themselves. In the case of Wings Over Dallas, that was the Commemorative Air Force. As this article has already established, Chairman Linebarger, a CAF employee, selected Russell Royce as the air boss because she liked his “way of doing things.” But as Linebarger herself freely admitted, she was too busy during air shows to actually monitor the air boss’s performance, and even if she could monitor it, she hadn’t received any training or guidance on what proper air bossing was supposed to look like, other than her own lived experience. In fact, she told the NTSB that the only way for her to find out about a problem with the air boss’s directives was for someone to bring it to her attention. However, as we’ve already noted, many senior CAF pilots didn’t see anything wrong with the way the Royces directed performances either. Furthermore, a small group who did have complaints about Ralph Royce after Wings Over Houston didn’t bring their concerns to the Air Show Chairman or indeed any other authority figure. It was therefore quite probable that Chairman Linebarger had little idea that the Royces’ freeform air bossing technique was controversial, or that other air bosses employed better safeguards. Even months after the accident, some interviewees didn’t appear to understand what the problem was, including Chairman Linebarger herself. In one regretful interview passage, the NTSB asked her what she would do differently if she were to organize a 2024 rendition of Wings Over Dallas, to which she replied that she wouldn’t do anything differently at all. And when further pressed on whether she had learned anything from the disaster, she said, “I learned how quickly things can change in a blink of an eye regardless of how well you’ve planned it; anything can happen. But you just have to move forward.” Is there any interpretation of that statement other than an admission that she learned nothing? I don’t want to be overly judgmental considering that some people freeze up when put on the spot, but it’s hard to imagine what would be a worse way to answer those questions. Another view of NTSB investigators examining the tail section. (Dallas Morning News) Based on extensive testimony gleaned from multiple interviews and other information sources, this attitude appears emblematic of a larger problem with the safety culture at the CAF. The Commemorative Air Force has some structural and historical barriers to achieving the kind of open, responsive, regimented safety culture that you might find at an airline. As an organization composed of volunteers, almost all of them retired pilots, the average CAF member is old and set in their ways. For instance, Chief Aviation Officer Jim Lasche described visiting a CAF wing that was having problems and discovering that everyone was over the age of 75 and the lead mechanic was 84. He said that many CAF members didn’t like it when he made changes, were uncomfortable around computers, and resented requirements being imposed upon them. Some of the units had turned into “old boys clubs,” ostensibly representing the warbird community in a major metropolitan area without adding any new members for a decade or more. And most importantly of all, everyone was there because they wanted to fly warbirds — and the implicit fear that that privilege could be taken away sometimes created a culture of silence. In a robust organization, that pressure can be alleviated by erecting clearly defined and well-understood safeguards for members who want to express safety concerns. But in a tight-knit organization full of experienced people, each of them bringing a lifetime of agendas and opinions to the table, abuse was known to occur. For instance, Lasche mentioned one incident in which a CAF member unsuccessfully propositioned a female wing mate, then falsely reported that her plane was unsafe in order to get back at her for turning him down. And while Lasche freely gave out his personal phone number as a 24/7 safety hotline, he stated that some people would call him just to rant or vent off steam, sometimes at odd hours of the night. But perhaps the most concerning story about the CAF’s internal culture was directly related to Wings Over Dallas. As you might recall, a two-seater Stearman on a revenue ride flight landed on runway 31 at approximately the same time as the collision. The NTSB was alarmed not only because the Stearman was potentially distracting for the air boss during a critical period of the show, but also because the plane was nearly struck by falling debris from the colliding aircraft, which could have injured or killed the occupants, one of whom was a paying passenger. In his interview, Jim Lasche stated that it had always been CAF policy to run revenue ride flights during breaks between warbird performances, when no other aircraft were airborne. But in recent years, air show timelines had been condensed to eliminate breaks that would cause some of the audience to leave. Because revenue ride flights are crucial to balancing the CAF’s books, this in turn resulted in pressure to conduct revenue rides during performances. On the other hand, Lasche felt that having paying passengers airborne in the same airspace as an ongoing performance was dangerous, so he came out against the move. But he “acquiesced” after receiving intense pushback from what he described as “higher up people,” including the air boss. As a compromise, he agreed to allow revenue ride flights during performances, but only as long as they didn’t take off or land while another act was flying. However, this agreement apparently wasn’t recorded in writing and wasn’t properly enforced, because the air boss authorized several revenue ride flights to take off or land during the accident performance, including the Stearman that almost became collateral damage. The decision to allow these takeoffs and landings was made without Lasche’s knowledge and he was upset to find out about them after the fact. Nevertheless, the Stearman issue made it clear that concerns such as entertainment value and revenue were sometimes taking priority over safety, and that there was no framework in place to ensure that safety won out. NTSB investigators examine the main B-17 wreckage. (NTSB) Years before the accident, Lasche had attempted to create such a framework by implementing a safety management system, or SMS. The idea behind an SMS is to provide a secure, anonymous avenue for members to report incidents and concerns, creating a stream of data that can then be analyzed to detect unsafe trends. The CAF wasn’t required to have an SMS, but implementing one was undoubtedly a good idea and it could have been very useful, in theory. Lasche explained that he was inspired to develop an SMS after he asked a group of CAF pilots where they thought the organization’s next accident would come from, only for everyone present to agree, without hesitation, that a particular named person was at risk. When that very pilot killed himself and a passenger in a crash a few weeks later, Lasche concluded that if a system had been in place to turn those concerns into action, lives might have been saved. But the SMS as actually implemented was something of a disappointment. Despite efforts to make sure all CAF members knew how it worked, the system was rarely utilized and received zero reports during the year leading up to the crash. Nevertheless, Lasche never really tried to figure out why. The NTSB rightly pointed out that receiving zero reports is usually indicative of a lack of understanding or trust in the SMS rather than a lack of safety issues to report. It was clear that some CAF members preferred to keep reporting issues using Lasche’s personal phone number, but the lack of anonymity in that approach and the absence of detailed record-keeping limited its usefulness. Nor was this problem confined to the CAF, because ICAS had its own similar safety reporting system that only received 10 reports in 15 years. In its final report, the NTSB wrote, “An organization’s culture can become unhealthy if motivational factors exert influence that turn it into something colloquially known as “don’t rock the boat” syndrome.” The investigators felt it was understandable that such a culture could develop at the CAF, where members paid for the privilege to participate and didn’t have the protections afforded to an employee. Further, the NTSB added, “ In such cases, the performance can become more about thrilling the crowd, sometimes to the detriment of aviation safety. While some air show event organizers have actively worked to apply administrative controls to preclude such performances, the circumstances of this accident suggest that, for some parts of the warbird community, it may persist.” Another view of the B-17 main wreckage. (Unknown) The NTSB notes that these circumstances created a culture where members did not speak up about deficiencies. But I would like to point out that the CAF interviews show many members didn’t recognize that safety deficiencies existed in the first place. I want to caveat this section by mentioning that I am not a CAF member, not a pilot, and not an expert on air shows. Weeks of research don’t replace or equal years of experience. But in all the testimony I read and all the guidance I reviewed, I didn’t find a credible defense of the way that the accident performance was carried out, not from Russell Royce nor from anyone else. Royce’s beliefs about the proper way to conduct an air boss directed performance were inconsistent with basic, well-established safety principles that transcend most modes of aviation. It is a fact that in a complex maneuvering environment involving multiple aircraft, especially dissimilar aircraft, visual separation is insufficient to preclude an accident, and it is a fact that the use of procedural separation in addition to visual separation is superior to visual separation alone. Of course pilots should be keeping each other in sight; that’s a basic part of flying any aircraft. But that doesn’t mean that the air boss shouldn’t take easily available measures to reinforce the imperfect substrate of see-and-avoid, and the existence of visual separation certainly doesn’t allow the air boss to wash their hands of all responsibility when two pilots inevitably and predictably lose sight of each other and collide — because saying, “that’s too bad, I guess they should have looked harder,” and then moving on to the next air show, is a surefire way to kill another 6 people next year. As seasoned aviators, often with decades of experience in the highly regimented and extremely safe world of passenger airlines, most CAF pilots and managers should theoretically have been in a position to recognize that Royce wasn’t using procedural separation, and that this represented a serious safety issue. But not only did nobody speak up about this before the accident, several prominent CAF members interviewed by the NTSB still didn’t see anything wrong with it even six or eight months later. As far as they were concerned, Russell Royce’s freeform air bossing style was just the way things were done, and the possibility that “the way things were done” was unsafe and should be improved may not have occurred to them. Looking in from the outside, as someone who hasn’t spent my life in the air show environment, and as someone who knows what the consequences of these unsafe practices turned out to be, it’s easy for me to say they should have reacted differently. But I won’t say that, because it’s a well-documented fact that human beings, existing day in and day out within a system where safety measures are not being used, will quickly become accustomed to the unsafe environment even if they know, in principle, what a safe environment ought to look like. And that’s what sociologist Diane Vaughan famously called “normalization of deviance.” Aerial view of the tail section of the B-17. (NBC News) The normalization of deviance is the process by which actors within a complex system featuring complex safety requirements unconsciously tolerate increasingly unsafe behaviors and practices as long as those practices are not met with adverse consequences. It’s really not a very difficult concept to understand, because all of us have probably experienced a basic version of it at some point in our lives. For example, I spent years writing articles using a laptop with broken keys, even though this resulted in a greatly elevated number of typographical errors, simply because I had become accustomed to my little workarounds for using the keyboard and catching mistakes. But if someone else had tried to sit down and write using my laptop, they would have been frustrated beyond belief. So while it would have been better to get my computer some professional help, I never did, even though I knew the problem existed and I could afford the repairs. The point is that when you’re inside the unsafe system, it can be hard to see the forest for the trees, and sometimes even if you do, inertia can prevent change. This was probably especially true at the CAF, an organization with a tight-knit membership, disincentives to rocking the boat, and a unique mission that requires flying 85-year-old aircraft with all the safety issues inherent to that. And when you’re up there in a formation, flying a B-17 Flying Fortress or a P-51 Mustang, living the dream in front of thousands of awed spectators, who really wants to be the one to say, “Hey, maybe we shouldn’t be doing this?” Hell, who even wants to think that? ◊◊◊ In October 2023, a memorial to the lives lost in the accident was erected in Conroe, Texas, home of the CAF unit that operated the B-17. (The Courier of Montgomery County) To its credit, the CAF made several changes to its operations after the accident without waiting for NTSB recommendations. In 2023, CAF management drafted new air show guidance banning CAF aircraft from participating in air boss directed performances that included aircraft of differing performance; multiple parades of different aircraft categories without at least 500 feet of altitude separation; or maneuvers other than racetrack patterns. Dog bone turns and crossing paths would only be allowed as part of an approved maneuvers package. Air bosses would be required to meet these stipulations before the CAF would agree to participate. At the same time, the FAA issued a Safety Alert For Operators (SAFO) recommending that air show organizers provide all pilots with a detailed written plan, including well-defined separation strategies, in advance of any performance. The SAFO also stated that maneuvers other than racetrack patterns were not recommended for air boss directed performances, and linked to a new ICAS document that outlined a strategy for using altitude blocks and lateral buffers to separate aircraft. A form that air bosses could use to assign aircraft to these blocks was also included. In its final report, the NTSB made several recommendations that went even farther. These included the following: · The FAA and ICAS should establish air show standard operating practices addressing procedural separation in air boss directed performances, risk assessments for air show operations, and a post-show debriefing with reporting of results to the FAA and ICAS. · The FAA should require recurrent evaluations upon renewing air boss letters of authorization. · ICAS should develop a set of standard terminology for air bosses to use when describing air show maneuvers. · FAA inspectors should observe air boss performance during air shows and provide feedback during the debriefing, and the FAA should provide its inspectors with evaluation guidance. · The CAF should use existing FAA guidance to develop a risk assessment and mitigation process tailored to its unique operations. So far, the effect of these reforms remains to be seen. Some changes are already visible; for example, Chief Aviation Officer Jim Lasche said he refused to allow CAF airplanes and pilots to participate in a February 2023 air show where Russell Royce was the air boss. Later, the family of Len Root sued Royce for negligence, and the CAF for hiring him. The outcome of that suit is pending as of this writing. A makeshift memorial honors the victims near Dallas Executive Airport. (Fort Worth Star-Telegram) However, one of the points I want to end on is that while there were characters in this story who came across very unfavorably, a true safety reckoning doesn’t stop at blaming those people — in fact, searching for blame isn’t really part of the process at all. Everyone in this story was a product of the environment that they operated in, while at the same time, each contributed in some unconscious way to the perpetuation of that environment. It requires vision and drive to change the culture of an organization — or it can require tragedy, lawsuits, and new FAA regulations. But both methods require that the root causes of unsafe practices be identified, and it’s clear that the practices that led to the disaster at Wings Over Dallas didn’t start with Russell Royce, or with his dad, or with the air show chairman. They probably didn’t even start with the CAF. Most likely, those practices arose from the informal origins of warbird demonstrations — a few guys getting together on weekends to pluck ex-military aircraft from boneyards to fly for fun — and those origins created attitudes that shaped the institutional character thereafter. And because of the Wings Over Dallas tragedy and other recent warbird incidents, it’s now up to the current generation of warbird enthusiasts to ensure that it remains possible to uphold the CAF’s mission — to preserve the aviation heritage of the Second World War for the education and enjoyment not only of current, but also future generations. In that mission, I wish them luck. I am not and probably never will be a warbird enthusiast, but it nevertheless fills me with a certain wonder to look up and see a B-17, its mighty radial engines turning fuel into noise, like a beast out of another age, brashly carving its way through our modern skies. I hope that some lucky few will one day get to ride on a B-17 as it celebrates its hundredth birthday. The biggest obstacle to that milestone isn’t the age of the planes, but the way that we treat them, and if nothing is learned from this latest tragedy, then not only will we lose this part of our history — we will deserve it. _______________________________________________________________ Thanks for your patience in waiting for this article! After publishing my piece on EgyptAir 804 in December, I moved half way across the country in a long, messy relocation process fraught with other struggles along the way. But here I am, and here it is. Thank you! _______________________________________________________________ Don’t forget to listen to Controlled Pod Into Terrain, my podcast (with slides!), where I discuss aerospace disasters with my cohosts Ariadne and J! Check out our channel here , and listen to our latest episode about a titanic battle between a BAC 1–11 and some wind. Alternatively, download audio-only versions via RSS.com , or look us up on Spotify! _______________________________________________________________ Join the discussion of this article on Reddit Support me on Patreon (Note: I do not earn money from views on Medium!) Follow me on Bluesky Visit r/admiralcloudberg to read and discuss over 260 similar articles Bibliography"
"Show HN: I made a website where you can create your own ""Life in Weeks"" timeline",https://lifeweeks.app/,Hacker News,2025-02-26T08:58:24,Hacker News,https://images.unsplash.com/photo-1579403124614-197f69d8187b?q=80&w=3394&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Create a map of your life where each week is a little box. “Sometimes life seems really short, and other times it seems impossibly long. But this chart helps to emphasize that it’s most certainly finite. Those are your weeks and they’re all you’ve got.” In 2014, blogger Tim Urban, published a post called ""Your Life in Weeks,""
    where he showed a visualization of life as a series of little squares,
    with each one representing a week.
    It's a powerful, concise way to visualize an entire human life. This site helps you create one of those visualizations for yourself.
    You can use it to track key periods and events in your life,
    see the progress you've made over time,
    and easily share your life story with friends and loved ones. I built this app after seeingGina Tripani's interpretationof the idea,
    and realizing I wanted one for myself.
    And then I thought, well, if I want one maybe other people do too. Check outmy own life in weeksto see what you can build. Your life story, one week at a time Document your life's journey like you never have before. See your entire life laid out in weeks. Past, present, and future — all in one colorful view. Gain perspective on how you're spending your most precious resource:time. Color-code the seasons of your life — childhood, college years, jobs, and places you've lived. Define and document the meaningful periods that have shaped who you are today. Highlight the moments and memories that matter — achievements, transitions, encounters, or trips. Make each important memory in your life visible on your timeline. Add depth to your timeline with longer notes on events and periods. Hover over any square in your timeline to reveal the full story. Auto-add birthdays, world events, and curated collections to your timeline. See how your personal journey aligns with historical moments and annual celebrations. Export your timeline to YAML for easy editing and backup. Import it back when you're done. Your data remains yours, in a format you can understand and control. Share your timeline through a unique link or keep it completely private. It's your life story — you decide whether to share it with the world or keep it for yourself. Life keeps happening. Return anytime to update your timeline with new events and chapters or add past memories over time. Your visualization is a living, growing representation of your life`. Life in Weeks — Copyright2024","Create a map of your life where each week is a little box. “Sometimes life seems really short, and other times it seems impossibly long. But this chart helps to emphasize that it’s most certainly finite. Those are your weeks and they’re all you’ve got.” In 2014, blogger Tim Urban, published a post called ""Your Life in Weeks,""
    where he showed a visualization of life as a series of little squares,
    with each one representing a week.
    It's a powerful, concise way to visualize an entire human life. This site helps you create one of those visualizations for yourself.
    You can use it to track key periods and events in your life,
    see the progress you've made over time,
    and easily share your life story with friends and loved ones. I built this app after seeingGina Tripani's interpretationof the idea,
    and realizing I wanted one for myself.
    And then I thought, well, if I want one maybe other people do too. Check outmy own life in weeksto see what you can build. Your life story, one week at a time Document your life's journey like you never have before. See your entire life laid out in weeks. Past, present, and future — all in one colorful view. Gain perspective on how you're spending your most precious resource:time. Color-code the seasons of your life — childhood, college years, jobs, and places you've lived. Define and document the meaningful periods that have shaped who you are today. Highlight the moments and memories that matter — achievements, transitions, encounters, or trips. Make each important memory in your life visible on your timeline. Add depth to your timeline with longer notes on events and periods. Hover over any square in your timeline to reveal the full story. Auto-add birthdays, world events, and curated collections to your timeline. See how your personal journey aligns with historical moments and annual celebrations. Export your timeline to YAML for easy editing and backup. Import it back when you're done. Your data remains yours, in a format you can understand and control. Share your timeline through a unique link or keep it completely private. It's your life story — you decide whether to share it with the world or keep it for yourself. Life keeps happening. Return anytime to update your timeline with new events and chapters or add past memories over time. Your visualization is a living, growing representation of your life`. Life in Weeks — Copyright2024","Show HN: I made a website where you can create your own ""Life in Weeks"" timeline","

Key Points:
",Software Development,"Create a map of your life where each week is a little box. “Sometimes life seems really short, and other times it seems impossibly long. But this chart helps to emphasize that it’s most certainly finite. Those are your weeks and they’re all you’ve got.” In 2014, blogger Tim Urban, published a post called ""Your Life in Weeks,""
    where he showed a visualization of life as a series of little squares,
    with each one representing a week.
    It's a powerful, concise way to visualize an entire human life. This site helps you create one of those visualizations for yourself.
    You can use it to track key periods and events in your life,
    see the progress you've made over time,
    and easily share your life story with friends and loved ones. I built this app after seeingGina Tripani's interpretationof the idea,
    and realizing I wanted one for myself.
    And then I thought, well, if I want one maybe other people do too. Check outmy own life in weeksto see what you can build. Your life story, one week at a time Document your life's journey like you never have before. See your entire life laid out in weeks. Past, present, and future — all in one colorful view. Gain perspective on how you're spending your most precious resource:time. Color-code the seasons of your life — childhood, college years, jobs, and places you've lived. Define and document the meaningful periods that have shaped who you are today. Highlight the moments and memories that matter — achievements, transitions, encounters, or trips. Make each important memory in your life visible on your timeline. Add depth to your timeline with longer notes on events and periods. Hover over any square in your timeline to reveal the full story. Auto-add birthdays, world events, and curated collections to your timeline. See how your personal journey aligns with historical moments and annual celebrations. Export your timeline to YAML for easy editing and backup. Import it back when you're done. Your data remains yours, in a format you can understand and control. Share your timeline through a unique link or keep it completely private. It's your life story — you decide whether to share it with the world or keep it for yourself. Life keeps happening. Return anytime to update your timeline with new events and chapters or add past memories over time. Your visualization is a living, growing representation of your life`. Life in Weeks — Copyright2024"
Nuclear Reactor Lasers: From fission to photon,http://toughsf.blogspot.com/2019/04/nuclear-reactor-lasers-from-fission-to.html,Hacker News,2025-02-26T09:27:15,Hacker News,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Nuclear reactor lasers are devices that can generate lasers from nuclear energy with little to no intermediate conversion steps. We work out just how effective they can be, and how they stack up against conventional electrically-powered lasers. You might want to re-think your space warfare and power beaming after this. Nuclear energy and space have been intertwined since the dawn
of the space age. Fission power is reliable, enduring, compact and powerful.
These attributes make it ideal for spacecraft that must make every kilogram of
mass as useful and as functional as possible, as any excess mass would cost
several times its weight in extra propellant. They aim for equipment for the
highest specific power (or power density PD), meaning that it produces the most
watts per kilogram. Lasers use a lasing medium that is rapidly energized or
‘pumped’ by a power source. Modern lasers use electric discharges from
capacitors to pump gases, or a current running through diodes. The electrical
power source means that they need a generator and low temperature radiators in
addition to a nuclear reactor… these are significant mass penalties to a
spaceship. Fission reactions produce X-rays, neutrons and high energy
ions. The idea to use them to pump a lasing medium has existed ever since the
first coherent wavelengths were released from a ruby crystal in 1960. Much
research has been done in the 80s and 90s into nuclear-pumped lasers,
especially as part of the Strategic Defense Initiative. If laser power can be
generated directly from a reactor, there could be significant gains in power
density. The research findings on nuclear reactor lasers were
promising in many cases but did not succeed in convincing the US and Russian
governments to continue their development. Why were they unsuccessful and what
alternative designs could realize their promise of high power density lasers? Distinction between NBPLs and NRLs Most mentions of nuclear pumped lasers relate to nuclear bomb-pumped lasers . They are exemplified by project Excalibur: the idea was to use
the output of a nuclear device to blast metal tubes with X-rays and have them
produce coherent beams of their own. We will not be focusing on it. The concept has many problems that prevent it from being a
useful replacement for conventional lasers. You first need to expend a nuclear
warhead, which is a terribly wasteful use of fissile material. Only a tiny
fraction of the warhead’s X-rays, which are emitted in all directions, are
intercepted by the metal tube. From those, a tiny fraction of its energy is
converted into coherent X-rays. If you multiply both fractions, you find an
exceedingly low conversion ratio . Further
research has revealed this to be on the order
of <0.00001% . It also works for just a microsecond, each shot
destroys its surroundings and its effective range is limited by relatively poor
divergence of the beam. These downsides are acceptable for a system meant to
take down a sudden and massive wave of ICBMs at ranges of 100 to 1000
kilometers, but not much else. Instead, we will be looking at nuclear reactor pumped
lasers. These are lasers that draw power from the continuous output of a
controlled fission reaction. Performance We talk about efficiency and power density to compare the
lasers mentioned in this post. How are we working them out? For efficiency, we multiply the reactor’s output by the
individual efficiencies of the laser conversion steps, and assume all
inefficiencies become waste heat. The waste heat is handled by flat
double-sided radiator panels operating at the lowest temperature of all the
components, which is usually the laser itself. This will give a slightly poorer performance than what could
be obtained from a real world engineered concept. The choice of radiator is
influenced by the need for easy comparison instead of maximizing performance in
individual designs. We will note the individual efficiencies as Er for the
reactor, El for the laser and Ex for other components. The overall efficiency
will be OE. OE = Er * Ex * El * Eh In most cases, Er and Eh can be approximated as equal to 1.
As we are considering lasers for use in space with output on the order of
several megawatts and beyond, it is more accurate to use the slope efficiency
of a design rather than the reported efficiency. Laboratory tests on the
milliwatt scale are dominated by the threshold pumping power, which cuts into
output and reduces the efficiency. As the power is scaled up, the threshold
power becomes a smaller and smaller fraction of the total power. Calculating power density (PD) in Watts per kg for several
components working with each other’s outputs is a bit more complicated. As
above, we’ll note them PDr, PDl, PDh, PDx and so on. The equation is: PD = (PDr * OE) / (1 + PDr (Ex/PDx + Ex*El/PDl + (1 -
Ex*El)/PDh)) Generally, the reactor is a negligible contributor to the
total mass of equipment, as it is in the several hundred kW/kg, so we can
simplify the equation to: PD = OE / (Ex/PDx + Ex*El/PDl + (1 - Ex*El)/PDh) Inputting PDx, PDl and PDh values in kW/kg creates a PD value
also in kW/kg. Direct Pumping The most straightforward way of creating a nuclear reactor
laser is to have fission products interact directly with a lasing medium. Only
gaseous lasing mediums, such as xenon or neon, could survive the conditions
inside a nuclear reactor indefinitely, but this has not stopped attempts at
pumping a solid lasing medium. Three methods of energizing or pumping a laser medium have
been successful. Wall pumping Wall pumping uses a channel through which a gaseous lasing
medium flows while surrounded by nuclear fuel. The fuel is bombarded by
neutrons from a nearby reactor. The walls then release fission fragments that
collide with atoms in the lasing medium and transfer their energy to be
released as photons. The fragments are large and slow so they don’t travel far
into a gas and tend to concentrate their energy near the walls. If the channels
are too wide, the center of the channel is untouched and the lasing medium is
unevenly pumped. This can create a laser of very poor quality. To counter this, the channels are made as narrow as possible,
giving the fragments less distance to travel. However, this multiplies the
numbers of channels needed to produce a certain amount of power, and with it
the mass penalty from having many walls filled with dense fissile fuel. The walls absorb half of the fission fragments they create immediately. They release the surviving
fragments from both faces of fissile fuel wall. So, a large fraction of the
fission fragment power is wasted. They are also limited by the melting
temperatures of the fuel. If too many fission fragments are absorbed, the heat
would the walls to fail, so active cooling is needed for high power output. The FALCON experiments achieved an efficiency of 2.5% when
using xenon to produce a 1733 nm wavelength beam. Gas
laser experiments at relatively low temperatures reported
single-wavelength efficiencies as high as 3.6%. The best reported performance
was 5.6% efficiency from an Argon-Xenon mix producing 1733 nm laser light, from
Sandia National Laboratory. Producing shorter wavelengths using other lasing
mediums, such as metal vapours, resulted in much worse performance (<0.01%
efficiency). Higher efficiencies could be gained from a carbon monoxide or
carbon dioxide lasing medium, with up to 70%
possible , but their wavelengths are 5 and 10 micrometers
respectively (which makes for a very short ranged laser) and a real efficiency
of only 0.5%
has been demonstrated . One estimate
presented in this
paper is a wall-pumped mix of Helium and Xenon that
converts 400 MW of nuclear power into 1 MW of laser power with a 1733 nm
wavelength. It is expected to mass 100 tons. That is an efficiency of 0.25% and
a power density of just 10 W/kg. It illustrates the fact that designs meant to
sit on the ground are not useful
references. A chart from this NASA report reads as a direct pumped nuclear
reactor laser with 10% overall efficiency having a power density of about 500
W/kg, brought down to 200 W/kg when including radiators, shielding and other
components. Volumetric
pumping Volumetric
pumping has Helium-3 mixed in with a gaseous lasing medium to absorb neutrons
from a reactor. Neutrons are quite penetrating and can traverse large volumes
of gas, while Helium 3 is very good at absorbing neutrons. When Helium-3
absorbs neutrons, it creates charged particles that in turn energize lasing
atoms when they enter into contact with each other. Therefore, neutrons can
fully energize the entire volume of gas. The main advantages of this type of
laser pumping is the much reduced temperature restrictions and the lighter
structures needed to handle the gas when compared to multiple narrow channels
filled with dense fuel. However,
Helium-3 converts neutrons into charged particles with very low efficiency,
with volumetric pumping experiments reporting 0.1 to 1% efficiency overall. This
is because the charged particles being created contain only a small portion of
the energy the Helium-3 initially receives. Semiconductor
pumping The final
successful pumping method is direct pumping of a semiconductor laser with
fission fragments. The efficiency is respectable at 20%, and the compact laser
allows for significant mass savings, but the lasing medium is quickly destroyed
by the intense radiation. It consists of a thin layer of highly enriched
uranium sitting on a silicon or gallium semiconductor, with diamond serving as
both moderator and heatsink. There are very
few details available on this type of pumping. A
space-optimized semiconductor design from this
paper that suggests that an overall power density of 5
kW/kg is possible. It notes later on that even 18 kW/kg is achievable. It is
unknown how the radiation degradation issue could be solved and whether this
includes waste heat management equipment. Without an operating temperature and
a detailed breakdown of the component masses assumed, we cannot work it out on
our own. Other direct
pumped designs Wall or
volumetric pumping designs were conceived when nuclear technology was still new
and fission fuel had to stay in dense and solid masses to achieve criticality.
More modern advances allow for more effective forms for the fuel to take. The lasing medium
could be made to interact directly with a self-sustaining reactor core. This
involves mixing the lasing medium with uranium
fluoride gas , uranium aerosols, uranium vapour at very high
temperatures or uranium micro-particles at low temperatures. The trouble with
uranium fluoride gas and aerosols or micro-particles is the tendency for them
to re-absorb
the energy (quenching) of excited lasing atoms. This has
prevented any lasing action from being realized in all experiments so far. As this
diagram shows, uranium fluoride gas absorbs most
wavelengths very well, further reducing laser output. If there is a
lasing medium that is not quenched by uranium fluoride, then there is potential
for extraordinary performance. An early NASA
report on an uranium fluoride reactor lasers for space
gives a best figure of 73.3 W/kg from what is understood to be a 100 MW reactor
converting 5% of its output into 340 nanometer wavelength laser light. With the
radiators in the report, this falls to 56.8 W/kg. If we bump up
the operating temperature to 1000K, reduce the moderator to the 20cm minimum,
replace the pressure vessel with ceramics and use more modern carbon fiber
radiators, we can expect the power density of that design to increase to 136
W/kg. Uranium vapours
are another option. They require temperatures of 4000K and upwards but if the
problem of handling those temperatures is solved (perhaps by using actively
cooled graphite containers), then 80%
of the nuclear output can be used to excite the lasing medium, for an
overall efficiency that is increased four-fold over wall pumping designs. More speculative is encasing uranium inside a C60
Buckminsterfullerene sphere. Fission fragments could exit the sphere while also
preventing the quenching of the lasing material. This would allow for excellent
transmission of nuclear power into the lasing medium, without extreme
temperature requirements. Nuclear-electric
comparison With these
numbers in mind, it does not look like direct pumping is the revolutionary
upgrade over electric lasers that was predicted in the 60s. Turbines,
generators, radiators and laser diodes have improved by a lot, and they deliver
a large fraction of a reactor’s output in laser light. We expect a
space-optimized nuclear-electric powerplant with a diode laser to have rather
good performance when using cutting edge technology available today. With a 100 kW/kg
reactor core, a 50% efficient turbine at 10 kW/kg, an 80% efficient electrical
generator at 5 kW/kg, powering 60% efficient diodes at 7 kW/kg and using 1.34 kW/kg radiators to
get rid of waste heat ( 323K
temperature ), we get an overall efficiency of 24% and a
power density of 323 W/kg. A more advanced system using a very powerful 1 MW/kg reactor core, a 60% efficient MHD generator at 100 kW/kg with 1000K 56.7 kW/kg radiators, powering a 50% efficient fiber laser cooled by 450K 2.3 kW/kg radiators, would get an overall efficiency of 30% and a power density of 2.5 kW/kg. Can we beat these
figures with reactor lasers? Indirect pumping The direct pumping method uses the small fraction of a
reactor’s output that is released in the form of neutrons, or problematic
fission fragments. Would it not be better to use the entire output of the
nuclear reaction? Indirect pumping allows us to use 100% of the output in the
form of heat. This heat can then be converted into laser light in various ways. Research and data for some of the following types of lasers
comes from solar-heated
designs that attempt to use concentrated sunlight to
heat up an intermediate blackbody that in turn radiates onto a lasing medium.
For our purposes, we are replacing the heat of the Sun with a reactor power
source. It is sometimes called a ‘blackbody laser’ in that case. Blackbody radiation pump At high temperatures, a blackbody emitter radiates strongly
in certain wavelengths that lasing materials can be pumped with. A reactor can
easily heat up a black carbon surface to temperatures of 2000 to 3000K – this
is what nuclear rockets are expected to operate at anyhow. Some of the spectrum of a blackbody at those temperatures
lies within the wavelengths that are absorbed well by certain crystal and
gaseous lasing mediums. Neodymium-doped Ytrrium-Aluminium-Garnet (Nd:YAG)
specifically is a crystal lasing medium that has been thoroughly investigated
as a candidate for a blackbody-pumped laser. It produces 1060 nm beams. Efficiency figures vary. A simple single-pass configuration results in very poor
efficiency (0.1 to 2%). This is because the lasing medium only absorbs a small
portion of the entire blackbody spectrum. In simpler terms, if we shine
everything from 100 nm to 10,000 nm onto a lasing medium, it will convert 0.1
to 2% of that light into a laser beam and turn the rest into waste heat. With
this performance, blackbody pumped lasers are no better than direct pumped
reactor laser designs from the previous section. Instead, researchers have come up with a way to recover the
99 to 99.9% of the blackbody spectrum that the lasing medium does not use. This
is the recycled-heat blackbody pumped laser. An Nd:YAG crystal sits inside a ‘hot tube’. Blackbody
radiation coming from the tube walls passes through the crystal. The crystal is
thin and nearly transparent to all wavelengths. The illustration above uses Ti:Sapphire but the concept is the same for any laser crystal. Only about 2% of blackbody spectrum is absorbed with every
pass through the crystal. The remaining 97 to 98% pass through to return to the
hot tube’s walls. They are absorbed by a black carbon surface and recycled into
heat. Over many radiation, absorption and recycling cycles, the fraction of
total energy that becomes laser light increases for an excellent overall
efficiency. 35%
efficiency with a Nd:YAG laser was achieved. The only downside is that the Nd:YAG crystal needs intense radiation
within it to start producing a beam. The previous document suggests that 150
MW/m^3 is needed. Another
source indicates 800 MW/m^3. We also know that
efficiency increases with intensity. If we aim for 1 GW/m^3, which corresponds
to 268 Watts shining on each square centimetre of a 1 cm diameter lasing rod,
we would need a 1:1 ratio of emitting to receiving area if the emitter has a
temperature of at least 2622K. From a power conversion perspective, a 98% transparent
crystal that converts 35% of spectrum it absorbs means it is only converting
0.7% of every Watt of blackbody radiation that shines through it. So, a crystal
rod that receives 268 Watts on each square centimetre will release 1.87 W of
laser light. We can use the 1:1 ratio of emitter and receiver area to
reduce weight and increase power density. Ideally, we can stack emitter and
receiver as flat surfaces separated by just enough space to prevent heat
transfer through conduction. Reactor coolant channels, carbon emitting surface (1cm),
filler gas, Nd:YAG crystal (1cm) and helium channels can be placed back to
back. The volume could end up looking like a rectangular cuboid, interspaced by
mirror cavities. 20 kg/m^2 carbon layers and 45.5 kg/m^2 crystal layers that
release 1.87 W per square centimetre, with a 15% weight surplus for other
structures and coolant pipes, puts this component’s power density at about 250
W/kg. The laser crystal is cooled from 417K according to the set-up in
this paper . Getting rid of megawatts at such a low
temperature is troublesome. Huge radiator surface areas will be required. As we are using flat panel radiators throughout this post, we
have only two variables: material density, material thickness and operating
temperature. The latter is set by the referenced document. We will choose a 1mm thick radiator made of low
density polyethylene . We obtain 0.46 kg/m^2 are plausible. When
radiating at 417K, they could achieve 3.73 kW/kg. It is likely that they will operate at a slightly lower
temperature to allow for a thermal gradient that transfers heat out of the
lasing medium and into the panels, and the mass of piping and pumps is not to
be ignored, but it is all very hard to estimate and is more easily included in
a 15% overall power density penalty for unaccounted-for components. A 100 kW/kg reactor, 250 W/kg emitter-laser stack and 3.73
kW/kg radiators would mean an overall power density of 188 W/kg, after applying
the penalty. Gaseous lasing mediums could hold many advantages over a
crystal lasing medium. They require much less radiation intensity (W/m^3) to
start producing a laser beam. This
research states that an iodine laser requires 450 times
less intensity than an equivalent solid-state laser. It is also easier to cool
a gas laser , as we can simply get the gas to flow through a radiator. On the
other hand, turbulent flow and thermal lensing effects can deteriorate the
quality of a beam into uselessness. No attempts have been reported on applying the heat recycling
method from the Nd:YAG laser to greatly boost efficiency in a gas laser. Much
research has been performed instead on direct solar-pumped lasers where the
sunlight passes through a gaseous medium just once. The Sun can be considered to be a blackbody emitter at a
temperature of 5850K. Scientists have found the lasing mediums best suited to
being pumped by concentrated sunlight – they absorb the largest fraction of the
sunlight’s energy. That fraction is low in absolute terms, meaning poor overall
performance. An iodine-based
lasing medium reported 0.2% efficiency. Even worse efficiency of 0.01% was achieved when using an optically-pumped bromine
laser. Similarly, C3F7I, an iodine molecule which produces 1315 nm laser light,
was considered the best at 1% efficiency. Solid blackbody emitters are limited to temperatures just
above 3000K. There would be a great mismatch between the spectrum this sort of
blackbody releases and the wavelengths the gaseous lasing mediums cited above
require. In short, the efficiency would fall below 0.1% in all cases. One final option is Gallium-Arsenic-Phosphorus Vertical
External Cavity Surface Emitting Laser (VECSEL) designed for use in
solar-powered designs. It can absorb wavelengths between 300 and 900nm, which
represents 65% of the solar wavelengths but only 20% of the radiation from a
3000K blackbody. This works out to an emitter with a power density of 45.9
kW/kg. The average efficiency is 50% when producing a 1100nm beam.
Since it is extracting 20% of the wavelengths from the emitter, this amounts to
10% overall efficiency. Using the numbers in this
paper , we can surmise that the VECSEL can handle just
under 20 MW/kg. The mass of the laser is therefore negligible. With a 100 kW/kg
reactor, we work out a power density of 3.1 kW/kg. VECSELs can operate at high temperatures, but they suffer
from a significant
efficiency loss . We will keep them at 300K at most. It is very
troublesome as 20 MW of light is needed to be concentrated on the VECSEL to
start producing a laser beam. 90% of that light is being turned into waste heat
within a surface a few micrometers thick. Diamond heatsink helps in the short
term but not in continuous operation. Radiator power density will suffer. Even lightweight plastic
panels at 300K struggle to reach 1 kW/kg. When paired with the previous
equipment and under a 15% penalty for unaccounted for components, it means an
overall power density of 91 W/kg. This illustrates why an opaque pumping medium is unsuitable
for direct pumping as it does not allow for recycling of the waste heat. Filtered blackbody pumping A high temperature emitter radiates all of its wavelengths into
the blackbody-pumped lasing medium. We described a method above for preventing
the lasing medium from absorbing 98 to 99.9% of the incoming energy and turning
it immediately into waste heat. The requirement was that the lasing medium be
very transparent to simply let through the unwanted wavelengths. However, this imposes several design restrictions on the
lasing medium. It has to be thin, it has to be cooled by transparent fluids,
and it might have to sit right next to a source of high temperature heat while
staying at a low temperature itself. We can instead filter out solely the laser pumping
wavelengths from the blackbody spectrum and send those to the lasing medium
while recycling the rest. The tool to do this is a diffraction grating . There are many
other ways of extracting specific wavelengths from a blackbody radiation
spectrum, such as luminescent dyes or simple filters, but this method is the
most efficient. Like a prism, a diffraction grating can separate out
wavelengths from white light and send them off in different directions. For
most of those paths, we can put a mirror in the way that send the unwanted
wavelengths back into the blackbody emitter. For a small number of them, we
have a different mirror that reflects a specific wavelength into the lasing
medium. A lasing medium that receives just a small selection of
optimal wavelengths is called optically pumped. It is a common feature of a large
number of lasers, most notably LED-pumped designs. We can use them as a
reference for the potential performance of this method. We must note that while we can get high efficiencies, power
is still limited, as in the previous section. Extracting a portion of the
broadband spectrum that the lasing medium accepts also means that power output
is reduced to that portion. Another limitation is the temperature of the material serving
as a blackbody emitter. The nuclear reactor that supplies the heat to the
emitter is limited to 3000K in most cases, so the emitter must be at that
temperature or lower (even if a carbon emitter can handle 3915K at low
pressures and up
to 4800K at high pressures, while sublimating rapidly). Thankfully, the emission spectrum of a 3000K blackbody
overlaps well with the range of wavelengths an infrared fiber laser can be
pumped with. A good example is an erbium-doped lithium-lanthanide-fluoride
lasing medium in fiber lasers. We could use it to produce green light as pictured above, but invisible infrared is more effective. As we can see from here , erbium absorbs wavelengths between 960 and 1000 nm
rather well. It re-emits them at 1530 nm wavelength laser light with an
efficiency reported to be 42% in the ‘high Al content’ configuration, which is
close the 50% slope efficiency. In fact, the 960-1000 nm band represents 2.7% of the total
energy emitted. It is absorbing 125 kW from each square meter of emitter. If
the emitter is 1 cm thick plate of carbon and the diffraction grating, with
other internal optics needed to guide light into the narrow fiber laser, are
90% efficient, then we can expect an emitter power density of about 5.6 kW/kg. Another example absorbs 1460
to 1530 nm light to produce a 1650 nm beam. This is 3.7% of
the 3000K emitter’s spectrum, meaning an emitter power density of 7.7 kW/kg. The best numbers come from ytterbium
fiber lasers . They have a wider band of wavelengths that can
be pumped with, 850
to 1000 nm (which is 10.1% of the emitter’s output), and
they convert it into 1060 nm laser light with a very high efficiency (90%). It
would give the emitter an effective power density of 23.4 kW/kg. More
importantly, we have
examples operating at 773K. The respected
Thorlabs manufacturer gives information about the fiber
lasers themselves. They can handle 2.5 GW/m^2 continuously, up to 10GW/m^2
before destruction. Their largest LMA-20 core seems to be able to handle 38 kW/kg
of pumping power. It is far from the limit. Based on numbers provided by this
experiment , we estimate the fiber laser alone to be on the
order of 95kW/kg. Another
source works out a thermal-load-limited fiber laser
with 84% efficiency to have a power density of 695 kW/kg before the polymer
cladding melts at 473K. We can try to estimate the overall power density of a fiber
laser. A 100 kW/kg reactor is used to heat a 23.4 kW/kg emitter, where a diffraction
grating filters out 90% of the output to be fed into a fiber laser with 90%
efficiency and negligible mass. The waste heat is handled by 1mm thick carbon
fiber panels operating at 773K for a power density of 20.2 kW/kg. Altogether, this gives us 11 kW/kg after we include the same
penalty as before. If it is too difficult to direct light from a blackbody
emitter into the narrow cores of fiber lasers, then a simple lasing crystal
could be used. This is unlikely, as it has already been done , even in high radiation environments. Nd:YAG, liberated from the constraint of having to be nearly
entirely transparent, can achieve good performance. It can sustain a temperature
of 789K . We know that Nd:YAG can achieve excellent
efficiency when being pumped by very intense 808nm light to
produce a 1064nm beam, of 62%. It is hoped that this efficiency is maintained
across the lasing crystal’s 730 to 830nm absorption band. A 3000K blackbody emitter releases 6% of its energy in that
band. At 20 kg/m^2, this gives a power density of 13.8 kW/kg. We will cut off
10% due to losses involved in the filtering and internal optics. As before, the laser crystal itself handles enough pumping
power on its own to have a negligible mass. The radiators operating at 789K will require carbon fiber
panels. They’ll manage a power density of 22 kW/kg. Optimistically, we can expect a power density of 3.7 kW/kg
(reduced by 15%) when we include all the components necessary. Ultra-high-temperature blackbody pumped laser We must increase the
temperature of the blackbody emitter. It can radiate more energy across the
entire spectrum, and concentrates it in a narrower selection of shorter
wavelengths. Solid blackbody
surfaces are insufficient. To go beyond temperatures of 4000K, we must consider
liquid, gaseous and even plasma blackbody emitters. This requires us to abandon
conventional solid-fuel reactors and look at more extreme designs. There is a synergy to
be gained though. The nuclear fuel can also act as blackbody emitter if light
is allowed to escape the reactor. Let us consider two
very high to ultra-high temperature reactor designs that can do that: a 4200K
liquid uranium core with a gas-layer-protected transparent quartz window and a 19,000K
gaseous uranium-fluoride ‘lightbulb’ reactor. For each design, we
will try to find an appropriate laser that makes the best use of the blackbody
spectrum that is available. 4200K: Uranium melts at
1450K and boils at 4500K. It can therefore be held as a dense liquid at 4200K. We
base ourselves on this liquid-core nuclear
thermal rocket ,
where a layer of fissile fuel is held against the walls of a drum by
centrifugal effects. The walls are 10% reflective and 90% transparent. The reflective
sections hold neutron moderators to maintain criticality. This will be
beryllium protected by a protected silver
mirror .
It absorbs wavelengths shorter than 250 nm and reflects longer wavelengths with
98% reflectivity. We expect the neutron
moderator in the reflective sections, combined with a very highly enriched
uranium fuel, to still manage criticality. The spinning liquid should spread
the heat evenly and create a somewhat uniform 4200K surface acting as a
blackbody emitter. The transparent
sections are multi-layered fused quartz. It is very transparent to the wavelengths a
4200K blackbody emitter radiates – this means it does not heat up much by
absorbing the light passing through. We cannot have the
molten uranium touch the drum walls. We need a low thermal conductivity gas
layer to separate the fuel from the walls and act like a cushion of air for the
spinning fuel to sit on. Neon is perfect for this. It is mentioned as ideal for
being placed between quartz walls and fission fuel in nuclear lightbulb reactor
designs. The density difference between hot neon gas and uranium fuel is great
enough to prevent mixing, and the low thermal conductivity (coupled with high
gas velocity) reduces heat transfer through conduction. We might aim to have
neon enter the core at 1000K and exit at 2000K. There is still some
transfer of energy between the fuel and the walls because the mirrors are not
perfect; about 1.8% of the reactor’s emitted light is absorbed as heat in the
walls. Another 0.7% in the form of neutrons and gamma rays enters the
moderator. We therefore require an active cooling solution to channel coolant through
the beryllium and between the quartz layers. Helium can be used. It has the one
of the highest heat capacities of all simple gases, is inert and is even more
transparent than quartz. Beryllium and silver can survive 1000K temperatures,
so that will set our helium gas temperature limit. A heat exchanger can
transfer the heat the neon picks up to the cooler helium loop. The helium is
first expanded through a turbine. It radiates its accumulated heat at 1000K. It
is then compressed by a shaft driven by the turbine. If we assume that the
reactor has power density levels similar to this liquid core rocket (1 MW/kg) and that 2.5%
of its output becomes waste heat, then it can act as a blackbody emitter with a
power density of 980 kW/kg. Getting rid of the waste heat requires 1 mm thick
carbon fiber radiators operating at 1400K. Adding in the weight of those
radiators and we get 676 kW/kg. A good fit might be a
titanium-sapphire laser. It would absorb the large range of wavelengths between 400 and 650 nm . That’s 18.5% of a
4200K emitter’s spectrum. If we use a diffraction grating to filter out just
those wavelengths, and include some losses due to internal optics, we get 125
kW of useful wavelengths per kg of reactor-emitter. The crystal can
operate at up to 450K temperature , with 40% efficiency . Other experiments into the temperature
sensitivity of the Ti:Al2O3 crystal reveals lasing action even at 500K, with
mention of a 10% reduction to efficiency. We will use the 36% figure for the
laser to be on the safe side. Based on data from this flashpumping
experiment and this crystal database , we know that it can
easily handle 1.88 MW/kg. The mass contribution of the laser itself is
negligible. Any wavelengths that
get absorbed but are not turned into laser light become waste heat. At 450K
temperature, we can still use the lower density by HDPE plastic panels to get a
waste heat management solution with 4.6 kW/kg. Putting all the
components together and applying a 15% penalty just to be conservative, we obtain
an overall power density of 2.2 kW/kg. 19,000: If we want to go
hotter, we have to go for fissioning gases. Gas-core ‘lightbulb’ nuclear
reactors will be our model. The closed-cycle
‘lightbulb’ design has uranium heat up to the point where it is a very high temperature gas. That
gas radiated most of its energy in the form of ultraviolet light. A rocket
engine, as described in the ‘ NASA reference ’ designs, would have
the ultraviolet be absorbed by small tungsten particles seeded within a
hydrogen propellant flow. 4600 MW of power was released from an 8333K gas held
by quartz tubes, with a total engine mass of 32 tons. We want to use the
uranium gas as a light source. More specifically, we want to maximize the
amount of energy released in wavelengths between 120 and 190 nm. 19,000K is
required. It is within reach, as is shown here . Unlike a rocket
engine, we cannot have a hydrogen propellant absorb waste heat and release it
through a nozzle. The NASA reference was designed around reducing
waste heat to remove the need for radiators, but we will need them. Compared to
the reference design, we would have 27 times the output due to the higher
temperatures, but then we have to add the mass of the extra radiators. About 15% of the
reactor’s output is lost as waste heat in the original design . It was expected
that all the remaining output is absorbed by the propellant. We will be having
a lasing gas instead of propellant in between the quartz tube and the reactor
walls. The gas is too thin to absorb all the radiation, so to prevent it all
from being absorbed by the gas walls, we will use mirrors. Polished, UV-grade
aluminium can handle the UV radiation. It reflects it back through the laser
medium and into the quartz tubes to be recycled into heat. Just like the
blackbody-pumped Nd:YAG laser, we can create a situation where the pumping
light makes multiple passes through the lasing medium until the maximum
fraction is absorbed. Based on this calculator and this UV enhanced coating , we can say that
>95% of the wavelengths emitted by a 19,000K blackbody surface are
reflected. In total, 20% of the
reactor’s output becomes waste heat. Since aluminium melts
at 933K, we will keep a safe temperature margin and operate at 800K. This
should have only a marginal effect on the mirror’s
reflective properties. Waste heat must be removed at this temperature. As in
the liquid fuel reactor, the coolant fluid passes through a turbine, into a
radiator and is compressed on its way back into the reactor. Neon is used for
the quartz tube, helium for the reactor walls and the gaseous lasing medium is
its own coolant. Based on the
reference design, the reactor would have 4.56 MW/kg in output, or 3.65 MW/kg
after inefficiencies. If the radiators operate at 750K and use carbon fiber
fins, we can expect a power density for the reactor-emitter of 70.57 kW/kg. 28.9% of the
radiation emitted by a 19,000K blackbody surface, specifically wavelengths
between 120 and 190nm, is absorbed by a Xenon-Fluoride gas
laser . They are converted into a 350nm beam with 10% efficiency in a single-pass
experiment. In our case, the lasing medium is optically thin. Much of the
radiated energy passes through un-absorbed. The mirrors on the walls recycles
those wavelengths for multiple passes, similar to the Nd:YAG design mentioned
previously. Efficiency could rise as high as the maximal 43%. This paper suggests the maximal
efficiency for converting between absorbed and emitted light is 39%. We’ll use
an in-between figure of 30%. This means that the effective power density of the
reactor-emitter-laser system is 6.12 kW/kg. The XeF lasing medium
is mostly unaffected by temperatures of 800K, so long as the proper density is
maintained. We can therefore cool down the lasing medium with same radiators as
for the reactor-emitter (17.94 kW/kg). When we include the waste heat of the
laser, we get an overall power density of 2.9 kW/kg, after applying a 15%
penalty. A better power
density can be obtained by having a separate radiator for each component that
absorbs waste heat (quartz tubes, lasing medium, reactor walls) so that they
operate at higher temperatures, but that would be much more complex. Aerosol fluorescer reactor The design can be found with all its details in this
paper . Tiny micrometer-sized particles of fissile fuel are
surrounded in moderator and held at high temperatures. Their nuclear output, in
the form of fission fragments, escapes the micro-particles and strikes
Xenon-Fluoride or Iodine gas mixtures to create XeF* or I2* excimers. These
return to their stable state by releasing photons of a specific wavelength
through fluorescence. Their efficiency according to the following table is 19-50%. Simply, it is an excimer laser that is pumped by fission
fragments instead of electron beams. I2* is preferred for its greater
efficiency and ability to produce 342 nm beams. Technically, this is an
indirect pumping method, but it shares most of its attributes with direct
pumping reactor lasers. The overall design is conservatively estimated at 15 tons
overall mass, but with improvements to the micro-particle composition (such as
using plutonium or a reflective coating), it could be reduced even further. It
is able to produce 1 MJ pulses of 1 millisecond duration. With one pulse a
second, this a power density of 66 W/kg. One hundred pulses mean 6.6 kW/kg. One
thousand pulses, or quasi-continuous operation, would yield 66 kW per kg. The only limit to the reactor-laser’s power density is heat
build-up. At 5% efficiency, there is nineteen times more waste heat than laser
power leaving the reactor. We expect that using the UV mirrors from the previous design could drastically improve this figure by recycling light that was not absorbed by the lasing medium in the first pass through. Thankfully, the 1000K temperature allows for some
pretty effective management of waste heat. Carbon fiber panels of 1mm thickness, operating at 1000K
would handle 56.7 kW/kg. It would give the reactor a maximum power density of
2.4 kW/kg, including a 15% penalty for other equipment. If the reactor can operate closer to the melting point of its
beryllium moderator, perhaps 1400K, then it can increase its power density to
8.3 kW/kg. Conclusion Reactor lasers, when
designed appropriately, allow for high powered lasers from lightweight devices.
We have multiple examples of designs, either from references or calculated,
that output several kW of laser power per kg. The primary
limitations of many of the designs can be adjusted in ways that drastically
improve performance. The assumptions made (for instance, 1 cm thick carbon
emitter or flat panel radiators) are solely for the sake of easy comparison. It
is entirely acceptable to use 1mm thick emitting surfaces or one of the
alternate heat radiator designs mentioned in this
previous blog post .
Even better, many of the lower temperature lasers can have their waste heat
raised to a higher temperature using a heat pump. Smaller and lighter radiators
can then be used for a small penalty in overall efficiency to power the heat
pumps. Most of the lasers
discussed have rather long wavelengths. This is not great for use in space, as
the distances the beam has to traverse are huge and it multiplies the size of
the focusing optics required. For this reason, a method of shortening the
wavelengths, perhaps using frequency doubling, is recommended. Halving the
wavelength doubles the effective range. However, there is a 20-30% efficiency
penalty for using frequency doubling. Conversely, lasers which produce short
wavelength beams have a great advantage. The list of laser
options for each type of pumping is also by no means exhaustive. There might be
options not considered here that would allow for much greater performance… but
research on such options is very limited. For example, blackbody and LED
pumping seems to be a ‘dead’ field of research, now that diodes can produce a
single wavelength of the desired power. Up-to-date performance of those options
is therefore non-existent and so we cannot fairly compare their performance to
lasers which have been developed in their stead. It should be pointed out that a direct comparison between
reactor and electric lasers is not the whole story. Reactor lasers can easily
be converted into dual-mode use, where 100% of their heat is used for
propulsion purposes. A spaceship with an electric laser can only a fraction of
their output in an electric rocket. For example, the 4200K laser can have a
performance close to the liquid-core rocket design it was derived from. Other,
like the aerosol fluorescer laser, can both create a beam and heat propellant
at the same time. A nuclear-electric system must choose where to send its
electrical output and must accept the 60% reduction in overall power due to the
conversion steps between heat and electricity at all times. Finally, certain reactor lasers have hidden strength when
facing hostile forces. Mirrors work both ways. The same optics and mirrors that
transport your laser beam from the lasing medium out into space and to an enemy
target can be exploited by an enemy to get their beam to travel down the optics
and mirrors and reach your lasing medium. The lasing medium, assumed to be diodes or other
semiconductor lasers, has to operate at relatively low temperatures and so it
will melt and be destroyed under the focused glare of the enemy beam. Tactics around using lasers and counter-lasers, something
called ‘ eyeball-frying
contests ’ can sometimes lead to a large and powerful
warship being brought to a stalemate by a small counter-laser. A nuclear reactor laser’s lasing medium can be hot gas or
fissioning fuel. They are pretty much immune to the extra heat from an enemy
beam. It would render them much more resistant to ‘eye-frying’ tactics. This, and many other
strengths and consequences, become available to you if you include nuclear
reactor lasers in your science fiction. PS: I must apologize for using many sources that can only be fully accessed through a paywall. It was a necessity when researching this topic, on which little detail is available to the public. For this same reason, illustrations had to be derived from documents I cannot directly link to, but they are all referenced in links in this post.","Nuclear reactor lasers are devices that can generate lasers from nuclear energy with little to no intermediate conversion steps. We work out just how effective they can be, and how they stack up against conventional electrically-powered lasers. You might want to re-think your space warfare and power beaming after this. Nuclear energy and space have been intertwined since the dawn
of the space age. Fission power is reliable, enduring, compact and powerful.
These attributes make it ideal for spacecraft that must make every kilogram of
mass as useful and as functional as possible, as any excess mass would cost
several times its weight in extra propellant. They aim for equipment for the
highest specific power (or power density PD), meaning that it produces the most
watts per kilogram. Lasers use a lasing medium that is rapidly energized or
‘pumped’ by a power source. Modern lasers use electric discharges from
capacitors to pump gases, or a current running through diodes. The electrical
power source means that they need a generator and low temperature radiators in
addition to a nuclear reactor… these are significant mass penalties to a
spaceship. Fission reactions produce X-rays, neutrons and high energy
ions. The idea to use them to pump a lasing medium has existed ever since the
first coherent wavelengths were released from a ruby crystal in 1960. Much
research has been done in the 80s and 90s into nuclear-pumped lasers,
especially as part of the Strategic Defense Initiative. If laser power can be
generated directly from a reactor, there could be significant gains in power
density. The research findings on nuclear reactor lasers were
promising in many cases but did not succeed in convincing the US and Russian
governments to continue their development. Why were they unsuccessful and what
alternative designs could realize their promise of high power density lasers? Distinction between NBPLs and NRLs Most mentions of nuclear pumped lasers relate to nuclear bomb-pumped lasers . They are exemplified by project Excalibur: the idea was to use
the output of a nuclear device to blast metal tubes with X-rays and have them
produce coherent beams of their own. We will not be focusing on it. The concept has many problems that prevent it from being a
useful replacement for conventional lasers. You first need to expend a nuclear
warhead, which is a terribly wasteful use of fissile material. Only a tiny
fraction of the warhead’s X-rays, which are emitted in all directions, are
intercepted by the metal tube. From those, a tiny fraction of its energy is
converted into coherent X-rays. If you multiply both fractions, you find an
exceedingly low conversion ratio . Further
research has revealed this to be on the order
of <0.00001% . It also works for just a microsecond, each shot
destroys its surroundings and its effective range is limited by relatively poor
divergence of the beam. These downsides are acceptable for a system meant to
take down a sudden and massive wave of ICBMs at ranges of 100 to 1000
kilometers, but not much else. Instead, we will be looking at nuclear reactor pumped
lasers. These are lasers that draw power from the continuous output of a
controlled fission reaction. Performance We talk about efficiency and power density to compare the
lasers mentioned in this post. How are we working them out? For efficiency, we multiply the reactor’s output by the
individual efficiencies of the laser conversion steps, and assume all
inefficiencies become waste heat. The waste heat is handled by flat
double-sided radiator panels operating at the lowest temperature of all the
components, which is usually the laser itself. This will give a slightly poorer performance than what could
be obtained from a real world engineered concept. The choice of radiator is
influenced by the need for easy comparison instead of maximizing performance in
individual designs. We will note the individual efficiencies as Er for the
reactor, El for the laser and Ex for other components. The overall efficiency
will be OE. OE = Er * Ex * El * Eh In most cases, Er and Eh can be approximated as equal to 1.
As we are considering lasers for use in space with output on the order of
several megawatts and beyond, it is more accurate to use the slope efficiency
of a design rather than the reported efficiency. Laboratory tests on the
milliwatt scale are dominated by the threshold pumping power, which cuts into
output and reduces the efficiency. As the power is scaled up, the threshold
power becomes a smaller and smaller fraction of the total power. Calculating power density (PD) in Watts per kg for several
components working with each other’s outputs is a bit more complicated. As
above, we’ll note them PDr, PDl, PDh, PDx and so on. The equation is: PD = (PDr * OE) / (1 + PDr (Ex/PDx + Ex*El/PDl + (1 -
Ex*El)/PDh)) Generally, the reactor is a negligible contributor to the
total mass of equipment, as it is in the several hundred kW/kg, so we can
simplify the equation to: PD = OE / (Ex/PDx + Ex*El/PDl + (1 - Ex*El)/PDh) Inputting PDx, PDl and PDh values in kW/kg creates a PD value
also in kW/kg. Direct Pumping The most straightforward way of creating a nuclear reactor
laser is to have fission products interact directly with a lasing medium. Only
gaseous lasing mediums, such as xenon or neon, could survive the conditions
inside a nuclear reactor indefinitely, but this has not stopped attempts at
pumping a solid lasing medium. Three methods of energizing or pumping a laser medium have
been successful. Wall pumping Wall pumping uses a channel through which a gaseous lasing
medium flows while surrounded by nuclear fuel. The fuel is bombarded by
neutrons from a nearby reactor. The walls then release fission fragments that
collide with atoms in the lasing medium and transfer their energy to be
released as photons. The fragments are large and slow so they don’t travel far
into a gas and tend to concentrate their energy near the walls. If the channels
are too wide, the center of the channel is untouched and the lasing medium is
unevenly pumped. This can create a laser of very poor quality. To counter this, the channels are made as narrow as possible,
giving the fragments less distance to travel. However, this multiplies the
numbers of channels needed to produce a certain amount of power, and with it
the mass penalty from having many walls filled with dense fissile fuel. The walls absorb half of the fission fragments they create immediately. They release the surviving
fragments from both faces of fissile fuel wall. So, a large fraction of the
fission fragment power is wasted. They are also limited by the melting
temperatures of the fuel. If too many fission fragments are absorbed, the heat
would the walls to fail, so active cooling is needed for high power output. The FALCON experiments achieved an efficiency of 2.5% when
using xenon to produce a 1733 nm wavelength beam. Gas
laser experiments at relatively low temperatures reported
single-wavelength efficiencies as high as 3.6%. The best reported performance
was 5.6% efficiency from an Argon-Xenon mix producing 1733 nm laser light, from
Sandia National Laboratory. Producing shorter wavelengths using other lasing
mediums, such as metal vapours, resulted in much worse performance (<0.01%
efficiency). Higher efficiencies could be gained from a carbon monoxide or
carbon dioxide lasing medium, with up to 70%
possible , but their wavelengths are 5 and 10 micrometers
respectively (which makes for a very short ranged laser) and a real efficiency
of only 0.5%
has been demonstrated . One estimate
presented in this
paper is a wall-pumped mix of Helium and Xenon that
converts 400 MW of nuclear power into 1 MW of laser power with a 1733 nm
wavelength. It is expected to mass 100 tons. That is an efficiency of 0.25% and
a power density of just 10 W/kg. It illustrates the fact that designs meant to
sit on the ground are not useful
references. A chart from this NASA report reads as a direct pumped nuclear
reactor laser with 10% overall efficiency having a power density of about 500
W/kg, brought down to 200 W/kg when including radiators, shielding and other
components. Volumetric
pumping Volumetric
pumping has Helium-3 mixed in with a gaseous lasing medium to absorb neutrons
from a reactor. Neutrons are quite penetrating and can traverse large volumes
of gas, while Helium 3 is very good at absorbing neutrons. When Helium-3
absorbs neutrons, it creates charged particles that in turn energize lasing
atoms when they enter into contact with each other. Therefore, neutrons can
fully energize the entire volume of gas. The main advantages of this type of
laser pumping is the much reduced temperature restrictions and the lighter
structures needed to handle the gas when compared to multiple narrow channels
filled with dense fuel. However,
Helium-3 converts neutrons into charged particles with very low efficiency,
with volumetric pumping experiments reporting 0.1 to 1% efficiency overall. This
is because the charged particles being created contain only a small portion of
the energy the Helium-3 initially receives. Semiconductor
pumping The final
successful pumping method is direct pumping of a semiconductor laser with
fission fragments. The efficiency is respectable at 20%, and the compact laser
allows for significant mass savings, but the lasing medium is quickly destroyed
by the intense radiation. It consists of a thin layer of highly enriched
uranium sitting on a silicon or gallium semiconductor, with diamond serving as
both moderator and heatsink. There are very
few details available on this type of pumping. A
space-optimized semiconductor design from this
paper that suggests that an overall power density of 5
kW/kg is possible. It notes later on that even 18 kW/kg is achievable. It is
unknown how the radiation degradation issue could be solved and whether this
includes waste heat management equipment. Without an operating temperature and
a detailed breakdown of the component masses assumed, we cannot work it out on
our own. Other direct
pumped designs Wall or
volumetric pumping designs were conceived when nuclear technology was still new
and fission fuel had to stay in dense and solid masses to achieve criticality.
More modern advances allow for more effective forms for the fuel to take. The lasing medium
could be made to interact directly with a self-sustaining reactor core. This
involves mixing the lasing medium with uranium
fluoride gas , uranium aerosols, uranium vapour at very high
temperatures or uranium micro-particles at low temperatures. The trouble with
uranium fluoride gas and aerosols or micro-particles is the tendency for them
to re-absorb
the energy (quenching) of excited lasing atoms. This has
prevented any lasing action from being realized in all experiments so far. As this
diagram shows, uranium fluoride gas absorbs most
wavelengths very well, further reducing laser output. If there is a
lasing medium that is not quenched by uranium fluoride, then there is potential
for extraordinary performance. An early NASA
report on an uranium fluoride reactor lasers for space
gives a best figure of 73.3 W/kg from what is understood to be a 100 MW reactor
converting 5% of its output into 340 nanometer wavelength laser light. With the
radiators in the report, this falls to 56.8 W/kg. If we bump up
the operating temperature to 1000K, reduce the moderator to the 20cm minimum,
replace the pressure vessel with ceramics and use more modern carbon fiber
radiators, we can expect the power density of that design to increase to 136
W/kg. Uranium vapours
are another option. They require temperatures of 4000K and upwards but if the
problem of handling those temperatures is solved (perhaps by using actively
cooled graphite containers), then 80%
of the nuclear output can be used to excite the lasing medium, for an
overall efficiency that is increased four-fold over wall pumping designs. More speculative is encasing uranium inside a C60
Buckminsterfullerene sphere. Fission fragments could exit the sphere while also
preventing the quenching of the lasing material. This would allow for excellent
transmission of nuclear power into the lasing medium, without extreme
temperature requirements. Nuclear-electric
comparison With these
numbers in mind, it does not look like direct pumping is the revolutionary
upgrade over electric lasers that was predicted in the 60s. Turbines,
generators, radiators and laser diodes have improved by a lot, and they deliver
a large fraction of a reactor’s output in laser light. We expect a
space-optimized nuclear-electric powerplant with a diode laser to have rather
good performance when using cutting edge technology available today. With a 100 kW/kg
reactor core, a 50% efficient turbine at 10 kW/kg, an 80% efficient electrical
generator at 5 kW/kg, powering 60% efficient diodes at 7 kW/kg and using 1.34 kW/kg radiators to
get rid of waste heat ( 323K
temperature ), we get an overall efficiency of 24% and a
power density of 323 W/kg. A more advanced system using a very powerful 1 MW/kg reactor core, a 60% efficient MHD generator at 100 kW/kg with 1000K 56.7 kW/kg radiators, powering a 50% efficient fiber laser cooled by 450K 2.3 kW/kg radiators, would get an overall efficiency of 30% and a power density of 2.5 kW/kg. Can we beat these
figures with reactor lasers? Indirect pumping The direct pumping method uses the small fraction of a
reactor’s output that is released in the form of neutrons, or problematic
fission fragments. Would it not be better to use the entire output of the
nuclear reaction? Indirect pumping allows us to use 100% of the output in the
form of heat. This heat can then be converted into laser light in various ways. Research and data for some of the following types of lasers
comes from solar-heated
designs that attempt to use concentrated sunlight to
heat up an intermediate blackbody that in turn radiates onto a lasing medium.
For our purposes, we are replacing the heat of the Sun with a reactor power
source. It is sometimes called a ‘blackbody laser’ in that case. Blackbody radiation pump At high temperatures, a blackbody emitter radiates strongly
in certain wavelengths that lasing materials can be pumped with. A reactor can
easily heat up a black carbon surface to temperatures of 2000 to 3000K – this
is what nuclear rockets are expected to operate at anyhow. Some of the spectrum of a blackbody at those temperatures
lies within the wavelengths that are absorbed well by certain crystal and
gaseous lasing mediums. Neodymium-doped Ytrrium-Aluminium-Garnet (Nd:YAG)
specifically is a crystal lasing medium that has been thoroughly investigated
as a candidate for a blackbody-pumped laser. It produces 1060 nm beams. Efficiency figures vary. A simple single-pass configuration results in very poor
efficiency (0.1 to 2%). This is because the lasing medium only absorbs a small
portion of the entire blackbody spectrum. In simpler terms, if we shine
everything from 100 nm to 10,000 nm onto a lasing medium, it will convert 0.1
to 2% of that light into a laser beam and turn the rest into waste heat. With
this performance, blackbody pumped lasers are no better than direct pumped
reactor laser designs from the previous section. Instead, researchers have come up with a way to recover the
99 to 99.9% of the blackbody spectrum that the lasing medium does not use. This
is the recycled-heat blackbody pumped laser. An Nd:YAG crystal sits inside a ‘hot tube’. Blackbody
radiation coming from the tube walls passes through the crystal. The crystal is
thin and nearly transparent to all wavelengths. The illustration above uses Ti:Sapphire but the concept is the same for any laser crystal. Only about 2% of blackbody spectrum is absorbed with every
pass through the crystal. The remaining 97 to 98% pass through to return to the
hot tube’s walls. They are absorbed by a black carbon surface and recycled into
heat. Over many radiation, absorption and recycling cycles, the fraction of
total energy that becomes laser light increases for an excellent overall
efficiency. 35%
efficiency with a Nd:YAG laser was achieved. The only downside is that the Nd:YAG crystal needs intense radiation
within it to start producing a beam. The previous document suggests that 150
MW/m^3 is needed. Another
source indicates 800 MW/m^3. We also know that
efficiency increases with intensity. If we aim for 1 GW/m^3, which corresponds
to 268 Watts shining on each square centimetre of a 1 cm diameter lasing rod,
we would need a 1:1 ratio of emitting to receiving area if the emitter has a
temperature of at least 2622K. From a power conversion perspective, a 98% transparent
crystal that converts 35% of spectrum it absorbs means it is only converting
0.7% of every Watt of blackbody radiation that shines through it. So, a crystal
rod that receives 268 Watts on each square centimetre will release 1.87 W of
laser light. We can use the 1:1 ratio of emitter and receiver area to
reduce weight and increase power density. Ideally, we can stack emitter and
receiver as flat surfaces separated by just enough space to prevent heat
transfer through conduction. Reactor coolant channels, carbon emitting surface (1cm),
filler gas, Nd:YAG crystal (1cm) and helium channels can be placed back to
back. The volume could end up looking like a rectangular cuboid, interspaced by
mirror cavities. 20 kg/m^2 carbon layers and 45.5 kg/m^2 crystal layers that
release 1.87 W per square centimetre, with a 15% weight surplus for other
structures and coolant pipes, puts this component’s power density at about 250
W/kg. The laser crystal is cooled from 417K according to the set-up in
this paper . Getting rid of megawatts at such a low
temperature is troublesome. Huge radiator surface areas will be required. As we are using flat panel radiators throughout this post, we
have only two variables: material density, material thickness and operating
temperature. The latter is set by the referenced document. We will choose a 1mm thick radiator made of low
density polyethylene . We obtain 0.46 kg/m^2 are plausible. When
radiating at 417K, they could achieve 3.73 kW/kg. It is likely that they will operate at a slightly lower
temperature to allow for a thermal gradient that transfers heat out of the
lasing medium and into the panels, and the mass of piping and pumps is not to
be ignored, but it is all very hard to estimate and is more easily included in
a 15% overall power density penalty for unaccounted-for components. A 100 kW/kg reactor, 250 W/kg emitter-laser stack and 3.73
kW/kg radiators would mean an overall power density of 188 W/kg, after applying
the penalty. Gaseous lasing mediums could hold many advantages over a
crystal lasing medium. They require much less radiation intensity (W/m^3) to
start producing a laser beam. This
research states that an iodine laser requires 450 times
less intensity than an equivalent solid-state laser. It is also easier to cool
a gas laser , as we can simply get the gas to flow through a radiator. On the
other hand, turbulent flow and thermal lensing effects can deteriorate the
quality of a beam into uselessness. No attempts have been reported on applying the heat recycling
method from the Nd:YAG laser to greatly boost efficiency in a gas laser. Much
research has been performed instead on direct solar-pumped lasers where the
sunlight passes through a gaseous medium just once. The Sun can be considered to be a blackbody emitter at a
temperature of 5850K. Scientists have found the lasing mediums best suited to
being pumped by concentrated sunlight – they absorb the largest fraction of the
sunlight’s energy. That fraction is low in absolute terms, meaning poor overall
performance. An iodine-based
lasing medium reported 0.2% efficiency. Even worse efficiency of 0.01% was achieved when using an optically-pumped bromine
laser. Similarly, C3F7I, an iodine molecule which produces 1315 nm laser light,
was considered the best at 1% efficiency. Solid blackbody emitters are limited to temperatures just
above 3000K. There would be a great mismatch between the spectrum this sort of
blackbody releases and the wavelengths the gaseous lasing mediums cited above
require. In short, the efficiency would fall below 0.1% in all cases. One final option is Gallium-Arsenic-Phosphorus Vertical
External Cavity Surface Emitting Laser (VECSEL) designed for use in
solar-powered designs. It can absorb wavelengths between 300 and 900nm, which
represents 65% of the solar wavelengths but only 20% of the radiation from a
3000K blackbody. This works out to an emitter with a power density of 45.9
kW/kg. The average efficiency is 50% when producing a 1100nm beam.
Since it is extracting 20% of the wavelengths from the emitter, this amounts to
10% overall efficiency. Using the numbers in this
paper , we can surmise that the VECSEL can handle just
under 20 MW/kg. The mass of the laser is therefore negligible. With a 100 kW/kg
reactor, we work out a power density of 3.1 kW/kg. VECSELs can operate at high temperatures, but they suffer
from a significant
efficiency loss . We will keep them at 300K at most. It is very
troublesome as 20 MW of light is needed to be concentrated on the VECSEL to
start producing a laser beam. 90% of that light is being turned into waste heat
within a surface a few micrometers thick. Diamond heatsink helps in the short
term but not in continuous operation. Radiator power density will suffer. Even lightweight plastic
panels at 300K struggle to reach 1 kW/kg. When paired with the previous
equipment and under a 15% penalty for unaccounted for components, it means an
overall power density of 91 W/kg. This illustrates why an opaque pumping medium is unsuitable
for direct pumping as it does not allow for recycling of the waste heat. Filtered blackbody pumping A high temperature emitter radiates all of its wavelengths into
the blackbody-pumped lasing medium. We described a method above for preventing
the lasing medium from absorbing 98 to 99.9% of the incoming energy and turning
it immediately into waste heat. The requirement was that the lasing medium be
very transparent to simply let through the unwanted wavelengths. However, this imposes several design restrictions on the
lasing medium. It has to be thin, it has to be cooled by transparent fluids,
and it might have to sit right next to a source of high temperature heat while
staying at a low temperature itself. We can instead filter out solely the laser pumping
wavelengths from the blackbody spectrum and send those to the lasing medium
while recycling the rest. The tool to do this is a diffraction grating . There are many
other ways of extracting specific wavelengths from a blackbody radiation
spectrum, such as luminescent dyes or simple filters, but this method is the
most efficient. Like a prism, a diffraction grating can separate out
wavelengths from white light and send them off in different directions. For
most of those paths, we can put a mirror in the way that send the unwanted
wavelengths back into the blackbody emitter. For a small number of them, we
have a different mirror that reflects a specific wavelength into the lasing
medium. A lasing medium that receives just a small selection of
optimal wavelengths is called optically pumped. It is a common feature of a large
number of lasers, most notably LED-pumped designs. We can use them as a
reference for the potential performance of this method. We must note that while we can get high efficiencies, power
is still limited, as in the previous section. Extracting a portion of the
broadband spectrum that the lasing medium accepts also means that power output
is reduced to that portion. Another limitation is the temperature of the material serving
as a blackbody emitter. The nuclear reactor that supplies the heat to the
emitter is limited to 3000K in most cases, so the emitter must be at that
temperature or lower (even if a carbon emitter can handle 3915K at low
pressures and up
to 4800K at high pressures, while sublimating rapidly). Thankfully, the emission spectrum of a 3000K blackbody
overlaps well with the range of wavelengths an infrared fiber laser can be
pumped with. A good example is an erbium-doped lithium-lanthanide-fluoride
lasing medium in fiber lasers. We could use it to produce green light as pictured above, but invisible infrared is more effective. As we can see from here , erbium absorbs wavelengths between 960 and 1000 nm
rather well. It re-emits them at 1530 nm wavelength laser light with an
efficiency reported to be 42% in the ‘high Al content’ configuration, which is
close the 50% slope efficiency. In fact, the 960-1000 nm band represents 2.7% of the total
energy emitted. It is absorbing 125 kW from each square meter of emitter. If
the emitter is 1 cm thick plate of carbon and the diffraction grating, with
other internal optics needed to guide light into the narrow fiber laser, are
90% efficient, then we can expect an emitter power density of about 5.6 kW/kg. Another example absorbs 1460
to 1530 nm light to produce a 1650 nm beam. This is 3.7% of
the 3000K emitter’s spectrum, meaning an emitter power density of 7.7 kW/kg. The best numbers come from ytterbium
fiber lasers . They have a wider band of wavelengths that can
be pumped with, 850
to 1000 nm (which is 10.1% of the emitter’s output), and
they convert it into 1060 nm laser light with a very high efficiency (90%). It
would give the emitter an effective power density of 23.4 kW/kg. More
importantly, we have
examples operating at 773K. The respected
Thorlabs manufacturer gives information about the fiber
lasers themselves. They can handle 2.5 GW/m^2 continuously, up to 10GW/m^2
before destruction. Their largest LMA-20 core seems to be able to handle 38 kW/kg
of pumping power. It is far from the limit. Based on numbers provided by this
experiment , we estimate the fiber laser alone to be on the
order of 95kW/kg. Another
source works out a thermal-load-limited fiber laser
with 84% efficiency to have a power density of 695 kW/kg before the polymer
cladding melts at 473K. We can try to estimate the overall power density of a fiber
laser. A 100 kW/kg reactor is used to heat a 23.4 kW/kg emitter, where a diffraction
grating filters out 90% of the output to be fed into a fiber laser with 90%
efficiency and negligible mass. The waste heat is handled by 1mm thick carbon
fiber panels operating at 773K for a power density of 20.2 kW/kg. Altogether, this gives us 11 kW/kg after we include the same
penalty as before. If it is too difficult to direct light from a blackbody
emitter into the narrow cores of fiber lasers, then a simple lasing crystal
could be used. This is unlikely, as it has already been done , even in high radiation environments. Nd:YAG, liberated from the constraint of having to be nearly
entirely transparent, can achieve good performance. It can sustain a temperature
of 789K . We know that Nd:YAG can achieve excellent
efficiency when being pumped by very intense 808nm light to
produce a 1064nm beam, of 62%. It is hoped that this efficiency is maintained
across the lasing crystal’s 730 to 830nm absorption band. A 3000K blackbody emitter releases 6% of its energy in that
band. At 20 kg/m^2, this gives a power density of 13.8 kW/kg. We will cut off
10% due to losses involved in the filtering and internal optics. As before, the laser crystal itself handles enough pumping
power on its own to have a negligible mass. The radiators operating at 789K will require carbon fiber
panels. They’ll manage a power density of 22 kW/kg. Optimistically, we can expect a power density of 3.7 kW/kg
(reduced by 15%) when we include all the components necessary. Ultra-high-temperature blackbody pumped laser We must increase the
temperature of the blackbody emitter. It can radiate more energy across the
entire spectrum, and concentrates it in a narrower selection of shorter
wavelengths. Solid blackbody
surfaces are insufficient. To go beyond temperatures of 4000K, we must consider
liquid, gaseous and even plasma blackbody emitters. This requires us to abandon
conventional solid-fuel reactors and look at more extreme designs. There is a synergy to
be gained though. The nuclear fuel can also act as blackbody emitter if light
is allowed to escape the reactor. Let us consider two
very high to ultra-high temperature reactor designs that can do that: a 4200K
liquid uranium core with a gas-layer-protected transparent quartz window and a 19,000K
gaseous uranium-fluoride ‘lightbulb’ reactor. For each design, we
will try to find an appropriate laser that makes the best use of the blackbody
spectrum that is available. 4200K: Uranium melts at
1450K and boils at 4500K. It can therefore be held as a dense liquid at 4200K. We
base ourselves on this liquid-core nuclear
thermal rocket ,
where a layer of fissile fuel is held against the walls of a drum by
centrifugal effects. The walls are 10% reflective and 90% transparent. The reflective
sections hold neutron moderators to maintain criticality. This will be
beryllium protected by a protected silver
mirror .
It absorbs wavelengths shorter than 250 nm and reflects longer wavelengths with
98% reflectivity. We expect the neutron
moderator in the reflective sections, combined with a very highly enriched
uranium fuel, to still manage criticality. The spinning liquid should spread
the heat evenly and create a somewhat uniform 4200K surface acting as a
blackbody emitter. The transparent
sections are multi-layered fused quartz. It is very transparent to the wavelengths a
4200K blackbody emitter radiates – this means it does not heat up much by
absorbing the light passing through. We cannot have the
molten uranium touch the drum walls. We need a low thermal conductivity gas
layer to separate the fuel from the walls and act like a cushion of air for the
spinning fuel to sit on. Neon is perfect for this. It is mentioned as ideal for
being placed between quartz walls and fission fuel in nuclear lightbulb reactor
designs. The density difference between hot neon gas and uranium fuel is great
enough to prevent mixing, and the low thermal conductivity (coupled with high
gas velocity) reduces heat transfer through conduction. We might aim to have
neon enter the core at 1000K and exit at 2000K. There is still some
transfer of energy between the fuel and the walls because the mirrors are not
perfect; about 1.8% of the reactor’s emitted light is absorbed as heat in the
walls. Another 0.7% in the form of neutrons and gamma rays enters the
moderator. We therefore require an active cooling solution to channel coolant through
the beryllium and between the quartz layers. Helium can be used. It has the one
of the highest heat capacities of all simple gases, is inert and is even more
transparent than quartz. Beryllium and silver can survive 1000K temperatures,
so that will set our helium gas temperature limit. A heat exchanger can
transfer the heat the neon picks up to the cooler helium loop. The helium is
first expanded through a turbine. It radiates its accumulated heat at 1000K. It
is then compressed by a shaft driven by the turbine. If we assume that the
reactor has power density levels similar to this liquid core rocket (1 MW/kg) and that 2.5%
of its output becomes waste heat, then it can act as a blackbody emitter with a
power density of 980 kW/kg. Getting rid of the waste heat requires 1 mm thick
carbon fiber radiators operating at 1400K. Adding in the weight of those
radiators and we get 676 kW/kg. A good fit might be a
titanium-sapphire laser. It would absorb the large range of wavelengths between 400 and 650 nm . That’s 18.5% of a
4200K emitter’s spectrum. If we use a diffraction grating to filter out just
those wavelengths, and include some losses due to internal optics, we get 125
kW of useful wavelengths per kg of reactor-emitter. The crystal can
operate at up to 450K temperature , with 40% efficiency . Other experiments into the temperature
sensitivity of the Ti:Al2O3 crystal reveals lasing action even at 500K, with
mention of a 10% reduction to efficiency. We will use the 36% figure for the
laser to be on the safe side. Based on data from this flashpumping
experiment and this crystal database , we know that it can
easily handle 1.88 MW/kg. The mass contribution of the laser itself is
negligible. Any wavelengths that
get absorbed but are not turned into laser light become waste heat. At 450K
temperature, we can still use the lower density by HDPE plastic panels to get a
waste heat management solution with 4.6 kW/kg. Putting all the
components together and applying a 15% penalty just to be conservative, we obtain
an overall power density of 2.2 kW/kg. 19,000: If we want to go
hotter, we have to go for fissioning gases. Gas-core ‘lightbulb’ nuclear
reactors will be our model. The closed-cycle
‘lightbulb’ design has uranium heat up to the point where it is a very high temperature gas. That
gas radiated most of its energy in the form of ultraviolet light. A rocket
engine, as described in the ‘ NASA reference ’ designs, would have
the ultraviolet be absorbed by small tungsten particles seeded within a
hydrogen propellant flow. 4600 MW of power was released from an 8333K gas held
by quartz tubes, with a total engine mass of 32 tons. We want to use the
uranium gas as a light source. More specifically, we want to maximize the
amount of energy released in wavelengths between 120 and 190 nm. 19,000K is
required. It is within reach, as is shown here . Unlike a rocket
engine, we cannot have a hydrogen propellant absorb waste heat and release it
through a nozzle. The NASA reference was designed around reducing
waste heat to remove the need for radiators, but we will need them. Compared to
the reference design, we would have 27 times the output due to the higher
temperatures, but then we have to add the mass of the extra radiators. About 15% of the
reactor’s output is lost as waste heat in the original design . It was expected
that all the remaining output is absorbed by the propellant. We will be having
a lasing gas instead of propellant in between the quartz tube and the reactor
walls. The gas is too thin to absorb all the radiation, so to prevent it all
from being absorbed by the gas walls, we will use mirrors. Polished, UV-grade
aluminium can handle the UV radiation. It reflects it back through the laser
medium and into the quartz tubes to be recycled into heat. Just like the
blackbody-pumped Nd:YAG laser, we can create a situation where the pumping
light makes multiple passes through the lasing medium until the maximum
fraction is absorbed. Based on this calculator and this UV enhanced coating , we can say that
>95% of the wavelengths emitted by a 19,000K blackbody surface are
reflected. In total, 20% of the
reactor’s output becomes waste heat. Since aluminium melts
at 933K, we will keep a safe temperature margin and operate at 800K. This
should have only a marginal effect on the mirror’s
reflective properties. Waste heat must be removed at this temperature. As in
the liquid fuel reactor, the coolant fluid passes through a turbine, into a
radiator and is compressed on its way back into the reactor. Neon is used for
the quartz tube, helium for the reactor walls and the gaseous lasing medium is
its own coolant. Based on the
reference design, the reactor would have 4.56 MW/kg in output, or 3.65 MW/kg
after inefficiencies. If the radiators operate at 750K and use carbon fiber
fins, we can expect a power density for the reactor-emitter of 70.57 kW/kg. 28.9% of the
radiation emitted by a 19,000K blackbody surface, specifically wavelengths
between 120 and 190nm, is absorbed by a Xenon-Fluoride gas
laser . They are converted into a 350nm beam with 10% efficiency in a single-pass
experiment. In our case, the lasing medium is optically thin. Much of the
radiated energy passes through un-absorbed. The mirrors on the walls recycles
those wavelengths for multiple passes, similar to the Nd:YAG design mentioned
previously. Efficiency could rise as high as the maximal 43%. This paper suggests the maximal
efficiency for converting between absorbed and emitted light is 39%. We’ll use
an in-between figure of 30%. This means that the effective power density of the
reactor-emitter-laser system is 6.12 kW/kg. The XeF lasing medium
is mostly unaffected by temperatures of 800K, so long as the proper density is
maintained. We can therefore cool down the lasing medium with same radiators as
for the reactor-emitter (17.94 kW/kg). When we include the waste heat of the
laser, we get an overall power density of 2.9 kW/kg, after applying a 15%
penalty. A better power
density can be obtained by having a separate radiator for each component that
absorbs waste heat (quartz tubes, lasing medium, reactor walls) so that they
operate at higher temperatures, but that would be much more complex. Aerosol fluorescer reactor The design can be found with all its details in this
paper . Tiny micrometer-sized particles of fissile fuel are
surrounded in moderator and held at high temperatures. Their nuclear output, in
the form of fission fragments, escapes the micro-particles and strikes
Xenon-Fluoride or Iodine gas mixtures to create XeF* or I2* excimers. These
return to their stable state by releasing photons of a specific wavelength
through fluorescence. Their efficiency according to the following table is 19-50%. Simply, it is an excimer laser that is pumped by fission
fragments instead of electron beams. I2* is preferred for its greater
efficiency and ability to produce 342 nm beams. Technically, this is an
indirect pumping method, but it shares most of its attributes with direct
pumping reactor lasers. The overall design is conservatively estimated at 15 tons
overall mass, but with improvements to the micro-particle composition (such as
using plutonium or a reflective coating), it could be reduced even further. It
is able to produce 1 MJ pulses of 1 millisecond duration. With one pulse a
second, this a power density of 66 W/kg. One hundred pulses mean 6.6 kW/kg. One
thousand pulses, or quasi-continuous operation, would yield 66 kW per kg. The only limit to the reactor-laser’s power density is heat
build-up. At 5% efficiency, there is nineteen times more waste heat than laser
power leaving the reactor. We expect that using the UV mirrors from the previous design could drastically improve this figure by recycling light that was not absorbed by the lasing medium in the first pass through. Thankfully, the 1000K temperature allows for some
pretty effective management of waste heat. Carbon fiber panels of 1mm thickness, operating at 1000K
would handle 56.7 kW/kg. It would give the reactor a maximum power density of
2.4 kW/kg, including a 15% penalty for other equipment. If the reactor can operate closer to the melting point of its
beryllium moderator, perhaps 1400K, then it can increase its power density to
8.3 kW/kg. Conclusion Reactor lasers, when
designed appropriately, allow for high powered lasers from lightweight devices.
We have multiple examples of designs, either from references or calculated,
that output several kW of laser power per kg. The primary
limitations of many of the designs can be adjusted in ways that drastically
improve performance. The assumptions made (for instance, 1 cm thick carbon
emitter or flat panel radiators) are solely for the sake of easy comparison. It
is entirely acceptable to use 1mm thick emitting surfaces or one of the
alternate heat radiator designs mentioned in this
previous blog post .
Even better, many of the lower temperature lasers can have their waste heat
raised to a higher temperature using a heat pump. Smaller and lighter radiators
can then be used for a small penalty in overall efficiency to power the heat
pumps. Most of the lasers
discussed have rather long wavelengths. This is not great for use in space, as
the distances the beam has to traverse are huge and it multiplies the size of
the focusing optics required. For this reason, a method of shortening the
wavelengths, perhaps using frequency doubling, is recommended. Halving the
wavelength doubles the effective range. However, there is a 20-30% efficiency
penalty for using frequency doubling. Conversely, lasers which produce short
wavelength beams have a great advantage. The list of laser
options for each type of pumping is also by no means exhaustive. There might be
options not considered here that would allow for much greater performance… but
research on such options is very limited. For example, blackbody and LED
pumping seems to be a ‘dead’ field of research, now that diodes can produce a
single wavelength of the desired power. Up-to-date performance of those options
is therefore non-existent and so we cannot fairly compare their performance to
lasers which have been developed in their stead. It should be pointed out that a direct comparison between
reactor and electric lasers is not the whole story. Reactor lasers can easily
be converted into dual-mode use, where 100% of their heat is used for
propulsion purposes. A spaceship with an electric laser can only a fraction of
their output in an electric rocket. For example, the 4200K laser can have a
performance close to the liquid-core rocket design it was derived from. Other,
like the aerosol fluorescer laser, can both create a beam and heat propellant
at the same time. A nuclear-electric system must choose where to send its
electrical output and must accept the 60% reduction in overall power due to the
conversion steps between heat and electricity at all times. Finally, certain reactor lasers have hidden strength when
facing hostile forces. Mirrors work both ways. The same optics and mirrors that
transport your laser beam from the lasing medium out into space and to an enemy
target can be exploited by an enemy to get their beam to travel down the optics
and mirrors and reach your lasing medium. The lasing medium, assumed to be diodes or other
semiconductor lasers, has to operate at relatively low temperatures and so it
will melt and be destroyed under the focused glare of the enemy beam. Tactics around using lasers and counter-lasers, something
called ‘ eyeball-frying
contests ’ can sometimes lead to a large and powerful
warship being brought to a stalemate by a small counter-laser. A nuclear reactor laser’s lasing medium can be hot gas or
fissioning fuel. They are pretty much immune to the extra heat from an enemy
beam. It would render them much more resistant to ‘eye-frying’ tactics. This, and many other
strengths and consequences, become available to you if you include nuclear
reactor lasers in your science fiction. PS: I must apologize for using many sources that can only be fully accessed through a paywall. It was a necessity when researching this topic, on which little detail is available to the public. For this same reason, illustrations had to be derived from documents I cannot directly link to, but they are all referenced in links in this post.",Nuclear Reactor Lasers: From fission to photon,"

Key Points:
",Software Development,"Nuclear reactor lasers are devices that can generate lasers from nuclear energy with little to no intermediate conversion steps. We work out just how effective they can be, and how they stack up against conventional electrically-powered lasers. You might want to re-think your space warfare and power beaming after this. Nuclear energy and space have been intertwined since the dawn
of the space age. Fission power is reliable, enduring, compact and powerful.
These attributes make it ideal for spacecraft that must make every kilogram of
mass as useful and as functional as possible, as any excess mass would cost
several times its weight in extra propellant. They aim for equipment for the
highest specific power (or power density PD), meaning that it produces the most
watts per kilogram. Lasers use a lasing medium that is rapidly energized or
‘pumped’ by a power source. Modern lasers use electric discharges from
capacitors to pump gases, or a current running through diodes. The electrical
power source means that they need a generator and low temperature radiators in
addition to a nuclear reactor… these are significant mass penalties to a
spaceship. Fission reactions produce X-rays, neutrons and high energy
ions. The idea to use them to pump a lasing medium has existed ever since the
first coherent wavelengths were released from a ruby crystal in 1960. Much
research has been done in the 80s and 90s into nuclear-pumped lasers,
especially as part of the Strategic Defense Initiative. If laser power can be
generated directly from a reactor, there could be significant gains in power
density. The research findings on nuclear reactor lasers were
promising in many cases but did not succeed in convincing the US and Russian
governments to continue their development. Why were they unsuccessful and what
alternative designs could realize their promise of high power density lasers? Distinction between NBPLs and NRLs Most mentions of nuclear pumped lasers relate to nuclear bomb-pumped lasers . They are exemplified by project Excalibur: the idea was to use
the output of a nuclear device to blast metal tubes with X-rays and have them
produce coherent beams of their own. We will not be focusing on it. The concept has many problems that prevent it from being a
useful replacement for conventional lasers. You first need to expend a nuclear
warhead, which is a terribly wasteful use of fissile material. Only a tiny
fraction of the warhead’s X-rays, which are emitted in all directions, are
intercepted by the metal tube. From those, a tiny fraction of its energy is
converted into coherent X-rays. If you multiply both fractions, you find an
exceedingly low conversion ratio . Further
research has revealed this to be on the order
of <0.00001% . It also works for just a microsecond, each shot
destroys its surroundings and its effective range is limited by relatively poor
divergence of the beam. These downsides are acceptable for a system meant to
take down a sudden and massive wave of ICBMs at ranges of 100 to 1000
kilometers, but not much else. Instead, we will be looking at nuclear reactor pumped
lasers. These are lasers that draw power from the continuous output of a
controlled fission reaction. Performance We talk about efficiency and power density to compare the
lasers mentioned in this post. How are we working them out? For efficiency, we multiply the reactor’s output by the
individual efficiencies of the laser conversion steps, and assume all
inefficiencies become waste heat. The waste heat is handled by flat
double-sided radiator panels operating at the lowest temperature of all the
components, which is usually the laser itself. This will give a slightly poorer performance than what could
be obtained from a real world engineered concept. The choice of radiator is
influenced by the need for easy comparison instead of maximizing performance in
individual designs. We will note the individual efficiencies as Er for the
reactor, El for the laser and Ex for other components. The overall efficiency
will be OE. OE = Er * Ex * El * Eh In most cases, Er and Eh can be approximated as equal to 1.
As we are considering lasers for use in space with output on the order of
several megawatts and beyond, it is more accurate to use the slope efficiency
of a design rather than the reported efficiency. Laboratory tests on the
milliwatt scale are dominated by the threshold pumping power, which cuts into
output and reduces the efficiency. As the power is scaled up, the threshold
power becomes a smaller and smaller fraction of the total power. Calculating power density (PD) in Watts per kg for several
components working with each other’s outputs is a bit more complicated. As
above, we’ll note them PDr, PDl, PDh, PDx and so on. The equation is: PD = (PDr * OE) / (1 + PDr (Ex/PDx + Ex*El/PDl + (1 -
Ex*El)/PDh)) Generally, the reactor is a negligible contributor to the
total mass of equipment, as it is in the several hundred kW/kg, so we can
simplify the equation to: PD = OE / (Ex/PDx + Ex*El/PDl + (1 - Ex*El)/PDh) Inputting PDx, PDl and PDh values in kW/kg creates a PD value
also in kW/kg. Direct Pumping The most straightforward way of creating a nuclear reactor
laser is to have fission products interact directly with a lasing medium. Only
gaseous lasing mediums, such as xenon or neon, could survive the conditions
inside a nuclear reactor indefinitely, but this has not stopped attempts at
pumping a solid lasing medium. Three methods of energizing or pumping a laser medium have
been successful. Wall pumping Wall pumping uses a channel through which a gaseous lasing
medium flows while surrounded by nuclear fuel. The fuel is bombarded by
neutrons from a nearby reactor. The walls then release fission fragments that
collide with atoms in the lasing medium and transfer their energy to be
released as photons. The fragments are large and slow so they don’t travel far
into a gas and tend to concentrate their energy near the walls. If the channels
are too wide, the center of the channel is untouched and the lasing medium is
unevenly pumped. This can create a laser of very poor quality. To counter this, the channels are made as narrow as possible,
giving the fragments less distance to travel. However, this multiplies the
numbers of channels needed to produce a certain amount of power, and with it
the mass penalty from having many walls filled with dense fissile fuel. The walls absorb half of the fission fragments they create immediately. They release the surviving
fragments from both faces of fissile fuel wall. So, a large fraction of the
fission fragment power is wasted. They are also limited by the melting
temperatures of the fuel. If too many fission fragments are absorbed, the heat
would the walls to fail, so active cooling is needed for high power output. The FALCON experiments achieved an efficiency of 2.5% when
using xenon to produce a 1733 nm wavelength beam. Gas
laser experiments at relatively low temperatures reported
single-wavelength efficiencies as high as 3.6%. The best reported performance
was 5.6% efficiency from an Argon-Xenon mix producing 1733 nm laser light, from
Sandia National Laboratory. Producing shorter wavelengths using other lasing
mediums, such as metal vapours, resulted in much worse performance (<0.01%
efficiency). Higher efficiencies could be gained from a carbon monoxide or
carbon dioxide lasing medium, with up to 70%
possible , but their wavelengths are 5 and 10 micrometers
respectively (which makes for a very short ranged laser) and a real efficiency
of only 0.5%
has been demonstrated . One estimate
presented in this
paper is a wall-pumped mix of Helium and Xenon that
converts 400 MW of nuclear power into 1 MW of laser power with a 1733 nm
wavelength. It is expected to mass 100 tons. That is an efficiency of 0.25% and
a power density of just 10 W/kg. It illustrates the fact that designs meant to
sit on the ground are not useful
references. A chart from this NASA report reads as a direct pumped nuclear
reactor laser with 10% overall efficiency having a power density of about 500
W/kg, brought down to 200 W/kg when including radiators, shielding and other
components. Volumetric
pumping Volumetric
pumping has Helium-3 mixed in with a gaseous lasing medium to absorb neutrons
from a reactor. Neutrons are quite penetrating and can traverse large volumes
of gas, while Helium 3 is very good at absorbing neutrons. When Helium-3
absorbs neutrons, it creates charged particles that in turn energize lasing
atoms when they enter into contact with each other. Therefore, neutrons can
fully energize the entire volume of gas. The main advantages of this type of
laser pumping is the much reduced temperature restrictions and the lighter
structures needed to handle the gas when compared to multiple narrow channels
filled with dense fuel. However,
Helium-3 converts neutrons into charged particles with very low efficiency,
with volumetric pumping experiments reporting 0.1 to 1% efficiency overall. This
is because the charged particles being created contain only a small portion of
the energy the Helium-3 initially receives. Semiconductor
pumping The final
successful pumping method is direct pumping of a semiconductor laser with
fission fragments. The efficiency is respectable at 20%, and the compact laser
allows for significant mass savings, but the lasing medium is quickly destroyed
by the intense radiation. It consists of a thin layer of highly enriched
uranium sitting on a silicon or gallium semiconductor, with diamond serving as
both moderator and heatsink. There are very
few details available on this type of pumping. A
space-optimized semiconductor design from this
paper that suggests that an overall power density of 5
kW/kg is possible. It notes later on that even 18 kW/kg is achievable. It is
unknown how the radiation degradation issue could be solved and whether this
includes waste heat management equipment. Without an operating temperature and
a detailed breakdown of the component masses assumed, we cannot work it out on
our own. Other direct
pumped designs Wall or
volumetric pumping designs were conceived when nuclear technology was still new
and fission fuel had to stay in dense and solid masses to achieve criticality.
More modern advances allow for more effective forms for the fuel to take. The lasing medium
could be made to interact directly with a self-sustaining reactor core. This
involves mixing the lasing medium with uranium
fluoride gas , uranium aerosols, uranium vapour at very high
temperatures or uranium micro-particles at low temperatures. The trouble with
uranium fluoride gas and aerosols or micro-particles is the tendency for them
to re-absorb
the energy (quenching) of excited lasing atoms. This has
prevented any lasing action from being realized in all experiments so far. As this
diagram shows, uranium fluoride gas absorbs most
wavelengths very well, further reducing laser output. If there is a
lasing medium that is not quenched by uranium fluoride, then there is potential
for extraordinary performance. An early NASA
report on an uranium fluoride reactor lasers for space
gives a best figure of 73.3 W/kg from what is understood to be a 100 MW reactor
converting 5% of its output into 340 nanometer wavelength laser light. With the
radiators in the report, this falls to 56.8 W/kg. If we bump up
the operating temperature to 1000K, reduce the moderator to the 20cm minimum,
replace the pressure vessel with ceramics and use more modern carbon fiber
radiators, we can expect the power density of that design to increase to 136
W/kg. Uranium vapours
are another option. They require temperatures of 4000K and upwards but if the
problem of handling those temperatures is solved (perhaps by using actively
cooled graphite containers), then 80%
of the nuclear output can be used to excite the lasing medium, for an
overall efficiency that is increased four-fold over wall pumping designs. More speculative is encasing uranium inside a C60
Buckminsterfullerene sphere. Fission fragments could exit the sphere while also
preventing the quenching of the lasing material. This would allow for excellent
transmission of nuclear power into the lasing medium, without extreme
temperature requirements. Nuclear-electric
comparison With these
numbers in mind, it does not look like direct pumping is the revolutionary
upgrade over electric lasers that was predicted in the 60s. Turbines,
generators, radiators and laser diodes have improved by a lot, and they deliver
a large fraction of a reactor’s output in laser light. We expect a
space-optimized nuclear-electric powerplant with a diode laser to have rather
good performance when using cutting edge technology available today. With a 100 kW/kg
reactor core, a 50% efficient turbine at 10 kW/kg, an 80% efficient electrical
generator at 5 kW/kg, powering 60% efficient diodes at 7 kW/kg and using 1.34 kW/kg radiators to
get rid of waste heat ( 323K
temperature ), we get an overall efficiency of 24% and a
power density of 323 W/kg. A more advanced system using a very powerful 1 MW/kg reactor core, a 60% efficient MHD generator at 100 kW/kg with 1000K 56.7 kW/kg radiators, powering a 50% efficient fiber laser cooled by 450K 2.3 kW/kg radiators, would get an overall efficiency of 30% and a power density of 2.5 kW/kg. Can we beat these
figures with reactor lasers? Indirect pumping The direct pumping method uses the small fraction of a
reactor’s output that is released in the form of neutrons, or problematic
fission fragments. Would it not be better to use the entire output of the
nuclear reaction? Indirect pumping allows us to use 100% of the output in the
form of heat. This heat can then be converted into laser light in various ways. Research and data for some of the following types of lasers
comes from solar-heated
designs that attempt to use concentrated sunlight to
heat up an intermediate blackbody that in turn radiates onto a lasing medium.
For our purposes, we are replacing the heat of the Sun with a reactor power
source. It is sometimes called a ‘blackbody laser’ in that case. Blackbody radiation pump At high temperatures, a blackbody emitter radiates strongly
in certain wavelengths that lasing materials can be pumped with. A reactor can
easily heat up a black carbon surface to temperatures of 2000 to 3000K – this
is what nuclear rockets are expected to operate at anyhow. Some of the spectrum of a blackbody at those temperatures
lies within the wavelengths that are absorbed well by certain crystal and
gaseous lasing mediums. Neodymium-doped Ytrrium-Aluminium-Garnet (Nd:YAG)
specifically is a crystal lasing medium that has been thoroughly investigated
as a candidate for a blackbody-pumped laser. It produces 1060 nm beams. Efficiency figures vary. A simple single-pass configuration results in very poor
efficiency (0.1 to 2%). This is because the lasing medium only absorbs a small
portion of the entire blackbody spectrum. In simpler terms, if we shine
everything from 100 nm to 10,000 nm onto a lasing medium, it will convert 0.1
to 2% of that light into a laser beam and turn the rest into waste heat. With
this performance, blackbody pumped lasers are no better than direct pumped
reactor laser designs from the previous section. Instead, researchers have come up with a way to recover the
99 to 99.9% of the blackbody spectrum that the lasing medium does not use. This
is the recycled-heat blackbody pumped laser. An Nd:YAG crystal sits inside a ‘hot tube’. Blackbody
radiation coming from the tube walls passes through the crystal. The crystal is
thin and nearly transparent to all wavelengths. The illustration above uses Ti:Sapphire but the concept is the same for any laser crystal. Only about 2% of blackbody spectrum is absorbed with every
pass through the crystal. The remaining 97 to 98% pass through to return to the
hot tube’s walls. They are absorbed by a black carbon surface and recycled into
heat. Over many radiation, absorption and recycling cycles, the fraction of
total energy that becomes laser light increases for an excellent overall
efficiency. 35%
efficiency with a Nd:YAG laser was achieved. The only downside is that the Nd:YAG crystal needs intense radiation
within it to start producing a beam. The previous document suggests that 150
MW/m^3 is needed. Another
source indicates 800 MW/m^3. We also know that
efficiency increases with intensity. If we aim for 1 GW/m^3, which corresponds
to 268 Watts shining on each square centimetre of a 1 cm diameter lasing rod,
we would need a 1:1 ratio of emitting to receiving area if the emitter has a
temperature of at least 2622K. From a power conversion perspective, a 98% transparent
crystal that converts 35% of spectrum it absorbs means it is only converting
0.7% of every Watt of blackbody radiation that shines through it. So, a crystal
rod that receives 268 Watts on each square centimetre will release 1.87 W of
laser light. We can use the 1:1 ratio of emitter and receiver area to
reduce weight and increase power density. Ideally, we can stack emitter and
receiver as flat surfaces separated by just enough space to prevent heat
transfer through conduction. Reactor coolant channels, carbon emitting surface (1cm),
filler gas, Nd:YAG crystal (1cm) and helium channels can be placed back to
back. The volume could end up looking like a rectangular cuboid, interspaced by
mirror cavities. 20 kg/m^2 carbon layers and 45.5 kg/m^2 crystal layers that
release 1.87 W per square centimetre, with a 15% weight surplus for other
structures and coolant pipes, puts this component’s power density at about 250
W/kg. The laser crystal is cooled from 417K according to the set-up in
this paper . Getting rid of megawatts at such a low
temperature is troublesome. Huge radiator surface areas will be required. As we are using flat panel radiators throughout this post, we
have only two variables: material density, material thickness and operating
temperature. The latter is set by the referenced document. We will choose a 1mm thick radiator made of low
density polyethylene . We obtain 0.46 kg/m^2 are plausible. When
radiating at 417K, they could achieve 3.73 kW/kg. It is likely that they will operate at a slightly lower
temperature to allow for a thermal gradient that transfers heat out of the
lasing medium and into the panels, and the mass of piping and pumps is not to
be ignored, but it is all very hard to estimate and is more easily included in
a 15% overall power density penalty for unaccounted-for components. A 100 kW/kg reactor, 250 W/kg emitter-laser stack and 3.73
kW/kg radiators would mean an overall power density of 188 W/kg, after applying
the penalty. Gaseous lasing mediums could hold many advantages over a
crystal lasing medium. They require much less radiation intensity (W/m^3) to
start producing a laser beam. This
research states that an iodine laser requires 450 times
less intensity than an equivalent solid-state laser. It is also easier to cool
a gas laser , as we can simply get the gas to flow through a radiator. On the
other hand, turbulent flow and thermal lensing effects can deteriorate the
quality of a beam into uselessness. No attempts have been reported on applying the heat recycling
method from the Nd:YAG laser to greatly boost efficiency in a gas laser. Much
research has been performed instead on direct solar-pumped lasers where the
sunlight passes through a gaseous medium just once. The Sun can be considered to be a blackbody emitter at a
temperature of 5850K. Scientists have found the lasing mediums best suited to
being pumped by concentrated sunlight – they absorb the largest fraction of the
sunlight’s energy. That fraction is low in absolute terms, meaning poor overall
performance. An iodine-based
lasing medium reported 0.2% efficiency. Even worse efficiency of 0.01% was achieved when using an optically-pumped bromine
laser. Similarly, C3F7I, an iodine molecule which produces 1315 nm laser light,
was considered the best at 1% efficiency. Solid blackbody emitters are limited to temperatures just
above 3000K. There would be a great mismatch between the spectrum this sort of
blackbody releases and the wavelengths the gaseous lasing mediums cited above
require. In short, the efficiency would fall below 0.1% in all cases. One final option is Gallium-Arsenic-Phosphorus Vertical
External Cavity Surface Emitting Laser (VECSEL) designed for use in
solar-powered designs. It can absorb wavelengths between 300 and 900nm, which
represents 65% of the solar wavelengths but only 20% of the radiation from a
3000K blackbody. This works out to an emitter with a power density of 45.9
kW/kg. The average efficiency is 50% when producing a 1100nm beam.
Since it is extracting 20% of the wavelengths from the emitter, this amounts to
10% overall efficiency. Using the numbers in this
paper , we can surmise that the VECSEL can handle just
under 20 MW/kg. The mass of the laser is therefore negligible. With a 100 kW/kg
reactor, we work out a power density of 3.1 kW/kg. VECSELs can operate at high temperatures, but they suffer
from a significant
efficiency loss . We will keep them at 300K at most. It is very
troublesome as 20 MW of light is needed to be concentrated on the VECSEL to
start producing a laser beam. 90% of that light is being turned into waste heat
within a surface a few micrometers thick. Diamond heatsink helps in the short
term but not in continuous operation. Radiator power density will suffer. Even lightweight plastic
panels at 300K struggle to reach 1 kW/kg. When paired with the previous
equipment and under a 15% penalty for unaccounted for components, it means an
overall power density of 91 W/kg. This illustrates why an opaque pumping medium is unsuitable
for direct pumping as it does not allow for recycling of the waste heat. Filtered blackbody pumping A high temperature emitter radiates all of its wavelengths into
the blackbody-pumped lasing medium. We described a method above for preventing
the lasing medium from absorbing 98 to 99.9% of the incoming energy and turning
it immediately into waste heat. The requirement was that the lasing medium be
very transparent to simply let through the unwanted wavelengths. However, this imposes several design restrictions on the
lasing medium. It has to be thin, it has to be cooled by transparent fluids,
and it might have to sit right next to a source of high temperature heat while
staying at a low temperature itself. We can instead filter out solely the laser pumping
wavelengths from the blackbody spectrum and send those to the lasing medium
while recycling the rest. The tool to do this is a diffraction grating . There are many
other ways of extracting specific wavelengths from a blackbody radiation
spectrum, such as luminescent dyes or simple filters, but this method is the
most efficient. Like a prism, a diffraction grating can separate out
wavelengths from white light and send them off in different directions. For
most of those paths, we can put a mirror in the way that send the unwanted
wavelengths back into the blackbody emitter. For a small number of them, we
have a different mirror that reflects a specific wavelength into the lasing
medium. A lasing medium that receives just a small selection of
optimal wavelengths is called optically pumped. It is a common feature of a large
number of lasers, most notably LED-pumped designs. We can use them as a
reference for the potential performance of this method. We must note that while we can get high efficiencies, power
is still limited, as in the previous section. Extracting a portion of the
broadband spectrum that the lasing medium accepts also means that power output
is reduced to that portion. Another limitation is the temperature of the material serving
as a blackbody emitter. The nuclear reactor that supplies the heat to the
emitter is limited to 3000K in most cases, so the emitter must be at that
temperature or lower (even if a carbon emitter can handle 3915K at low
pressures and up
to 4800K at high pressures, while sublimating rapidly). Thankfully, the emission spectrum of a 3000K blackbody
overlaps well with the range of wavelengths an infrared fiber laser can be
pumped with. A good example is an erbium-doped lithium-lanthanide-fluoride
lasing medium in fiber lasers. We could use it to produce green light as pictured above, but invisible infrared is more effective. As we can see from here , erbium absorbs wavelengths between 960 and 1000 nm
rather well. It re-emits them at 1530 nm wavelength laser light with an
efficiency reported to be 42% in the ‘high Al content’ configuration, which is
close the 50% slope efficiency. In fact, the 960-1000 nm band represents 2.7% of the total
energy emitted. It is absorbing 125 kW from each square meter of emitter. If
the emitter is 1 cm thick plate of carbon and the diffraction grating, with
other internal optics needed to guide light into the narrow fiber laser, are
90% efficient, then we can expect an emitter power density of about 5.6 kW/kg. Another example absorbs 1460
to 1530 nm light to produce a 1650 nm beam. This is 3.7% of
the 3000K emitter’s spectrum, meaning an emitter power density of 7.7 kW/kg. The best numbers come from ytterbium
fiber lasers . They have a wider band of wavelengths that can
be pumped with, 850
to 1000 nm (which is 10.1% of the emitter’s output), and
they convert it into 1060 nm laser light with a very high efficiency (90%). It
would give the emitter an effective power density of 23.4 kW/kg. More
importantly, we have
examples operating at 773K. The respected
Thorlabs manufacturer gives information about the fiber
lasers themselves. They can handle 2.5 GW/m^2 continuously, up to 10GW/m^2
before destruction. Their largest LMA-20 core seems to be able to handle 38 kW/kg
of pumping power. It is far from the limit. Based on numbers provided by this
experiment , we estimate the fiber laser alone to be on the
order of 95kW/kg. Another
source works out a thermal-load-limited fiber laser
with 84% efficiency to have a power density of 695 kW/kg before the polymer
cladding melts at 473K. We can try to estimate the overall power density of a fiber
laser. A 100 kW/kg reactor is used to heat a 23.4 kW/kg emitter, where a diffraction
grating filters out 90% of the output to be fed into a fiber laser with 90%
efficiency and negligible mass. The waste heat is handled by 1mm thick carbon
fiber panels operating at 773K for a power density of 20.2 kW/kg. Altogether, this gives us 11 kW/kg after we include the same
penalty as before. If it is too difficult to direct light from a blackbody
emitter into the narrow cores of fiber lasers, then a simple lasing crystal
could be used. This is unlikely, as it has already been done , even in high radiation environments. Nd:YAG, liberated from the constraint of having to be nearly
entirely transparent, can achieve good performance. It can sustain a temperature
of 789K . We know that Nd:YAG can achieve excellent
efficiency when being pumped by very intense 808nm light to
produce a 1064nm beam, of 62%. It is hoped that this efficiency is maintained
across the lasing crystal’s 730 to 830nm absorption band. A 3000K blackbody emitter releases 6% of its energy in that
band. At 20 kg/m^2, this gives a power density of 13.8 kW/kg. We will cut off
10% due to losses involved in the filtering and internal optics. As before, the laser crystal itself handles enough pumping
power on its own to have a negligible mass. The radiators operating at 789K will require carbon fiber
panels. They’ll manage a power density of 22 kW/kg. Optimistically, we can expect a power density of 3.7 kW/kg
(reduced by 15%) when we include all the components necessary. Ultra-high-temperature blackbody pumped laser We must increase the
temperature of the blackbody emitter. It can radiate more energy across the
entire spectrum, and concentrates it in a narrower selection of shorter
wavelengths. Solid blackbody
surfaces are insufficient. To go beyond temperatures of 4000K, we must consider
liquid, gaseous and even plasma blackbody emitters. This requires us to abandon
conventional solid-fuel reactors and look at more extreme designs. There is a synergy to
be gained though. The nuclear fuel can also act as blackbody emitter if light
is allowed to escape the reactor. Let us consider two
very high to ultra-high temperature reactor designs that can do that: a 4200K
liquid uranium core with a gas-layer-protected transparent quartz window and a 19,000K
gaseous uranium-fluoride ‘lightbulb’ reactor. For each design, we
will try to find an appropriate laser that makes the best use of the blackbody
spectrum that is available. 4200K: Uranium melts at
1450K and boils at 4500K. It can therefore be held as a dense liquid at 4200K. We
base ourselves on this liquid-core nuclear
thermal rocket ,
where a layer of fissile fuel is held against the walls of a drum by
centrifugal effects. The walls are 10% reflective and 90% transparent. The reflective
sections hold neutron moderators to maintain criticality. This will be
beryllium protected by a protected silver
mirror .
It absorbs wavelengths shorter than 250 nm and reflects longer wavelengths with
98% reflectivity. We expect the neutron
moderator in the reflective sections, combined with a very highly enriched
uranium fuel, to still manage criticality. The spinning liquid should spread
the heat evenly and create a somewhat uniform 4200K surface acting as a
blackbody emitter. The transparent
sections are multi-layered fused quartz. It is very transparent to the wavelengths a
4200K blackbody emitter radiates – this means it does not heat up much by
absorbing the light passing through. We cannot have the
molten uranium touch the drum walls. We need a low thermal conductivity gas
layer to separate the fuel from the walls and act like a cushion of air for the
spinning fuel to sit on. Neon is perfect for this. It is mentioned as ideal for
being placed between quartz walls and fission fuel in nuclear lightbulb reactor
designs. The density difference between hot neon gas and uranium fuel is great
enough to prevent mixing, and the low thermal conductivity (coupled with high
gas velocity) reduces heat transfer through conduction. We might aim to have
neon enter the core at 1000K and exit at 2000K. There is still some
transfer of energy between the fuel and the walls because the mirrors are not
perfect; about 1.8% of the reactor’s emitted light is absorbed as heat in the
walls. Another 0.7% in the form of neutrons and gamma rays enters the
moderator. We therefore require an active cooling solution to channel coolant through
the beryllium and between the quartz layers. Helium can be used. It has the one
of the highest heat capacities of all simple gases, is inert and is even more
transparent than quartz. Beryllium and silver can survive 1000K temperatures,
so that will set our helium gas temperature limit. A heat exchanger can
transfer the heat the neon picks up to the cooler helium loop. The helium is
first expanded through a turbine. It radiates its accumulated heat at 1000K. It
is then compressed by a shaft driven by the turbine. If we assume that the
reactor has power density levels similar to this liquid core rocket (1 MW/kg) and that 2.5%
of its output becomes waste heat, then it can act as a blackbody emitter with a
power density of 980 kW/kg. Getting rid of the waste heat requires 1 mm thick
carbon fiber radiators operating at 1400K. Adding in the weight of those
radiators and we get 676 kW/kg. A good fit might be a
titanium-sapphire laser. It would absorb the large range of wavelengths between 400 and 650 nm . That’s 18.5% of a
4200K emitter’s spectrum. If we use a diffraction grating to filter out just
those wavelengths, and include some losses due to internal optics, we get 125
kW of useful wavelengths per kg of reactor-emitter. The crystal can
operate at up to 450K temperature , with 40% efficiency . Other experiments into the temperature
sensitivity of the Ti:Al2O3 crystal reveals lasing action even at 500K, with
mention of a 10% reduction to efficiency. We will use the 36% figure for the
laser to be on the safe side. Based on data from this flashpumping
experiment and this crystal database , we know that it can
easily handle 1.88 MW/kg. The mass contribution of the laser itself is
negligible. Any wavelengths that
get absorbed but are not turned into laser light become waste heat. At 450K
temperature, we can still use the lower density by HDPE plastic panels to get a
waste heat management solution with 4.6 kW/kg. Putting all the
components together and applying a 15% penalty just to be conservative, we obtain
an overall power density of 2.2 kW/kg. 19,000: If we want to go
hotter, we have to go for fissioning gases. Gas-core ‘lightbulb’ nuclear
reactors will be our model. The closed-cycle
‘lightbulb’ design has uranium heat up to the point where it is a very high temperature gas. That
gas radiated most of its energy in the form of ultraviolet light. A rocket
engine, as described in the ‘ NASA reference ’ designs, would have
the ultraviolet be absorbed by small tungsten particles seeded within a
hydrogen propellant flow. 4600 MW of power was released from an 8333K gas held
by quartz tubes, with a total engine mass of 32 tons. We want to use the
uranium gas as a light source. More specifically, we want to maximize the
amount of energy released in wavelengths between 120 and 190 nm. 19,000K is
required. It is within reach, as is shown here . Unlike a rocket
engine, we cannot have a hydrogen propellant absorb waste heat and release it
through a nozzle. The NASA reference was designed around reducing
waste heat to remove the need for radiators, but we will need them. Compared to
the reference design, we would have 27 times the output due to the higher
temperatures, but then we have to add the mass of the extra radiators. About 15% of the
reactor’s output is lost as waste heat in the original design . It was expected
that all the remaining output is absorbed by the propellant. We will be having
a lasing gas instead of propellant in between the quartz tube and the reactor
walls. The gas is too thin to absorb all the radiation, so to prevent it all
from being absorbed by the gas walls, we will use mirrors. Polished, UV-grade
aluminium can handle the UV radiation. It reflects it back through the laser
medium and into the quartz tubes to be recycled into heat. Just like the
blackbody-pumped Nd:YAG laser, we can create a situation where the pumping
light makes multiple passes through the lasing medium until the maximum
fraction is absorbed. Based on this calculator and this UV enhanced coating , we can say that
>95% of the wavelengths emitted by a 19,000K blackbody surface are
reflected. In total, 20% of the
reactor’s output becomes waste heat. Since aluminium melts
at 933K, we will keep a safe temperature margin and operate at 800K. This
should have only a marginal effect on the mirror’s
reflective properties. Waste heat must be removed at this temperature. As in
the liquid fuel reactor, the coolant fluid passes through a turbine, into a
radiator and is compressed on its way back into the reactor. Neon is used for
the quartz tube, helium for the reactor walls and the gaseous lasing medium is
its own coolant. Based on the
reference design, the reactor would have 4.56 MW/kg in output, or 3.65 MW/kg
after inefficiencies. If the radiators operate at 750K and use carbon fiber
fins, we can expect a power density for the reactor-emitter of 70.57 kW/kg. 28.9% of the
radiation emitted by a 19,000K blackbody surface, specifically wavelengths
between 120 and 190nm, is absorbed by a Xenon-Fluoride gas
laser . They are converted into a 350nm beam with 10% efficiency in a single-pass
experiment. In our case, the lasing medium is optically thin. Much of the
radiated energy passes through un-absorbed. The mirrors on the walls recycles
those wavelengths for multiple passes, similar to the Nd:YAG design mentioned
previously. Efficiency could rise as high as the maximal 43%. This paper suggests the maximal
efficiency for converting between absorbed and emitted light is 39%. We’ll use
an in-between figure of 30%. This means that the effective power density of the
reactor-emitter-laser system is 6.12 kW/kg. The XeF lasing medium
is mostly unaffected by temperatures of 800K, so long as the proper density is
maintained. We can therefore cool down the lasing medium with same radiators as
for the reactor-emitter (17.94 kW/kg). When we include the waste heat of the
laser, we get an overall power density of 2.9 kW/kg, after applying a 15%
penalty. A better power
density can be obtained by having a separate radiator for each component that
absorbs waste heat (quartz tubes, lasing medium, reactor walls) so that they
operate at higher temperatures, but that would be much more complex. Aerosol fluorescer reactor The design can be found with all its details in this
paper . Tiny micrometer-sized particles of fissile fuel are
surrounded in moderator and held at high temperatures. Their nuclear output, in
the form of fission fragments, escapes the micro-particles and strikes
Xenon-Fluoride or Iodine gas mixtures to create XeF* or I2* excimers. These
return to their stable state by releasing photons of a specific wavelength
through fluorescence. Their efficiency according to the following table is 19-50%. Simply, it is an excimer laser that is pumped by fission
fragments instead of electron beams. I2* is preferred for its greater
efficiency and ability to produce 342 nm beams. Technically, this is an
indirect pumping method, but it shares most of its attributes with direct
pumping reactor lasers. The overall design is conservatively estimated at 15 tons
overall mass, but with improvements to the micro-particle composition (such as
using plutonium or a reflective coating), it could be reduced even further. It
is able to produce 1 MJ pulses of 1 millisecond duration. With one pulse a
second, this a power density of 66 W/kg. One hundred pulses mean 6.6 kW/kg. One
thousand pulses, or quasi-continuous operation, would yield 66 kW per kg. The only limit to the reactor-laser’s power density is heat
build-up. At 5% efficiency, there is nineteen times more waste heat than laser
power leaving the reactor. We expect that using the UV mirrors from the previous design could drastically improve this figure by recycling light that was not absorbed by the lasing medium in the first pass through. Thankfully, the 1000K temperature allows for some
pretty effective management of waste heat. Carbon fiber panels of 1mm thickness, operating at 1000K
would handle 56.7 kW/kg. It would give the reactor a maximum power density of
2.4 kW/kg, including a 15% penalty for other equipment. If the reactor can operate closer to the melting point of its
beryllium moderator, perhaps 1400K, then it can increase its power density to
8.3 kW/kg. Conclusion Reactor lasers, when
designed appropriately, allow for high powered lasers from lightweight devices.
We have multiple examples of designs, either from references or calculated,
that output several kW of laser power per kg. The primary
limitations of many of the designs can be adjusted in ways that drastically
improve performance. The assumptions made (for instance, 1 cm thick carbon
emitter or flat panel radiators) are solely for the sake of easy comparison. It
is entirely acceptable to use 1mm thick emitting surfaces or one of the
alternate heat radiator designs mentioned in this
previous blog post .
Even better, many of the lower temperature lasers can have their waste heat
raised to a higher temperature using a heat pump. Smaller and lighter radiators
can then be used for a small penalty in overall efficiency to power the heat
pumps. Most of the lasers
discussed have rather long wavelengths. This is not great for use in space, as
the distances the beam has to traverse are huge and it multiplies the size of
the focusing optics required. For this reason, a method of shortening the
wavelengths, perhaps using frequency doubling, is recommended. Halving the
wavelength doubles the effective range. However, there is a 20-30% efficiency
penalty for using frequency doubling. Conversely, lasers which produce short
wavelength beams have a great advantage. The list of laser
options for each type of pumping is also by no means exhaustive. There might be
options not considered here that would allow for much greater performance… but
research on such options is very limited. For example, blackbody and LED
pumping seems to be a ‘dead’ field of research, now that diodes can produce a
single wavelength of the desired power. Up-to-date performance of those options
is therefore non-existent and so we cannot fairly compare their performance to
lasers which have been developed in their stead. It should be pointed out that a direct comparison between
reactor and electric lasers is not the whole story. Reactor lasers can easily
be converted into dual-mode use, where 100% of their heat is used for
propulsion purposes. A spaceship with an electric laser can only a fraction of
their output in an electric rocket. For example, the 4200K laser can have a
performance close to the liquid-core rocket design it was derived from. Other,
like the aerosol fluorescer laser, can both create a beam and heat propellant
at the same time. A nuclear-electric system must choose where to send its
electrical output and must accept the 60% reduction in overall power due to the
conversion steps between heat and electricity at all times. Finally, certain reactor lasers have hidden strength when
facing hostile forces. Mirrors work both ways. The same optics and mirrors that
transport your laser beam from the lasing medium out into space and to an enemy
target can be exploited by an enemy to get their beam to travel down the optics
and mirrors and reach your lasing medium. The lasing medium, assumed to be diodes or other
semiconductor lasers, has to operate at relatively low temperatures and so it
will melt and be destroyed under the focused glare of the enemy beam. Tactics around using lasers and counter-lasers, something
called ‘ eyeball-frying
contests ’ can sometimes lead to a large and powerful
warship being brought to a stalemate by a small counter-laser. A nuclear reactor laser’s lasing medium can be hot gas or
fissioning fuel. They are pretty much immune to the extra heat from an enemy
beam. It would render them much more resistant to ‘eye-frying’ tactics. This, and many other
strengths and consequences, become available to you if you include nuclear
reactor lasers in your science fiction. PS: I must apologize for using many sources that can only be fully accessed through a paywall. It was a necessity when researching this topic, on which little detail is available to the public. For this same reason, illustrations had to be derived from documents I cannot directly link to, but they are all referenced in links in this post."
BentoML: MLOps for Beginners,https://www.kdnuggets.com/bentoml-mlops-for-beginners,KDNuggets,2025-02-28T13:00:31,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Learn how to build, test, deploy, and monitor machine learning models in the cloud with the BentoML ecosystem. Image by Author As a data scientist, have you ever found yourself bogged down by DevOps tasks like creating Docker containers, learning Kubernetes, or managing cloud deployments? These challenges can feel overwhelming, especially for beginners in MLOps. That’s where BentoML comes in. BentoMLis a powerful yet beginner-friendly tool that simplifies MLOps workflows. It allows you to build model endpoints, create Docker images, and deploy models to the cloud—all with just a few CLI commands. No need to dive deep into complex DevOps processes; BentoML handles it for you, making it an ideal choice for those new to MLOps. In this tutorial, we will explore BentoML by building a Text-to-Speech application, deploying it to BentoCloud, testing model inference, and monitoring its performance.  BentoML is an open-source framework designed for model serving and deployment. It automates key tasks such as creating Docker images, setting up infrastructure and environment, scaling your applications on demand, and adding secure endpoints so the people who access them require API keys. This allows data scientists to quickly build production-ready AI systems with limited knowledge about what is going on behind the scenes. BentoML is not just a tool. It is an ecosystem that comes with BentoCloud, OpenLLM, OIC Image Builder, VLLM, and many more integrations.  We will set up the project first by installing the BentoML Python package using the PIP command.  After that, we will create the `app.py` file, which will contain all the code for model serving. We are building a text-to-speech (TTS) service for deployment using the Bark model via BentoML. app.py:  We will now create a `bentofile.yaml file that includes all the commands for creating the infrastructure and environment. bentofile.yaml:  The requirements.txt file lists all the Python packages needed to create the environment for the cloud. requirements.txt:  To deploy this application in the cloud, we will log in to BentoCloud using the CLI command. It will redirect you to create the account and API key.  Then, type the following command in the terminal to deploy your text-to-speech application.  It will push the Docker image and then containerize the application. After that, it will download the model and initiate the AI service.  You can go directly to your BentoCloud dashboard to see the deployment status.  You can also use the Events tab to check the deployment status. Our service is successfully running.   We will test our service using the Playground provided by BentoCloud. Just type the text and click on the Submit button. It will generate the WAV file containing the audio within a few seconds.  You can also access the API endpoint from your terminal using the CURL command.  We successfully created the mp3 file using the text provided, and it sounds perfect.    The best part of BentoCloud is that you don't have to set up monitoring services like Prometheus and Grafana. Simply go to the Monitoring tab and scroll down to view all kinds of metrics related to the model, machine, and model performance.    I am absolutely in love with the BentoML ecosystem. It provides a simple and efficient solution to most of my challenges. What makes it even more impressive is that I don’t need to learn complex concepts like cloud computing or Kubernetes to deploy a fully functional AI application. All it takes is writing a few lines of code and running a single CLI command to deploy the AI service seamlessly. If you are having trouble running or deploying the TTS service, here is the GitHub repositorykingabzpro/TTS-BentoMLto help you. All you have to do is clone the repository and run the command. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Learn how to build, test, deploy, and monitor machine learning models in the cloud with the BentoML ecosystem. Image by Author As a data scientist, have you ever found yourself bogged down by DevOps tasks like creating Docker containers, learning Kubernetes, or managing cloud deployments? These challenges can feel overwhelming, especially for beginners in MLOps. That’s where BentoML comes in. BentoMLis a powerful yet beginner-friendly tool that simplifies MLOps workflows. It allows you to build model endpoints, create Docker images, and deploy models to the cloud—all with just a few CLI commands. No need to dive deep into complex DevOps processes; BentoML handles it for you, making it an ideal choice for those new to MLOps. In this tutorial, we will explore BentoML by building a Text-to-Speech application, deploying it to BentoCloud, testing model inference, and monitoring its performance.  BentoML is an open-source framework designed for model serving and deployment. It automates key tasks such as creating Docker images, setting up infrastructure and environment, scaling your applications on demand, and adding secure endpoints so the people who access them require API keys. This allows data scientists to quickly build production-ready AI systems with limited knowledge about what is going on behind the scenes. BentoML is not just a tool. It is an ecosystem that comes with BentoCloud, OpenLLM, OIC Image Builder, VLLM, and many more integrations.  We will set up the project first by installing the BentoML Python package using the PIP command.  After that, we will create the `app.py` file, which will contain all the code for model serving. We are building a text-to-speech (TTS) service for deployment using the Bark model via BentoML. app.py:  We will now create a `bentofile.yaml file that includes all the commands for creating the infrastructure and environment. bentofile.yaml:  The requirements.txt file lists all the Python packages needed to create the environment for the cloud. requirements.txt:  To deploy this application in the cloud, we will log in to BentoCloud using the CLI command. It will redirect you to create the account and API key.  Then, type the following command in the terminal to deploy your text-to-speech application.  It will push the Docker image and then containerize the application. After that, it will download the model and initiate the AI service.  You can go directly to your BentoCloud dashboard to see the deployment status.  You can also use the Events tab to check the deployment status. Our service is successfully running.   We will test our service using the Playground provided by BentoCloud. Just type the text and click on the Submit button. It will generate the WAV file containing the audio within a few seconds.  You can also access the API endpoint from your terminal using the CURL command.  We successfully created the mp3 file using the text provided, and it sounds perfect.    The best part of BentoCloud is that you don't have to set up monitoring services like Prometheus and Grafana. Simply go to the Monitoring tab and scroll down to view all kinds of metrics related to the model, machine, and model performance.    I am absolutely in love with the BentoML ecosystem. It provides a simple and efficient solution to most of my challenges. What makes it even more impressive is that I don’t need to learn complex concepts like cloud computing or Kubernetes to deploy a fully functional AI application. All it takes is writing a few lines of code and running a single CLI command to deploy the AI service seamlessly. If you are having trouble running or deploying the TTS service, here is the GitHub repositorykingabzpro/TTS-BentoMLto help you. All you have to do is clone the repository and run the command. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",BentoML: MLOps for Beginners,"

Key Points:
",Data Science,"Learn how to build, test, deploy, and monitor machine learning models in the cloud with the BentoML ecosystem. Image by Author As a data scientist, have you ever found yourself bogged down by DevOps tasks like creating Docker containers, learning Kubernetes, or managing cloud deployments? These challenges can feel overwhelming, especially for beginners in MLOps. That’s where BentoML comes in. BentoMLis a powerful yet beginner-friendly tool that simplifies MLOps workflows. It allows you to build model endpoints, create Docker images, and deploy models to the cloud—all with just a few CLI commands. No need to dive deep into complex DevOps processes; BentoML handles it for you, making it an ideal choice for those new to MLOps. In this tutorial, we will explore BentoML by building a Text-to-Speech application, deploying it to BentoCloud, testing model inference, and monitoring its performance.  BentoML is an open-source framework designed for model serving and deployment. It automates key tasks such as creating Docker images, setting up infrastructure and environment, scaling your applications on demand, and adding secure endpoints so the people who access them require API keys. This allows data scientists to quickly build production-ready AI systems with limited knowledge about what is going on behind the scenes. BentoML is not just a tool. It is an ecosystem that comes with BentoCloud, OpenLLM, OIC Image Builder, VLLM, and many more integrations.  We will set up the project first by installing the BentoML Python package using the PIP command.  After that, we will create the `app.py` file, which will contain all the code for model serving. We are building a text-to-speech (TTS) service for deployment using the Bark model via BentoML. app.py:  We will now create a `bentofile.yaml file that includes all the commands for creating the infrastructure and environment. bentofile.yaml:  The requirements.txt file lists all the Python packages needed to create the environment for the cloud. requirements.txt:  To deploy this application in the cloud, we will log in to BentoCloud using the CLI command. It will redirect you to create the account and API key.  Then, type the following command in the terminal to deploy your text-to-speech application.  It will push the Docker image and then containerize the application. After that, it will download the model and initiate the AI service.  You can go directly to your BentoCloud dashboard to see the deployment status.  You can also use the Events tab to check the deployment status. Our service is successfully running.   We will test our service using the Playground provided by BentoCloud. Just type the text and click on the Submit button. It will generate the WAV file containing the audio within a few seconds.  You can also access the API endpoint from your terminal using the CURL command.  We successfully created the mp3 file using the text provided, and it sounds perfect.    The best part of BentoCloud is that you don't have to set up monitoring services like Prometheus and Grafana. Simply go to the Monitoring tab and scroll down to view all kinds of metrics related to the model, machine, and model performance.    I am absolutely in love with the BentoML ecosystem. It provides a simple and efficient solution to most of my challenges. What makes it even more impressive is that I don’t need to learn complex concepts like cloud computing or Kubernetes to deploy a fully functional AI application. All it takes is writing a few lines of code and running a single CLI command to deploy the AI service seamlessly. If you are having trouble running or deploying the TTS service, here is the GitHub repositorykingabzpro/TTS-BentoMLto help you. All you have to do is clone the repository and run the command. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
11 Python Libraries Every AI Engineer Should Know,https://www.kdnuggets.com/11-python-libraries-every-ai-engineer-should-know,KDNuggets,2025-02-27T15:00:35,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Looking to build your AI engineer toolkit in 2025? Here are Python libraries and frameworks you can’t miss! Image by Author | Canva With LLMs and generative AI going mainstream, AI engineering is becoming all the more relevant. And so is the role of the AI engineer. So what do you need to build useful AI applications? Well, you need a toolkit that spans model interaction, orchestration, data management, and more. In this article, we’ll go over Python libraries and framework you’ll need in your AI engineering toolkit, covering the following: Let’s get started.  What it’s for:Hugging Face Transformerslibrary is the swiss army knife for working with pre-trained models and NLP tasks. It is a comprehensive NLP toolkit that democratizes access to transformer models. It is a unified platform for downloading, using, and fine-tuning pre-trained models and makes state-of-the-art NLP accessible to developers without requiring deep ML expertise. Key Features Learning Resource:Hugging Face NLP Course  What it’s for:Ollamais a framework for running and managing open-source LLMs locally. It simplifies the process of running models like Llama and Mistral on your own hardware, handling the complexity of model quantization and deployment. Key Features Learning Resource:Ollama Course – Build AI Apps Locally  What it’s for: TheOpenAI Python SDKis the official toolkit for integrating OpenAI's language models into Python applications. It provides a programmatic interface to interact with GPT models, handling all the underlying API communication and token management complexities. Key Features Learning Resource:The official developer quickstart guide  What it’s for: TheAnthropic Python SDKis a specialized client library for integration with Claude and other Anthropic models. It provides a clean interface for chat-based applications and complex completions, with built-in support for streaming and system prompts. Key Features Learning Resource:Anthropic Python SDK  What it’s for:LangChainis a framework that helps developers build LLM applications. It provides abstractions and tools to combine LLMs with other sources of computation or knowledge. Key Features Learning Resource:LangChain for LLM Application Development - DeepLearning.AI  What it’s for:LlamaIndexis a framework specifically designed to help developers connect custom data with LLMs. It provides the infrastructure for ingesting, structuring, and accessing private or domain-specific data in LLM applications. Key Features Learning Resource:Building Agentic RAG with LlamaIndex - DeepLearning.AI  What it’s for:SQLAlchemyis a SQL toolkit and ORM (Object Relational Mapper) for Python. It abstracts database operations into Python code, making database interactions more pythonic and maintainable. Key Features Learning Resource:SQLAlchemy Unified Tutorial  What it’s for:ChromaDBis an open-source embeddings database for AI applications. It provides efficient storage and retrieval of vector embeddings. Great for semantic search and AI-powered information retrieval systems. Key Features Learning Resource:Getting Started - Chroma Docs  What it’s for:Weaviateis a cloud-native vector search engine that enables semantic search across multiple data types. It's designed to handle large-scale vector operations efficiently while providing rich querying capabilities through GraphQL. You can use thePython client library Weaviate Key Features Learning Resource:101T Work with: Text data | Weaviate,101V Work with: Your own vectors | Weaviate  What it’s for:Weights & Biasesis an ML experiment tracking and model monitoring platform. It helps teams monitor, compare, and improve machine learning models by providing comprehensive logging and visualization capabilities. Key Features  Learning Resource:Effective MLOps: Model Development  What it’s for:LangSmithis a production monitoring and evaluation platform for LLM applications. It provides insights into LLM interactions, helping you understand, debug, and optimize LLM-powered applications in production. Key Features Learning Resource:Introduction to LangSmith  That’s all for now. You can think of this collection as a toolkit for modern AI engineering. You can start building production-grade LLM applications and use these as needed. The most effective engineers understand not just individual libraries, but how to use them to solve relevant problems. We encourage you to experiment with these tools. There may be changes, new frameworks may become popular. But the fundamental patterns these libraries address will remain relevant. As you continue developing AI applications, however, remember that ongoing learning and community engagement are super important, too. Happy coding and learning! Bala Priya Cis a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews and coding tutorials. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Looking to build your AI engineer toolkit in 2025? Here are Python libraries and frameworks you can’t miss! Image by Author | Canva With LLMs and generative AI going mainstream, AI engineering is becoming all the more relevant. And so is the role of the AI engineer. So what do you need to build useful AI applications? Well, you need a toolkit that spans model interaction, orchestration, data management, and more. In this article, we’ll go over Python libraries and framework you’ll need in your AI engineering toolkit, covering the following: Let’s get started.  What it’s for:Hugging Face Transformerslibrary is the swiss army knife for working with pre-trained models and NLP tasks. It is a comprehensive NLP toolkit that democratizes access to transformer models. It is a unified platform for downloading, using, and fine-tuning pre-trained models and makes state-of-the-art NLP accessible to developers without requiring deep ML expertise. Key Features Learning Resource:Hugging Face NLP Course  What it’s for:Ollamais a framework for running and managing open-source LLMs locally. It simplifies the process of running models like Llama and Mistral on your own hardware, handling the complexity of model quantization and deployment. Key Features Learning Resource:Ollama Course – Build AI Apps Locally  What it’s for: TheOpenAI Python SDKis the official toolkit for integrating OpenAI's language models into Python applications. It provides a programmatic interface to interact with GPT models, handling all the underlying API communication and token management complexities. Key Features Learning Resource:The official developer quickstart guide  What it’s for: TheAnthropic Python SDKis a specialized client library for integration with Claude and other Anthropic models. It provides a clean interface for chat-based applications and complex completions, with built-in support for streaming and system prompts. Key Features Learning Resource:Anthropic Python SDK  What it’s for:LangChainis a framework that helps developers build LLM applications. It provides abstractions and tools to combine LLMs with other sources of computation or knowledge. Key Features Learning Resource:LangChain for LLM Application Development - DeepLearning.AI  What it’s for:LlamaIndexis a framework specifically designed to help developers connect custom data with LLMs. It provides the infrastructure for ingesting, structuring, and accessing private or domain-specific data in LLM applications. Key Features Learning Resource:Building Agentic RAG with LlamaIndex - DeepLearning.AI  What it’s for:SQLAlchemyis a SQL toolkit and ORM (Object Relational Mapper) for Python. It abstracts database operations into Python code, making database interactions more pythonic and maintainable. Key Features Learning Resource:SQLAlchemy Unified Tutorial  What it’s for:ChromaDBis an open-source embeddings database for AI applications. It provides efficient storage and retrieval of vector embeddings. Great for semantic search and AI-powered information retrieval systems. Key Features Learning Resource:Getting Started - Chroma Docs  What it’s for:Weaviateis a cloud-native vector search engine that enables semantic search across multiple data types. It's designed to handle large-scale vector operations efficiently while providing rich querying capabilities through GraphQL. You can use thePython client library Weaviate Key Features Learning Resource:101T Work with: Text data | Weaviate,101V Work with: Your own vectors | Weaviate  What it’s for:Weights & Biasesis an ML experiment tracking and model monitoring platform. It helps teams monitor, compare, and improve machine learning models by providing comprehensive logging and visualization capabilities. Key Features  Learning Resource:Effective MLOps: Model Development  What it’s for:LangSmithis a production monitoring and evaluation platform for LLM applications. It provides insights into LLM interactions, helping you understand, debug, and optimize LLM-powered applications in production. Key Features Learning Resource:Introduction to LangSmith  That’s all for now. You can think of this collection as a toolkit for modern AI engineering. You can start building production-grade LLM applications and use these as needed. The most effective engineers understand not just individual libraries, but how to use them to solve relevant problems. We encourage you to experiment with these tools. There may be changes, new frameworks may become popular. But the fundamental patterns these libraries address will remain relevant. As you continue developing AI applications, however, remember that ongoing learning and community engagement are super important, too. Happy coding and learning! Bala Priya Cis a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews and coding tutorials. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",11 Python Libraries Every AI Engineer Should Know,"

Key Points:
",Data Science,"Looking to build your AI engineer toolkit in 2025? Here are Python libraries and frameworks you can’t miss! Image by Author | Canva With LLMs and generative AI going mainstream, AI engineering is becoming all the more relevant. And so is the role of the AI engineer. So what do you need to build useful AI applications? Well, you need a toolkit that spans model interaction, orchestration, data management, and more. In this article, we’ll go over Python libraries and framework you’ll need in your AI engineering toolkit, covering the following: Let’s get started.  What it’s for:Hugging Face Transformerslibrary is the swiss army knife for working with pre-trained models and NLP tasks. It is a comprehensive NLP toolkit that democratizes access to transformer models. It is a unified platform for downloading, using, and fine-tuning pre-trained models and makes state-of-the-art NLP accessible to developers without requiring deep ML expertise. Key Features Learning Resource:Hugging Face NLP Course  What it’s for:Ollamais a framework for running and managing open-source LLMs locally. It simplifies the process of running models like Llama and Mistral on your own hardware, handling the complexity of model quantization and deployment. Key Features Learning Resource:Ollama Course – Build AI Apps Locally  What it’s for: TheOpenAI Python SDKis the official toolkit for integrating OpenAI's language models into Python applications. It provides a programmatic interface to interact with GPT models, handling all the underlying API communication and token management complexities. Key Features Learning Resource:The official developer quickstart guide  What it’s for: TheAnthropic Python SDKis a specialized client library for integration with Claude and other Anthropic models. It provides a clean interface for chat-based applications and complex completions, with built-in support for streaming and system prompts. Key Features Learning Resource:Anthropic Python SDK  What it’s for:LangChainis a framework that helps developers build LLM applications. It provides abstractions and tools to combine LLMs with other sources of computation or knowledge. Key Features Learning Resource:LangChain for LLM Application Development - DeepLearning.AI  What it’s for:LlamaIndexis a framework specifically designed to help developers connect custom data with LLMs. It provides the infrastructure for ingesting, structuring, and accessing private or domain-specific data in LLM applications. Key Features Learning Resource:Building Agentic RAG with LlamaIndex - DeepLearning.AI  What it’s for:SQLAlchemyis a SQL toolkit and ORM (Object Relational Mapper) for Python. It abstracts database operations into Python code, making database interactions more pythonic and maintainable. Key Features Learning Resource:SQLAlchemy Unified Tutorial  What it’s for:ChromaDBis an open-source embeddings database for AI applications. It provides efficient storage and retrieval of vector embeddings. Great for semantic search and AI-powered information retrieval systems. Key Features Learning Resource:Getting Started - Chroma Docs  What it’s for:Weaviateis a cloud-native vector search engine that enables semantic search across multiple data types. It's designed to handle large-scale vector operations efficiently while providing rich querying capabilities through GraphQL. You can use thePython client library Weaviate Key Features Learning Resource:101T Work with: Text data | Weaviate,101V Work with: Your own vectors | Weaviate  What it’s for:Weights & Biasesis an ML experiment tracking and model monitoring platform. It helps teams monitor, compare, and improve machine learning models by providing comprehensive logging and visualization capabilities. Key Features  Learning Resource:Effective MLOps: Model Development  What it’s for:LangSmithis a production monitoring and evaluation platform for LLM applications. It provides insights into LLM interactions, helping you understand, debug, and optimize LLM-powered applications in production. Key Features Learning Resource:Introduction to LangSmith  That’s all for now. You can think of this collection as a toolkit for modern AI engineering. You can start building production-grade LLM applications and use these as needed. The most effective engineers understand not just individual libraries, but how to use them to solve relevant problems. We encourage you to experiment with these tools. There may be changes, new frameworks may become popular. But the fundamental patterns these libraries address will remain relevant. As you continue developing AI applications, however, remember that ongoing learning and community engagement are super important, too. Happy coding and learning! Bala Priya Cis a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more. Bala also creates engaging resource overviews and coding tutorials. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
DeepSeek-Level AI? Train Your Own Reasoning Model in Just 7 Easy Steps!,https://www.kdnuggets.com/deepseek-level-ai-train-your-own-reasoning-model-in-just-7-easy-steps,KDNuggets,2025-02-27T13:15:21,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Who needs a supercomputer? Train your own powerful AI reasoning model with just 15GB VRAM! Image by Author | Canva DeepSeek’s R1 model has disrupted the LLM landscape by enabling more thoughtful reasoning without requiring human feedback. The key behind this breakthrough isGroup Relative Policy Optimization (GRPO)—a reinforcement learning technique that helps models develop reasoning capabilities autonomously. Unlike Proximal Policy Optimization (PPO), which relies on a value function, GRPO optimizes responses without requiring one, making it more efficient. The race to develop better reasoning models is in full swing. But what about those of us withlimited GPU resources? Thanks toUnsloth, training a 15B parameter model on consumer-grade GPUs with just 15GB VRAM is now possible. This guide will show you how to train your own reasoning-focused model using GRPO in a few steps.  GRPO helps AI models learn to think better by comparing their answers. Here’s how it works: For example, to teach math: GRPO rewards the correct answer, so the model learns to avoid mistakes. This technique allows models to develop structured reasoning without requiring massive labeled datasets.  This guide walks through training a reasoning-optimized LLM using GRPO and deploying it on Hugging Face. We will be usingmeta-llama/meta-Llama-3.1-8B-Instructfor this article and the reference notebook provided by unsloth that you can accesshere. Install Dependencies using the following code: Key Components:  UsePatchFastRLbefore all functions to patch GRPO and other RL algorithms. This step ensures that the model is optimized for RL tasks by integrating specific algorithm improvements intoFastLanguageModel. Then load upLlama 3.1 8B Instructwith following parameters and apply lora adaptation. Key Parameters:   In this step, we prepare a dataset that trains our model to reason step-by-step before producing an answer. The dataset format is important, as it influences how the model structures its responses. The base notebookoriginally uses GSM8K (Grade School Math 8K), a dataset of 8.5K grade school math word problems requiring multi-step reasoning. However, we will be using a different dataset that provides a broader reasoning coverage across multiple domains that you can find here -KingNish/reasoning-base-20k. Data Fields: We format the dataset using astructured response templateto ensure our model learns to separate reasoning from the final answer. Now, load theReasoning Base 20K dataset.   Reward functions are crucial in training a reasoning-optimized model as they guide the model what “good” performance means. The right reward design ensures that the model generates logically sound, well-formatted, and high-quality responses. Our dataset requires a different approach than GSM8K, as our responses contain detailed reasoning steps rather than just a numeric answer. Hence, our reward function evaluates multiple aspects: In the sample code below, you will find several reward functions—each focuses on a different aspect of the response. Below is a closer look at these functions:  This function measures how well the model’s response covers key terms in both the question prompt and a reference answer (if available). This ensures that the model at least mentions or addresses critical topics from the question.  This function ensures that the output strictly follows the required XML-style structure to maintain consistent output formatting for structured reasoning. Rewards 0.5 if the format is correct, else 0.0.  A more flexible reward function that allows minor deviations but still requires proper XML-style formatting. Also awards 0.5 points if matched, else 0.0. This can be helpful if the strict format is too rigid and might penalize small differences that do not affect usability.  This function evaluates how well the response adheres to expected XML structure by counting required tags. It penalizes if extra content appears afterand provides partial credit instead of binary rewards. In practice, you often want to combine some or all of these different signals for the final reward score calculation. The original notebook employed int and correctness reward functions, as the dataset contained single numerical answers. However, given our general reasoning model, a broader evaluation approach is necessary. Hence, we used the following reward functions:  Now, set up the GRPO Trainer and all configurations. I have reducedmax_stepsfrom 250 to 150 to save time and decreasednum_generationsfrom 6 to 4 to conserve memory. However, Unsloth recommends running for at least 300 steps to observe significant improvement. All other configurations remain the same and are as follows: Now, let's initialize and run the GRPO Trainer: The training logs provide insights into reward trends, loss values, and response quality improvements. Initially, rewards fluctuate due to random exploration, but they gradually improve over time. It took me approximately2 hours and 7 minutesto run this notebook on a Colab T4 GPU, and the finaltraining loss after 150 steps was 0.0003475.  Now that we have trained the model, let's compare the performance of the baseline LLaMA 3.1 8B Instruct with the GRPO-trained model.Before GRPO Training  The baseline model incorrectly identifies the number of 'r's in ""strawberry,"" highlighting a gap in factual reasoning. After GRPO TrainingNow we load the LoRA and test:   After GRPO training, the model shows improved accuracy and reasoning but is still not perfect. Since it was trained for only 2 hours on a T4 GPU, extending the sequence length and training time would further enhance its performance.  Once the model has been fine-tuned and evaluated, the next step is deploying it for real-world use and ensuring it can scale efficiently. Deployment involves converting the model into an optimized format, integrating it into an inference server, and making it accessible through an API or application. To ensure efficient inference, we save thetrained LoRA adapters and push them to Hugging Face Hubfor easy access. This allows others to load the fine-tuned model without needing extensive computational resources.  Saved lora model tohttps://huggingface.co/kanwal-mehreen18/Llama3.1-8B-GRPO.    Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Who needs a supercomputer? Train your own powerful AI reasoning model with just 15GB VRAM! Image by Author | Canva DeepSeek’s R1 model has disrupted the LLM landscape by enabling more thoughtful reasoning without requiring human feedback. The key behind this breakthrough isGroup Relative Policy Optimization (GRPO)—a reinforcement learning technique that helps models develop reasoning capabilities autonomously. Unlike Proximal Policy Optimization (PPO), which relies on a value function, GRPO optimizes responses without requiring one, making it more efficient. The race to develop better reasoning models is in full swing. But what about those of us withlimited GPU resources? Thanks toUnsloth, training a 15B parameter model on consumer-grade GPUs with just 15GB VRAM is now possible. This guide will show you how to train your own reasoning-focused model using GRPO in a few steps.  GRPO helps AI models learn to think better by comparing their answers. Here’s how it works: For example, to teach math: GRPO rewards the correct answer, so the model learns to avoid mistakes. This technique allows models to develop structured reasoning without requiring massive labeled datasets.  This guide walks through training a reasoning-optimized LLM using GRPO and deploying it on Hugging Face. We will be usingmeta-llama/meta-Llama-3.1-8B-Instructfor this article and the reference notebook provided by unsloth that you can accesshere. Install Dependencies using the following code: Key Components:  UsePatchFastRLbefore all functions to patch GRPO and other RL algorithms. This step ensures that the model is optimized for RL tasks by integrating specific algorithm improvements intoFastLanguageModel. Then load upLlama 3.1 8B Instructwith following parameters and apply lora adaptation. Key Parameters:   In this step, we prepare a dataset that trains our model to reason step-by-step before producing an answer. The dataset format is important, as it influences how the model structures its responses. The base notebookoriginally uses GSM8K (Grade School Math 8K), a dataset of 8.5K grade school math word problems requiring multi-step reasoning. However, we will be using a different dataset that provides a broader reasoning coverage across multiple domains that you can find here -KingNish/reasoning-base-20k. Data Fields: We format the dataset using astructured response templateto ensure our model learns to separate reasoning from the final answer. Now, load theReasoning Base 20K dataset.   Reward functions are crucial in training a reasoning-optimized model as they guide the model what “good” performance means. The right reward design ensures that the model generates logically sound, well-formatted, and high-quality responses. Our dataset requires a different approach than GSM8K, as our responses contain detailed reasoning steps rather than just a numeric answer. Hence, our reward function evaluates multiple aspects: In the sample code below, you will find several reward functions—each focuses on a different aspect of the response. Below is a closer look at these functions:  This function measures how well the model’s response covers key terms in both the question prompt and a reference answer (if available). This ensures that the model at least mentions or addresses critical topics from the question.  This function ensures that the output strictly follows the required XML-style structure to maintain consistent output formatting for structured reasoning. Rewards 0.5 if the format is correct, else 0.0.  A more flexible reward function that allows minor deviations but still requires proper XML-style formatting. Also awards 0.5 points if matched, else 0.0. This can be helpful if the strict format is too rigid and might penalize small differences that do not affect usability.  This function evaluates how well the response adheres to expected XML structure by counting required tags. It penalizes if extra content appears afterand provides partial credit instead of binary rewards. In practice, you often want to combine some or all of these different signals for the final reward score calculation. The original notebook employed int and correctness reward functions, as the dataset contained single numerical answers. However, given our general reasoning model, a broader evaluation approach is necessary. Hence, we used the following reward functions:  Now, set up the GRPO Trainer and all configurations. I have reducedmax_stepsfrom 250 to 150 to save time and decreasednum_generationsfrom 6 to 4 to conserve memory. However, Unsloth recommends running for at least 300 steps to observe significant improvement. All other configurations remain the same and are as follows: Now, let's initialize and run the GRPO Trainer: The training logs provide insights into reward trends, loss values, and response quality improvements. Initially, rewards fluctuate due to random exploration, but they gradually improve over time. It took me approximately2 hours and 7 minutesto run this notebook on a Colab T4 GPU, and the finaltraining loss after 150 steps was 0.0003475.  Now that we have trained the model, let's compare the performance of the baseline LLaMA 3.1 8B Instruct with the GRPO-trained model.Before GRPO Training  The baseline model incorrectly identifies the number of 'r's in ""strawberry,"" highlighting a gap in factual reasoning. After GRPO TrainingNow we load the LoRA and test:   After GRPO training, the model shows improved accuracy and reasoning but is still not perfect. Since it was trained for only 2 hours on a T4 GPU, extending the sequence length and training time would further enhance its performance.  Once the model has been fine-tuned and evaluated, the next step is deploying it for real-world use and ensuring it can scale efficiently. Deployment involves converting the model into an optimized format, integrating it into an inference server, and making it accessible through an API or application. To ensure efficient inference, we save thetrained LoRA adapters and push them to Hugging Face Hubfor easy access. This allows others to load the fine-tuned model without needing extensive computational resources.  Saved lora model tohttps://huggingface.co/kanwal-mehreen18/Llama3.1-8B-GRPO.    Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",DeepSeek-Level AI? Train Your Own Reasoning Model in Just 7 Easy Steps!,"

Key Points:
",Data Science,"Who needs a supercomputer? Train your own powerful AI reasoning model with just 15GB VRAM! Image by Author | Canva DeepSeek’s R1 model has disrupted the LLM landscape by enabling more thoughtful reasoning without requiring human feedback. The key behind this breakthrough isGroup Relative Policy Optimization (GRPO)—a reinforcement learning technique that helps models develop reasoning capabilities autonomously. Unlike Proximal Policy Optimization (PPO), which relies on a value function, GRPO optimizes responses without requiring one, making it more efficient. The race to develop better reasoning models is in full swing. But what about those of us withlimited GPU resources? Thanks toUnsloth, training a 15B parameter model on consumer-grade GPUs with just 15GB VRAM is now possible. This guide will show you how to train your own reasoning-focused model using GRPO in a few steps.  GRPO helps AI models learn to think better by comparing their answers. Here’s how it works: For example, to teach math: GRPO rewards the correct answer, so the model learns to avoid mistakes. This technique allows models to develop structured reasoning without requiring massive labeled datasets.  This guide walks through training a reasoning-optimized LLM using GRPO and deploying it on Hugging Face. We will be usingmeta-llama/meta-Llama-3.1-8B-Instructfor this article and the reference notebook provided by unsloth that you can accesshere. Install Dependencies using the following code: Key Components:  UsePatchFastRLbefore all functions to patch GRPO and other RL algorithms. This step ensures that the model is optimized for RL tasks by integrating specific algorithm improvements intoFastLanguageModel. Then load upLlama 3.1 8B Instructwith following parameters and apply lora adaptation. Key Parameters:   In this step, we prepare a dataset that trains our model to reason step-by-step before producing an answer. The dataset format is important, as it influences how the model structures its responses. The base notebookoriginally uses GSM8K (Grade School Math 8K), a dataset of 8.5K grade school math word problems requiring multi-step reasoning. However, we will be using a different dataset that provides a broader reasoning coverage across multiple domains that you can find here -KingNish/reasoning-base-20k. Data Fields: We format the dataset using astructured response templateto ensure our model learns to separate reasoning from the final answer. Now, load theReasoning Base 20K dataset.   Reward functions are crucial in training a reasoning-optimized model as they guide the model what “good” performance means. The right reward design ensures that the model generates logically sound, well-formatted, and high-quality responses. Our dataset requires a different approach than GSM8K, as our responses contain detailed reasoning steps rather than just a numeric answer. Hence, our reward function evaluates multiple aspects: In the sample code below, you will find several reward functions—each focuses on a different aspect of the response. Below is a closer look at these functions:  This function measures how well the model’s response covers key terms in both the question prompt and a reference answer (if available). This ensures that the model at least mentions or addresses critical topics from the question.  This function ensures that the output strictly follows the required XML-style structure to maintain consistent output formatting for structured reasoning. Rewards 0.5 if the format is correct, else 0.0.  A more flexible reward function that allows minor deviations but still requires proper XML-style formatting. Also awards 0.5 points if matched, else 0.0. This can be helpful if the strict format is too rigid and might penalize small differences that do not affect usability.  This function evaluates how well the response adheres to expected XML structure by counting required tags. It penalizes if extra content appears afterand provides partial credit instead of binary rewards. In practice, you often want to combine some or all of these different signals for the final reward score calculation. The original notebook employed int and correctness reward functions, as the dataset contained single numerical answers. However, given our general reasoning model, a broader evaluation approach is necessary. Hence, we used the following reward functions:  Now, set up the GRPO Trainer and all configurations. I have reducedmax_stepsfrom 250 to 150 to save time and decreasednum_generationsfrom 6 to 4 to conserve memory. However, Unsloth recommends running for at least 300 steps to observe significant improvement. All other configurations remain the same and are as follows: Now, let's initialize and run the GRPO Trainer: The training logs provide insights into reward trends, loss values, and response quality improvements. Initially, rewards fluctuate due to random exploration, but they gradually improve over time. It took me approximately2 hours and 7 minutesto run this notebook on a Colab T4 GPU, and the finaltraining loss after 150 steps was 0.0003475.  Now that we have trained the model, let's compare the performance of the baseline LLaMA 3.1 8B Instruct with the GRPO-trained model.Before GRPO Training  The baseline model incorrectly identifies the number of 'r's in ""strawberry,"" highlighting a gap in factual reasoning. After GRPO TrainingNow we load the LoRA and test:   After GRPO training, the model shows improved accuracy and reasoning but is still not perfect. Since it was trained for only 2 hours on a T4 GPU, extending the sequence length and training time would further enhance its performance.  Once the model has been fine-tuned and evaluated, the next step is deploying it for real-world use and ensuring it can scale efficiently. Deployment involves converting the model into an optimized format, integrating it into an inference server, and making it accessible through an API or application. To ensure efficient inference, we save thetrained LoRA adapters and push them to Hugging Face Hubfor easy access. This allows others to load the fine-tuned model without needing extensive computational resources.  Saved lora model tohttps://huggingface.co/kanwal-mehreen18/Llama3.1-8B-GRPO.    Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
OpenHands: Open Source AI Software Developer,https://www.kdnuggets.com/openhands-open-source-ai-software-developer,KDNuggets,2025-02-26T17:00:48,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Build, test, and deploy a complete application in minutes — just by chatting with OpenHands. Image by Author Have you heard aboutDevin, which claims it can replace software engineers with an AI system for just $500 a month? There has been a lot of hype surrounding the idea that AI will soon replace software engineers, enabling them to build, test, and deploy applications in minutes with minimal supervision. There's also a tool called ""OpenHands,"" which is essentially an open-source version of Devin and doesn't cost a fortune. All you need to do is connect to Anthropic or OpenAI to access the state-of-the-art model; the rest will be handled by the OpenHands application. In this tutorial, we will learn about OpenHands, how to install it locally, and how to use it.  OpenHands, formerly known as OpenDevin, is an advanced open-source platform for AI-powered software development that has evolved significantly since its inception. It is designed to create and deploy generalist AI agents capable of performing tasks akin to human developers, including modifying code, running commands, browsing the web, and calling APIs. OpenHands has demonstrated impressive capabilities, solving over 50% of real GitHub issues in software engineering benchmarks, which underscores its practical applicability in addressing real-world coding challenges. The platform supports various large language models (LLMs) and provides a flexible, sandboxed environment for developing and deploying AI agents.  To run the application locally, you need to installDocker Desktop, and additionally, for Windows, you must installWSL. Run the following commands in the terminal. It will pull the OpenHands Docker image and run it locally with all the necessary configurations.   After that, copy the URL `http://localhost:3000/` and paste it into your browser to access the OpenHands user interface. To set up the AI provider configuration, you can use any model for OpenHands; however, the community recommends the Anthropic 3.5 Opus model. I don't have access to that model, so I will be using the next available option, GPT-4o, by providing the API key.    After setting everything up, you will be directed to the main screen, which features a text box for you to write your prompt and request OpenHands to build your application.  Set up GitHub integration to easily create and push changes to the repository.  Here is the prompt I have provided and asked it to build a to-do list app using FastAPI and Jinja.  Prompt:“I want to create a FastAPI and Jinja app that allows me to: * See all the items on my todo list* add a new item to the list* mark an item as done* totally remove an item from the list* change the text of an item* set a due date on the item This should be a client-only app with no backend. The list should persist in localStorage. Please add tests for all of the above and make sure they pass”  OpenHands has begun creating folders and files, adding the necessary code. It has even tested the code by running the command in the terminal.  Next, it initialized the Git repository and tried to push it to GitHub and failed.  To resolve this issue, we will set the remote repository URL and ask it to try again.  As we can see all the necessary files have been pushed to your GitHub repository.  It took a few minutes for it to build, and a simple application.  OpenHands requires state-of-the-art large language models; it cannot function with local models or smaller models that have limited context windows. In this tutorial, we will learn about OpenHands and how to use it locally by connecting it with various LLM providers. OpenHands is an open-source solution that contrasts with Devin's offering of a fully automated AI designed to replace junior software engineers in performing simple tasks. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Build, test, and deploy a complete application in minutes — just by chatting with OpenHands. Image by Author Have you heard aboutDevin, which claims it can replace software engineers with an AI system for just $500 a month? There has been a lot of hype surrounding the idea that AI will soon replace software engineers, enabling them to build, test, and deploy applications in minutes with minimal supervision. There's also a tool called ""OpenHands,"" which is essentially an open-source version of Devin and doesn't cost a fortune. All you need to do is connect to Anthropic or OpenAI to access the state-of-the-art model; the rest will be handled by the OpenHands application. In this tutorial, we will learn about OpenHands, how to install it locally, and how to use it.  OpenHands, formerly known as OpenDevin, is an advanced open-source platform for AI-powered software development that has evolved significantly since its inception. It is designed to create and deploy generalist AI agents capable of performing tasks akin to human developers, including modifying code, running commands, browsing the web, and calling APIs. OpenHands has demonstrated impressive capabilities, solving over 50% of real GitHub issues in software engineering benchmarks, which underscores its practical applicability in addressing real-world coding challenges. The platform supports various large language models (LLMs) and provides a flexible, sandboxed environment for developing and deploying AI agents.  To run the application locally, you need to installDocker Desktop, and additionally, for Windows, you must installWSL. Run the following commands in the terminal. It will pull the OpenHands Docker image and run it locally with all the necessary configurations.   After that, copy the URL `http://localhost:3000/` and paste it into your browser to access the OpenHands user interface. To set up the AI provider configuration, you can use any model for OpenHands; however, the community recommends the Anthropic 3.5 Opus model. I don't have access to that model, so I will be using the next available option, GPT-4o, by providing the API key.    After setting everything up, you will be directed to the main screen, which features a text box for you to write your prompt and request OpenHands to build your application.  Set up GitHub integration to easily create and push changes to the repository.  Here is the prompt I have provided and asked it to build a to-do list app using FastAPI and Jinja.  Prompt:“I want to create a FastAPI and Jinja app that allows me to: * See all the items on my todo list* add a new item to the list* mark an item as done* totally remove an item from the list* change the text of an item* set a due date on the item This should be a client-only app with no backend. The list should persist in localStorage. Please add tests for all of the above and make sure they pass”  OpenHands has begun creating folders and files, adding the necessary code. It has even tested the code by running the command in the terminal.  Next, it initialized the Git repository and tried to push it to GitHub and failed.  To resolve this issue, we will set the remote repository URL and ask it to try again.  As we can see all the necessary files have been pushed to your GitHub repository.  It took a few minutes for it to build, and a simple application.  OpenHands requires state-of-the-art large language models; it cannot function with local models or smaller models that have limited context windows. In this tutorial, we will learn about OpenHands and how to use it locally by connecting it with various LLM providers. OpenHands is an open-source solution that contrasts with Devin's offering of a fully automated AI designed to replace junior software engineers in performing simple tasks. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",OpenHands: Open Source AI Software Developer,"

Key Points:
",Data Science,"Build, test, and deploy a complete application in minutes — just by chatting with OpenHands. Image by Author Have you heard aboutDevin, which claims it can replace software engineers with an AI system for just $500 a month? There has been a lot of hype surrounding the idea that AI will soon replace software engineers, enabling them to build, test, and deploy applications in minutes with minimal supervision. There's also a tool called ""OpenHands,"" which is essentially an open-source version of Devin and doesn't cost a fortune. All you need to do is connect to Anthropic or OpenAI to access the state-of-the-art model; the rest will be handled by the OpenHands application. In this tutorial, we will learn about OpenHands, how to install it locally, and how to use it.  OpenHands, formerly known as OpenDevin, is an advanced open-source platform for AI-powered software development that has evolved significantly since its inception. It is designed to create and deploy generalist AI agents capable of performing tasks akin to human developers, including modifying code, running commands, browsing the web, and calling APIs. OpenHands has demonstrated impressive capabilities, solving over 50% of real GitHub issues in software engineering benchmarks, which underscores its practical applicability in addressing real-world coding challenges. The platform supports various large language models (LLMs) and provides a flexible, sandboxed environment for developing and deploying AI agents.  To run the application locally, you need to installDocker Desktop, and additionally, for Windows, you must installWSL. Run the following commands in the terminal. It will pull the OpenHands Docker image and run it locally with all the necessary configurations.   After that, copy the URL `http://localhost:3000/` and paste it into your browser to access the OpenHands user interface. To set up the AI provider configuration, you can use any model for OpenHands; however, the community recommends the Anthropic 3.5 Opus model. I don't have access to that model, so I will be using the next available option, GPT-4o, by providing the API key.    After setting everything up, you will be directed to the main screen, which features a text box for you to write your prompt and request OpenHands to build your application.  Set up GitHub integration to easily create and push changes to the repository.  Here is the prompt I have provided and asked it to build a to-do list app using FastAPI and Jinja.  Prompt:“I want to create a FastAPI and Jinja app that allows me to: * See all the items on my todo list* add a new item to the list* mark an item as done* totally remove an item from the list* change the text of an item* set a due date on the item This should be a client-only app with no backend. The list should persist in localStorage. Please add tests for all of the above and make sure they pass”  OpenHands has begun creating folders and files, adding the necessary code. It has even tested the code by running the command in the terminal.  Next, it initialized the Git repository and tried to push it to GitHub and failed.  To resolve this issue, we will set the remote repository URL and ask it to try again.  As we can see all the necessary files have been pushed to your GitHub repository.  It took a few minutes for it to build, and a simple application.  OpenHands requires state-of-the-art large language models; it cannot function with local models or smaller models that have limited context windows. In this tutorial, we will learn about OpenHands and how to use it locally by connecting it with various LLM providers. OpenHands is an open-source solution that contrasts with Devin's offering of a fully automated AI designed to replace junior software engineers in performing simple tasks. Abid Ali Awan(@1abidaliawan) is a certified data scientist professional who loves building machine learning models. Currently, he is focusing on content creation and writing technical blogs on machine learning and data science technologies. Abid holds a Master's degree in technology management and a bachelor's degree in telecommunication engineering. His vision is to build an AI product using a graph neural network for students struggling with mental illness. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
7 Best Strategies (Besides Job Portals) to Land Top-Paying Jobs in 2025,https://www.kdnuggets.com/7-best-strategies-besides-job-portals-to-land-top-paying-jobs-in-2025,KDNuggets,2025-02-26T15:39:43,KDNuggets,https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Tired of the job portal grind? Don’t just apply—make them come to you! Check out 7 powerful strategies to land top-paying tech jobs in 2025. Image by Author| Canva Let’s be honest: the tech market is getting crowded.With every passing day, the competition gets fiercer. Recently,Meta CEO Mark Zuckerbergmade headlines by stating that mid-level IT engineers could soon be replaced by AI agents, leaving much of the work to shift towards design-level decision-making and advanced automation. This means the game is changing, and simply being""good enough""isn’t enough anymore. Another challenge I’ve noticed is the overwhelming number of applications companies receive for every job. For example, I recently had to urgently hire someone for a role. To my surprise, I received over2,000 applications. Sitting on the other side of the table for the first time, I realized that even with so many excellent resumes and portfolios, I had to narrow down the pool based on more than just qualifications. I started looking for things like:Do they have a strong social presence? Have they worked on a truly unique project? Did they reach out directly to me or someone in the company for a referral? Have I come across them before in any context? That experience made me realize one thing:applying to job portals isn’t enough in this competitive market. After researching, talking to experts, and reflecting on my own hiring experience, I’ve put together thislist of seven strategies that can help you rise above the noise.You don’t need to try them all—pick one or two where you can truly excel. A single, well-executed effort will always shine brighter than a scattershot approach. I will also be sharing some success stories in between to let you know that these aren’t just fancy terms or something impossible to achieve. So, let's get started.  Sometimes, the simplest way to get noticed is also the most effective—reach out to recruiters directly. Take Andrew Ng's advice:""Don't wait for opportunities to come to you.""Many companies prefer candidates who show initiative and genuine interest in their roles, and reaching out directly can help you stand out from the crowd. It might even help you skip the long queue of traditional applications. You might have heard about cold emailing in academia, but I feel it’s not used effectively in the industrial sector. Cold emailing isn’t just about shooting an email or messaging recruiters on LinkedIn. You need to respect their time and craft a message that grabs their attention.Your first few paragraphs can either make them read further or ignore your email. There’s no one-size-fits-all approach here—it depends on the role, the job description, your experience, and other factors. Use LinkedIn to identify recruiters or hiring managers. If possible, go for a premium version to message them directly. For students who can’t afford LinkedIn Premium, keep an eye out for giveaways or free trials. Example:Dhruv Loya, a recent graduate, landed a job at Tesla through cold emailing. Sharing his story makes it more relatable for people starting their careers. Another famous case isTim Ferriss, author of""The 4-Hour Workweek,""who used a direct, personalized email to reach out to entrepreneurPeter Thiel. That email eventually landed him a position on Thiel's board of advisors. Yes, it takes extra effort, but if the result is landing your dream job, isn’t it worth a shot? Resources:  I genuinely believe networking is one of the most underrated job-hunting tools. If you’re in the tech industry, I’m sure you’ve attended some sort of conference, event, or meetup. But ask yourself:Did you really make the most of the people you were surrounded by during those times?Conferences, workshops, and meetups (e.g., PyCon, NeurIPS, or local AI groups) aren’t just for learning—they’re absolute goldmines for networking. These events often give you access to exclusive, unadvertised roles and opportunities. Personally, I suggest that if you can, secure a speaking spot at these events. Share your project, work, or even conduct a technical workshop. You can start small with local AI workshops in your region to develop a speaking portfolio. If you’re not comfortable with public speaking, at least aim to network with people in the event. Face-to-face meetings allow you to make a memorable impression and engage in meaningful technical discussions. Afterward, be proactive—connect with them on LinkedIn or start a joint project if possible. Don't rush into focusing on monetary benefits at the start of your career.Instead, think about the long-term value of the relationships you build and how they can lead to unexpected opportunities. Example:Ali Ghodsi, CEO of Databricks, often shares how he networked with early adopters of Apache Spark at conferences. This community-building effort helped him secure investors and a strong team, which eventually led to the significant growth of Databricks. Resources:  “Building in public”means sharing your learning journey, projects, or experiments openly on social media or platforms like GitHub, Twitter, or Medium. A few years ago, this term was mainly used by startup founders who shared their journeys—from ideation to execution—in real-time. But now, it's expanded beyond just startups. Now, think about it: when you hear about big names like Meta or OpenAI, who comes to mind? Sam Altman, Mark Zuckerberg, Sundar Pichai? Sure, they’re the faces of these companies, but it’s not just them who make these companies successful. There’s a whole team working behind the scenes, and building in public can help get you noticed—even if you’re not in the spotlight. The real benefit of building in public is the long-term advantages. If you decide to start your own company or launch a product down the road,you’ll already have credibility and an audience that’s excited to support and test what you create.Even if you're just aiming for a top-paying job at a leading tech company, building a visible online presence can put you ahead of other candidates. Here is thesuccess formulathat you can use building in public: Example:TakeChip Huyenas an example. When people think of ML systems or AI engineering, her name is often one of the first that comes up. She started by sharing her insights on ML engineering and is now the founder of Claypot AI. By building in public, she not only established her credibility but also created opportunities that many others wouldn’t get by just sending job applications. Resources:  I’m glad to see that technical blogging and content creation are finally getting the recognition they deserve when it comes to job hunting and building your professional profile.Honestly, I can vouch for how rewarding this can be. I’ve received numerous offers from people who came across my technical articles and wanted to work with me. While it’s true it takes time, the payoff is real. When I first started technical writing, there weren’t many people doing it. But after COVID and especially with the rise of AI tools, the space has become quite crowded. If you want to stand out, you need to find your own unique angle.What’s going to make your content stand out? Why should someone spend their valuable time reading or watching your stuff?High-quality content always rises to the top. So, take a moment to think: What are you good at? What gaps exist in the current content landscape? Where’s there a lack of quality material?Once you figure this out, market and share your articles with the right audience. For technical posts, Reddit is a great platform. If you’re sharing more digestible content like roadmaps, career advice, or beginner-friendly tips, LinkedIn and Twitter are perfect. You could even start a newsletter, but make sure it offers something unique. One of the things I really enjoy about content creation is thatit keeps you updated with the latest trends and developments in your field.It forces you to learn and stay on top of things, which only sharpens your skills further. Example:Umar Jamilcreates some of the most detailed, beginner-friendly video tutorials on machine learning and AI. His content is gold because it goes beyond theory—he codes everything from scratch.  I’d bet my time on watching his videos because they offer something unique and valuable that’s hard to find elsewhere. Similarly, for the NLP folks,Jay Alammar’s visual guide to transformers became a go-to resource for anyone wanting to understand these models. His approach helped fill a huge gap in the community.Rachel Thomas, co-founder of fast.ai, also started out writing detailed explanations of deep learning concepts, and look at where she is now! Recruiters often look for candidates who can break down complex ideas clearly, and blogging is a great way to showcase that ability. Resources:  You’ve probably heard the saying,""Actions speak louder than words,""and it couldn’t be more true when it comes to building practical tools. But before discussing this, let’s clear up an important distinction: the difference between your typical resume projects and building real, usable tools. Resume projects often consist of theoretical work, research-based projects, or solutions that might not be fully deployed or usable. They may involve repetitive ideas that don’t solve real-world problems. In contrast, creating practical tools means you’re tackling tangible, real-world problems and offering solutions that others can use. These tools don’t need to be huge, groundbreaking ideas. They just need to be well-executed solutions for everyday problems.The key here is execution, not the complexity of the idea. And trust me, when done right, these tools serve as a portfolio piece that recruiters can directly interact with. They get to see your skills in action, which is far more powerful than just listing them on a resume. Let’s say you work in AI or software development. Instead of having a project that just sits in a repo gathering dust, create a tool that addresses a problem people actually face. For example, you could build a Chrome extension that helps generate quick email replies with a base template, or a simple tool that automates data cleaning. These kinds of tools show that you can not only come up with solutions but also implement them in a way that people can actually use. When you create these tools, you’re giving your audience something tangible. It’s more than just words on paper. You’re proving you can deliver something functional and useful. Example:Hugging Facestarted as a small side project by Clément Delangue and Julien Chaumond, and look at it now—it's a billion-dollar company revolutionizing NLP. What started as a side project for developers became one of the leading platforms in AI. This shows how even the simplest of tools can make a massive impact. Resources:  If you're in the tech space, you’ve probably heard about open source contributions. But here’s the thing: many people are more focused on the numbers, like how many contributions they can rack up, without really understanding the essence of what open source is all about. It’s disheartening because some folks treat it like a way to boost their GitHub profile, contributing low-effort stuff just for the sake of it. I’m not saying you shouldn’t get started with easier contributions. It's a great way to learn how it works. But once you have a decent understanding, aim for meaningful contributions that actually make a difference. The 80/20 principle applies here—focus on the 20% of contributions that really matter, and trust me, that will have a much bigger impact than dozens of shallow pull requests. If you want a deeper understanding of what I mean, I highly recommend readingEat That Frog. The truth is, open-source contributions aren’t just about showing off coding skills—they’re about demonstrating a commitment to collaboration, problem-solving, and improving the community. Plus, they get you noticed by recruiters. Some people even highlight the number of stars their projects have gotten as a part of their resume. Here’s how you can start contributing in a meaningful way: Example:Sebastian Raschkacontributed to open-source projects early in his career, which helped him build credibility and eventually publish the widely-known book ""Machine Learning with PyTorch and Scikit-Learn"". Similarly,Maxime Labonne's open-source work on LLM post-training gained him recognition throughout the AI community. And then there’sroadmap.shbyKamran Ahmed—which began as a straightforward project to guide developers through various tech career paths.  Today, it’s the 7th most-starred repository on GitHub and the 2nd most-starred codebase overall. Resources:  Many tech geeks prefer working solo, but let me tell you, no matter how skilled you are, this might not be the most helpful route when you’re looking for jobs. And trust me, no one’s going to steal your idea. Don’t let that hold youback—be open to collaboration. Who you collaborate with really depends on your long-term goals—whether you’re leaning toward academia, industry, or independent research.What kind of environment works best for you? For example, if you're more industry-focused, look for mentors or renowned professionals in your sector. Share your project ideas with them. If they can't contribute directly, ask them to mentor you. Their name alone adds credibility, and it will give your project more visibility. Don’t put yourself in a shell—reach out. Let me share a personal story: I wanted to work on a research project focused onAI generated text detection. However, my schedule didn’t allow for formal research work, and I didn’t have the full team. So, I reached out to an independent researcher—someone not hugely famous but with a clear vision and a team that was willing to get things done. We collaborated throughCohere for AI, and guess what? Eight months later, we’re finalizing our draft for the paper. And I loved every minute of it. Example:Many researchers at companies like Google DeepMind and OpenAI started their careers by collaborating with academic mentors. TakeIan Goodfellow, the inventor of GANs. He started his journey by working closely with his professors during his PhD. His groundbreaking work led to a job at Google Brain and, later, Apple. Resources: That’s all for now! If you want to know about thetop high-paying AI skills for 2025, clickhere. And hey, if you’ve tried anything different in your job search that helped you stand out, drop it in the comments. I’d love to check it out! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Tired of the job portal grind? Don’t just apply—make them come to you! Check out 7 powerful strategies to land top-paying tech jobs in 2025. Image by Author| Canva Let’s be honest: the tech market is getting crowded.With every passing day, the competition gets fiercer. Recently,Meta CEO Mark Zuckerbergmade headlines by stating that mid-level IT engineers could soon be replaced by AI agents, leaving much of the work to shift towards design-level decision-making and advanced automation. This means the game is changing, and simply being""good enough""isn’t enough anymore. Another challenge I’ve noticed is the overwhelming number of applications companies receive for every job. For example, I recently had to urgently hire someone for a role. To my surprise, I received over2,000 applications. Sitting on the other side of the table for the first time, I realized that even with so many excellent resumes and portfolios, I had to narrow down the pool based on more than just qualifications. I started looking for things like:Do they have a strong social presence? Have they worked on a truly unique project? Did they reach out directly to me or someone in the company for a referral? Have I come across them before in any context? That experience made me realize one thing:applying to job portals isn’t enough in this competitive market. After researching, talking to experts, and reflecting on my own hiring experience, I’ve put together thislist of seven strategies that can help you rise above the noise.You don’t need to try them all—pick one or two where you can truly excel. A single, well-executed effort will always shine brighter than a scattershot approach. I will also be sharing some success stories in between to let you know that these aren’t just fancy terms or something impossible to achieve. So, let's get started.  Sometimes, the simplest way to get noticed is also the most effective—reach out to recruiters directly. Take Andrew Ng's advice:""Don't wait for opportunities to come to you.""Many companies prefer candidates who show initiative and genuine interest in their roles, and reaching out directly can help you stand out from the crowd. It might even help you skip the long queue of traditional applications. You might have heard about cold emailing in academia, but I feel it’s not used effectively in the industrial sector. Cold emailing isn’t just about shooting an email or messaging recruiters on LinkedIn. You need to respect their time and craft a message that grabs their attention.Your first few paragraphs can either make them read further or ignore your email. There’s no one-size-fits-all approach here—it depends on the role, the job description, your experience, and other factors. Use LinkedIn to identify recruiters or hiring managers. If possible, go for a premium version to message them directly. For students who can’t afford LinkedIn Premium, keep an eye out for giveaways or free trials. Example:Dhruv Loya, a recent graduate, landed a job at Tesla through cold emailing. Sharing his story makes it more relatable for people starting their careers. Another famous case isTim Ferriss, author of""The 4-Hour Workweek,""who used a direct, personalized email to reach out to entrepreneurPeter Thiel. That email eventually landed him a position on Thiel's board of advisors. Yes, it takes extra effort, but if the result is landing your dream job, isn’t it worth a shot? Resources:  I genuinely believe networking is one of the most underrated job-hunting tools. If you’re in the tech industry, I’m sure you’ve attended some sort of conference, event, or meetup. But ask yourself:Did you really make the most of the people you were surrounded by during those times?Conferences, workshops, and meetups (e.g., PyCon, NeurIPS, or local AI groups) aren’t just for learning—they’re absolute goldmines for networking. These events often give you access to exclusive, unadvertised roles and opportunities. Personally, I suggest that if you can, secure a speaking spot at these events. Share your project, work, or even conduct a technical workshop. You can start small with local AI workshops in your region to develop a speaking portfolio. If you’re not comfortable with public speaking, at least aim to network with people in the event. Face-to-face meetings allow you to make a memorable impression and engage in meaningful technical discussions. Afterward, be proactive—connect with them on LinkedIn or start a joint project if possible. Don't rush into focusing on monetary benefits at the start of your career.Instead, think about the long-term value of the relationships you build and how they can lead to unexpected opportunities. Example:Ali Ghodsi, CEO of Databricks, often shares how he networked with early adopters of Apache Spark at conferences. This community-building effort helped him secure investors and a strong team, which eventually led to the significant growth of Databricks. Resources:  “Building in public”means sharing your learning journey, projects, or experiments openly on social media or platforms like GitHub, Twitter, or Medium. A few years ago, this term was mainly used by startup founders who shared their journeys—from ideation to execution—in real-time. But now, it's expanded beyond just startups. Now, think about it: when you hear about big names like Meta or OpenAI, who comes to mind? Sam Altman, Mark Zuckerberg, Sundar Pichai? Sure, they’re the faces of these companies, but it’s not just them who make these companies successful. There’s a whole team working behind the scenes, and building in public can help get you noticed—even if you’re not in the spotlight. The real benefit of building in public is the long-term advantages. If you decide to start your own company or launch a product down the road,you’ll already have credibility and an audience that’s excited to support and test what you create.Even if you're just aiming for a top-paying job at a leading tech company, building a visible online presence can put you ahead of other candidates. Here is thesuccess formulathat you can use building in public: Example:TakeChip Huyenas an example. When people think of ML systems or AI engineering, her name is often one of the first that comes up. She started by sharing her insights on ML engineering and is now the founder of Claypot AI. By building in public, she not only established her credibility but also created opportunities that many others wouldn’t get by just sending job applications. Resources:  I’m glad to see that technical blogging and content creation are finally getting the recognition they deserve when it comes to job hunting and building your professional profile.Honestly, I can vouch for how rewarding this can be. I’ve received numerous offers from people who came across my technical articles and wanted to work with me. While it’s true it takes time, the payoff is real. When I first started technical writing, there weren’t many people doing it. But after COVID and especially with the rise of AI tools, the space has become quite crowded. If you want to stand out, you need to find your own unique angle.What’s going to make your content stand out? Why should someone spend their valuable time reading or watching your stuff?High-quality content always rises to the top. So, take a moment to think: What are you good at? What gaps exist in the current content landscape? Where’s there a lack of quality material?Once you figure this out, market and share your articles with the right audience. For technical posts, Reddit is a great platform. If you’re sharing more digestible content like roadmaps, career advice, or beginner-friendly tips, LinkedIn and Twitter are perfect. You could even start a newsletter, but make sure it offers something unique. One of the things I really enjoy about content creation is thatit keeps you updated with the latest trends and developments in your field.It forces you to learn and stay on top of things, which only sharpens your skills further. Example:Umar Jamilcreates some of the most detailed, beginner-friendly video tutorials on machine learning and AI. His content is gold because it goes beyond theory—he codes everything from scratch.  I’d bet my time on watching his videos because they offer something unique and valuable that’s hard to find elsewhere. Similarly, for the NLP folks,Jay Alammar’s visual guide to transformers became a go-to resource for anyone wanting to understand these models. His approach helped fill a huge gap in the community.Rachel Thomas, co-founder of fast.ai, also started out writing detailed explanations of deep learning concepts, and look at where she is now! Recruiters often look for candidates who can break down complex ideas clearly, and blogging is a great way to showcase that ability. Resources:  You’ve probably heard the saying,""Actions speak louder than words,""and it couldn’t be more true when it comes to building practical tools. But before discussing this, let’s clear up an important distinction: the difference between your typical resume projects and building real, usable tools. Resume projects often consist of theoretical work, research-based projects, or solutions that might not be fully deployed or usable. They may involve repetitive ideas that don’t solve real-world problems. In contrast, creating practical tools means you’re tackling tangible, real-world problems and offering solutions that others can use. These tools don’t need to be huge, groundbreaking ideas. They just need to be well-executed solutions for everyday problems.The key here is execution, not the complexity of the idea. And trust me, when done right, these tools serve as a portfolio piece that recruiters can directly interact with. They get to see your skills in action, which is far more powerful than just listing them on a resume. Let’s say you work in AI or software development. Instead of having a project that just sits in a repo gathering dust, create a tool that addresses a problem people actually face. For example, you could build a Chrome extension that helps generate quick email replies with a base template, or a simple tool that automates data cleaning. These kinds of tools show that you can not only come up with solutions but also implement them in a way that people can actually use. When you create these tools, you’re giving your audience something tangible. It’s more than just words on paper. You’re proving you can deliver something functional and useful. Example:Hugging Facestarted as a small side project by Clément Delangue and Julien Chaumond, and look at it now—it's a billion-dollar company revolutionizing NLP. What started as a side project for developers became one of the leading platforms in AI. This shows how even the simplest of tools can make a massive impact. Resources:  If you're in the tech space, you’ve probably heard about open source contributions. But here’s the thing: many people are more focused on the numbers, like how many contributions they can rack up, without really understanding the essence of what open source is all about. It’s disheartening because some folks treat it like a way to boost their GitHub profile, contributing low-effort stuff just for the sake of it. I’m not saying you shouldn’t get started with easier contributions. It's a great way to learn how it works. But once you have a decent understanding, aim for meaningful contributions that actually make a difference. The 80/20 principle applies here—focus on the 20% of contributions that really matter, and trust me, that will have a much bigger impact than dozens of shallow pull requests. If you want a deeper understanding of what I mean, I highly recommend readingEat That Frog. The truth is, open-source contributions aren’t just about showing off coding skills—they’re about demonstrating a commitment to collaboration, problem-solving, and improving the community. Plus, they get you noticed by recruiters. Some people even highlight the number of stars their projects have gotten as a part of their resume. Here’s how you can start contributing in a meaningful way: Example:Sebastian Raschkacontributed to open-source projects early in his career, which helped him build credibility and eventually publish the widely-known book ""Machine Learning with PyTorch and Scikit-Learn"". Similarly,Maxime Labonne's open-source work on LLM post-training gained him recognition throughout the AI community. And then there’sroadmap.shbyKamran Ahmed—which began as a straightforward project to guide developers through various tech career paths.  Today, it’s the 7th most-starred repository on GitHub and the 2nd most-starred codebase overall. Resources:  Many tech geeks prefer working solo, but let me tell you, no matter how skilled you are, this might not be the most helpful route when you’re looking for jobs. And trust me, no one’s going to steal your idea. Don’t let that hold youback—be open to collaboration. Who you collaborate with really depends on your long-term goals—whether you’re leaning toward academia, industry, or independent research.What kind of environment works best for you? For example, if you're more industry-focused, look for mentors or renowned professionals in your sector. Share your project ideas with them. If they can't contribute directly, ask them to mentor you. Their name alone adds credibility, and it will give your project more visibility. Don’t put yourself in a shell—reach out. Let me share a personal story: I wanted to work on a research project focused onAI generated text detection. However, my schedule didn’t allow for formal research work, and I didn’t have the full team. So, I reached out to an independent researcher—someone not hugely famous but with a clear vision and a team that was willing to get things done. We collaborated throughCohere for AI, and guess what? Eight months later, we’re finalizing our draft for the paper. And I loved every minute of it. Example:Many researchers at companies like Google DeepMind and OpenAI started their careers by collaborating with academic mentors. TakeIan Goodfellow, the inventor of GANs. He started his journey by working closely with his professors during his PhD. His groundbreaking work led to a job at Google Brain and, later, Apple. Resources: That’s all for now! If you want to know about thetop high-paying AI skills for 2025, clickhere. And hey, if you’ve tried anything different in your job search that helped you stand out, drop it in the comments. I’d love to check it out! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",7 Best Strategies (Besides Job Portals) to Land Top-Paying Jobs in 2025,"

Key Points:
",Data Science,"Tired of the job portal grind? Don’t just apply—make them come to you! Check out 7 powerful strategies to land top-paying tech jobs in 2025. Image by Author| Canva Let’s be honest: the tech market is getting crowded.With every passing day, the competition gets fiercer. Recently,Meta CEO Mark Zuckerbergmade headlines by stating that mid-level IT engineers could soon be replaced by AI agents, leaving much of the work to shift towards design-level decision-making and advanced automation. This means the game is changing, and simply being""good enough""isn’t enough anymore. Another challenge I’ve noticed is the overwhelming number of applications companies receive for every job. For example, I recently had to urgently hire someone for a role. To my surprise, I received over2,000 applications. Sitting on the other side of the table for the first time, I realized that even with so many excellent resumes and portfolios, I had to narrow down the pool based on more than just qualifications. I started looking for things like:Do they have a strong social presence? Have they worked on a truly unique project? Did they reach out directly to me or someone in the company for a referral? Have I come across them before in any context? That experience made me realize one thing:applying to job portals isn’t enough in this competitive market. After researching, talking to experts, and reflecting on my own hiring experience, I’ve put together thislist of seven strategies that can help you rise above the noise.You don’t need to try them all—pick one or two where you can truly excel. A single, well-executed effort will always shine brighter than a scattershot approach. I will also be sharing some success stories in between to let you know that these aren’t just fancy terms or something impossible to achieve. So, let's get started.  Sometimes, the simplest way to get noticed is also the most effective—reach out to recruiters directly. Take Andrew Ng's advice:""Don't wait for opportunities to come to you.""Many companies prefer candidates who show initiative and genuine interest in their roles, and reaching out directly can help you stand out from the crowd. It might even help you skip the long queue of traditional applications. You might have heard about cold emailing in academia, but I feel it’s not used effectively in the industrial sector. Cold emailing isn’t just about shooting an email or messaging recruiters on LinkedIn. You need to respect their time and craft a message that grabs their attention.Your first few paragraphs can either make them read further or ignore your email. There’s no one-size-fits-all approach here—it depends on the role, the job description, your experience, and other factors. Use LinkedIn to identify recruiters or hiring managers. If possible, go for a premium version to message them directly. For students who can’t afford LinkedIn Premium, keep an eye out for giveaways or free trials. Example:Dhruv Loya, a recent graduate, landed a job at Tesla through cold emailing. Sharing his story makes it more relatable for people starting their careers. Another famous case isTim Ferriss, author of""The 4-Hour Workweek,""who used a direct, personalized email to reach out to entrepreneurPeter Thiel. That email eventually landed him a position on Thiel's board of advisors. Yes, it takes extra effort, but if the result is landing your dream job, isn’t it worth a shot? Resources:  I genuinely believe networking is one of the most underrated job-hunting tools. If you’re in the tech industry, I’m sure you’ve attended some sort of conference, event, or meetup. But ask yourself:Did you really make the most of the people you were surrounded by during those times?Conferences, workshops, and meetups (e.g., PyCon, NeurIPS, or local AI groups) aren’t just for learning—they’re absolute goldmines for networking. These events often give you access to exclusive, unadvertised roles and opportunities. Personally, I suggest that if you can, secure a speaking spot at these events. Share your project, work, or even conduct a technical workshop. You can start small with local AI workshops in your region to develop a speaking portfolio. If you’re not comfortable with public speaking, at least aim to network with people in the event. Face-to-face meetings allow you to make a memorable impression and engage in meaningful technical discussions. Afterward, be proactive—connect with them on LinkedIn or start a joint project if possible. Don't rush into focusing on monetary benefits at the start of your career.Instead, think about the long-term value of the relationships you build and how they can lead to unexpected opportunities. Example:Ali Ghodsi, CEO of Databricks, often shares how he networked with early adopters of Apache Spark at conferences. This community-building effort helped him secure investors and a strong team, which eventually led to the significant growth of Databricks. Resources:  “Building in public”means sharing your learning journey, projects, or experiments openly on social media or platforms like GitHub, Twitter, or Medium. A few years ago, this term was mainly used by startup founders who shared their journeys—from ideation to execution—in real-time. But now, it's expanded beyond just startups. Now, think about it: when you hear about big names like Meta or OpenAI, who comes to mind? Sam Altman, Mark Zuckerberg, Sundar Pichai? Sure, they’re the faces of these companies, but it’s not just them who make these companies successful. There’s a whole team working behind the scenes, and building in public can help get you noticed—even if you’re not in the spotlight. The real benefit of building in public is the long-term advantages. If you decide to start your own company or launch a product down the road,you’ll already have credibility and an audience that’s excited to support and test what you create.Even if you're just aiming for a top-paying job at a leading tech company, building a visible online presence can put you ahead of other candidates. Here is thesuccess formulathat you can use building in public: Example:TakeChip Huyenas an example. When people think of ML systems or AI engineering, her name is often one of the first that comes up. She started by sharing her insights on ML engineering and is now the founder of Claypot AI. By building in public, she not only established her credibility but also created opportunities that many others wouldn’t get by just sending job applications. Resources:  I’m glad to see that technical blogging and content creation are finally getting the recognition they deserve when it comes to job hunting and building your professional profile.Honestly, I can vouch for how rewarding this can be. I’ve received numerous offers from people who came across my technical articles and wanted to work with me. While it’s true it takes time, the payoff is real. When I first started technical writing, there weren’t many people doing it. But after COVID and especially with the rise of AI tools, the space has become quite crowded. If you want to stand out, you need to find your own unique angle.What’s going to make your content stand out? Why should someone spend their valuable time reading or watching your stuff?High-quality content always rises to the top. So, take a moment to think: What are you good at? What gaps exist in the current content landscape? Where’s there a lack of quality material?Once you figure this out, market and share your articles with the right audience. For technical posts, Reddit is a great platform. If you’re sharing more digestible content like roadmaps, career advice, or beginner-friendly tips, LinkedIn and Twitter are perfect. You could even start a newsletter, but make sure it offers something unique. One of the things I really enjoy about content creation is thatit keeps you updated with the latest trends and developments in your field.It forces you to learn and stay on top of things, which only sharpens your skills further. Example:Umar Jamilcreates some of the most detailed, beginner-friendly video tutorials on machine learning and AI. His content is gold because it goes beyond theory—he codes everything from scratch.  I’d bet my time on watching his videos because they offer something unique and valuable that’s hard to find elsewhere. Similarly, for the NLP folks,Jay Alammar’s visual guide to transformers became a go-to resource for anyone wanting to understand these models. His approach helped fill a huge gap in the community.Rachel Thomas, co-founder of fast.ai, also started out writing detailed explanations of deep learning concepts, and look at where she is now! Recruiters often look for candidates who can break down complex ideas clearly, and blogging is a great way to showcase that ability. Resources:  You’ve probably heard the saying,""Actions speak louder than words,""and it couldn’t be more true when it comes to building practical tools. But before discussing this, let’s clear up an important distinction: the difference between your typical resume projects and building real, usable tools. Resume projects often consist of theoretical work, research-based projects, or solutions that might not be fully deployed or usable. They may involve repetitive ideas that don’t solve real-world problems. In contrast, creating practical tools means you’re tackling tangible, real-world problems and offering solutions that others can use. These tools don’t need to be huge, groundbreaking ideas. They just need to be well-executed solutions for everyday problems.The key here is execution, not the complexity of the idea. And trust me, when done right, these tools serve as a portfolio piece that recruiters can directly interact with. They get to see your skills in action, which is far more powerful than just listing them on a resume. Let’s say you work in AI or software development. Instead of having a project that just sits in a repo gathering dust, create a tool that addresses a problem people actually face. For example, you could build a Chrome extension that helps generate quick email replies with a base template, or a simple tool that automates data cleaning. These kinds of tools show that you can not only come up with solutions but also implement them in a way that people can actually use. When you create these tools, you’re giving your audience something tangible. It’s more than just words on paper. You’re proving you can deliver something functional and useful. Example:Hugging Facestarted as a small side project by Clément Delangue and Julien Chaumond, and look at it now—it's a billion-dollar company revolutionizing NLP. What started as a side project for developers became one of the leading platforms in AI. This shows how even the simplest of tools can make a massive impact. Resources:  If you're in the tech space, you’ve probably heard about open source contributions. But here’s the thing: many people are more focused on the numbers, like how many contributions they can rack up, without really understanding the essence of what open source is all about. It’s disheartening because some folks treat it like a way to boost their GitHub profile, contributing low-effort stuff just for the sake of it. I’m not saying you shouldn’t get started with easier contributions. It's a great way to learn how it works. But once you have a decent understanding, aim for meaningful contributions that actually make a difference. The 80/20 principle applies here—focus on the 20% of contributions that really matter, and trust me, that will have a much bigger impact than dozens of shallow pull requests. If you want a deeper understanding of what I mean, I highly recommend readingEat That Frog. The truth is, open-source contributions aren’t just about showing off coding skills—they’re about demonstrating a commitment to collaboration, problem-solving, and improving the community. Plus, they get you noticed by recruiters. Some people even highlight the number of stars their projects have gotten as a part of their resume. Here’s how you can start contributing in a meaningful way: Example:Sebastian Raschkacontributed to open-source projects early in his career, which helped him build credibility and eventually publish the widely-known book ""Machine Learning with PyTorch and Scikit-Learn"". Similarly,Maxime Labonne's open-source work on LLM post-training gained him recognition throughout the AI community. And then there’sroadmap.shbyKamran Ahmed—which began as a straightforward project to guide developers through various tech career paths.  Today, it’s the 7th most-starred repository on GitHub and the 2nd most-starred codebase overall. Resources:  Many tech geeks prefer working solo, but let me tell you, no matter how skilled you are, this might not be the most helpful route when you’re looking for jobs. And trust me, no one’s going to steal your idea. Don’t let that hold youback—be open to collaboration. Who you collaborate with really depends on your long-term goals—whether you’re leaning toward academia, industry, or independent research.What kind of environment works best for you? For example, if you're more industry-focused, look for mentors or renowned professionals in your sector. Share your project ideas with them. If they can't contribute directly, ask them to mentor you. Their name alone adds credibility, and it will give your project more visibility. Don’t put yourself in a shell—reach out. Let me share a personal story: I wanted to work on a research project focused onAI generated text detection. However, my schedule didn’t allow for formal research work, and I didn’t have the full team. So, I reached out to an independent researcher—someone not hugely famous but with a clear vision and a team that was willing to get things done. We collaborated throughCohere for AI, and guess what? Eight months later, we’re finalizing our draft for the paper. And I loved every minute of it. Example:Many researchers at companies like Google DeepMind and OpenAI started their careers by collaborating with academic mentors. TakeIan Goodfellow, the inventor of GANs. He started his journey by working closely with his professors during his PhD. His groundbreaking work led to a job at Google Brain and, later, Apple. Resources: That’s all for now! If you want to know about thetop high-paying AI skills for 2025, clickhere. And hey, if you’ve tried anything different in your job search that helped you stand out, drop it in the comments. I’d love to check it out! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
A Beginner’s Guide to Integrating LLMs with Your Data Science Projects,https://www.kdnuggets.com/a-beginners-guide-to-integrating-llms-with-your-data-science-projects,KDNuggets,2025-02-26T11:00:33,KDNuggets,https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Learn the best ways to use LLM in your data projects. Image by Editor (Kanwal Mehreen) | Canva Large language models, or LLM, have changed the way we work. By implementing the model capability, the model could improve our work times by generating all the necessary text for the intended tasks. In data science projects, LLMs can help you in many ways that people have never considered. That’s why this article will guide you in integrating LLMs to support your data science project. The process might not be linear, but each point will help your project differently. Curious about it? Let’s get into it.  One of the jobs that data scientists always need to do is to perform data exploration. It’s one of the most tedious and repetitive jobs a data scientist could do.In this case, we can integrate LLM into our data project by allowing the model to assist in our data exploration phase. There are many ways to approach this, like asking directly to tools like ChatGPT or Gemini, and then you can copy the code to execute them. However, we will use a simpler approach, which is using the Pandasai library to help us explore the data with LLM without setting up much of the hard stuff. Let’s start by installing the library to start.  Next, we will set up the LLM we want to use. Many options exist, but this tutorial will only use the OpenAI LLM. We will also use the Titanic example dataset from Kaggle.  Once the dataset is ready and passed into the SmartDataFrame object, we will use Pandasai to facilitate LLM usage for data exploration. First, I can ask what the data is about with the following code. We can also specify the kind of exploration we want. For example, I want the percentage of missing data. It’s also possible to generate a chart by asking the Pandasai to do that. You can try it out yourself. Follow the prompt as needed, and Pandasai will use LLM to help with your project quickly. LLM can also be used to discuss and generate new features. For example, using the previous Pandasai approach, we can ask them to develop new features based on our dataset. A few new features are generated according to the dataset. The output is shown in the image below.If you need more domain-specific feature engineering, we can ask LLM for suggestions on how the features should be or even what kind of data we should collect. Another thing you can do with LLM is to generate vector embedding from your dataset, especially text data. As the embedding is numerical data, it can be processed further for any downstream tasks you have. For example, we can generate embedding with OpenAI using the following code. The code above will produce vector embedding, which you can use for further processing. LLMs can also help your data science project by acting as a classifier and assuming the model to classify data. For example, we can use Scikit-LLM, a Python package that enhances text data analytic tasks via LLM to classify text data. First, we will install the library with the following code. Then, we can try the library to create text prediction, such as sentiment analysis, with the following code. LLM can easily be used for the text classifier model without any additional model training. To improve the result, you can also extend it with a few shot examples. Another example of using synthetic data to support model building and training is generating synthetic data. LLM can produce a similar dataset but not an exact copy of the actual dataset. We can introduce more variation to the data using synthetic data and help the machine learning model generalize well. Here is an example code for generating synthetic datasets with LLM. A simple approach can improve your model. Try out synthetic data generation with your prompt to see if it helps your work. LLM has changed how we work, and it’s for the better. Integrating LLM into a data science project is one of the use cases that the model could do. In this article, we explore how we can incorporate LLM into your project, including: I hope this has helped! Cornellius Yudha Wijayais a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and data tips via social media and writing media. Cornellius writes on a variety of AI and machine learning topics. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Learn the best ways to use LLM in your data projects. Image by Editor (Kanwal Mehreen) | Canva Large language models, or LLM, have changed the way we work. By implementing the model capability, the model could improve our work times by generating all the necessary text for the intended tasks. In data science projects, LLMs can help you in many ways that people have never considered. That’s why this article will guide you in integrating LLMs to support your data science project. The process might not be linear, but each point will help your project differently. Curious about it? Let’s get into it.  One of the jobs that data scientists always need to do is to perform data exploration. It’s one of the most tedious and repetitive jobs a data scientist could do.In this case, we can integrate LLM into our data project by allowing the model to assist in our data exploration phase. There are many ways to approach this, like asking directly to tools like ChatGPT or Gemini, and then you can copy the code to execute them. However, we will use a simpler approach, which is using the Pandasai library to help us explore the data with LLM without setting up much of the hard stuff. Let’s start by installing the library to start.  Next, we will set up the LLM we want to use. Many options exist, but this tutorial will only use the OpenAI LLM. We will also use the Titanic example dataset from Kaggle.  Once the dataset is ready and passed into the SmartDataFrame object, we will use Pandasai to facilitate LLM usage for data exploration. First, I can ask what the data is about with the following code. We can also specify the kind of exploration we want. For example, I want the percentage of missing data. It’s also possible to generate a chart by asking the Pandasai to do that. You can try it out yourself. Follow the prompt as needed, and Pandasai will use LLM to help with your project quickly. LLM can also be used to discuss and generate new features. For example, using the previous Pandasai approach, we can ask them to develop new features based on our dataset. A few new features are generated according to the dataset. The output is shown in the image below.If you need more domain-specific feature engineering, we can ask LLM for suggestions on how the features should be or even what kind of data we should collect. Another thing you can do with LLM is to generate vector embedding from your dataset, especially text data. As the embedding is numerical data, it can be processed further for any downstream tasks you have. For example, we can generate embedding with OpenAI using the following code. The code above will produce vector embedding, which you can use for further processing. LLMs can also help your data science project by acting as a classifier and assuming the model to classify data. For example, we can use Scikit-LLM, a Python package that enhances text data analytic tasks via LLM to classify text data. First, we will install the library with the following code. Then, we can try the library to create text prediction, such as sentiment analysis, with the following code. LLM can easily be used for the text classifier model without any additional model training. To improve the result, you can also extend it with a few shot examples. Another example of using synthetic data to support model building and training is generating synthetic data. LLM can produce a similar dataset but not an exact copy of the actual dataset. We can introduce more variation to the data using synthetic data and help the machine learning model generalize well. Here is an example code for generating synthetic datasets with LLM. A simple approach can improve your model. Try out synthetic data generation with your prompt to see if it helps your work. LLM has changed how we work, and it’s for the better. Integrating LLM into a data science project is one of the use cases that the model could do. In this article, we explore how we can incorporate LLM into your project, including: I hope this has helped! Cornellius Yudha Wijayais a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and data tips via social media and writing media. Cornellius writes on a variety of AI and machine learning topics. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",A Beginner’s Guide to Integrating LLMs with Your Data Science Projects,"

Key Points:
",Data Science,"Learn the best ways to use LLM in your data projects. Image by Editor (Kanwal Mehreen) | Canva Large language models, or LLM, have changed the way we work. By implementing the model capability, the model could improve our work times by generating all the necessary text for the intended tasks. In data science projects, LLMs can help you in many ways that people have never considered. That’s why this article will guide you in integrating LLMs to support your data science project. The process might not be linear, but each point will help your project differently. Curious about it? Let’s get into it.  One of the jobs that data scientists always need to do is to perform data exploration. It’s one of the most tedious and repetitive jobs a data scientist could do.In this case, we can integrate LLM into our data project by allowing the model to assist in our data exploration phase. There are many ways to approach this, like asking directly to tools like ChatGPT or Gemini, and then you can copy the code to execute them. However, we will use a simpler approach, which is using the Pandasai library to help us explore the data with LLM without setting up much of the hard stuff. Let’s start by installing the library to start.  Next, we will set up the LLM we want to use. Many options exist, but this tutorial will only use the OpenAI LLM. We will also use the Titanic example dataset from Kaggle.  Once the dataset is ready and passed into the SmartDataFrame object, we will use Pandasai to facilitate LLM usage for data exploration. First, I can ask what the data is about with the following code. We can also specify the kind of exploration we want. For example, I want the percentage of missing data. It’s also possible to generate a chart by asking the Pandasai to do that. You can try it out yourself. Follow the prompt as needed, and Pandasai will use LLM to help with your project quickly. LLM can also be used to discuss and generate new features. For example, using the previous Pandasai approach, we can ask them to develop new features based on our dataset. A few new features are generated according to the dataset. The output is shown in the image below.If you need more domain-specific feature engineering, we can ask LLM for suggestions on how the features should be or even what kind of data we should collect. Another thing you can do with LLM is to generate vector embedding from your dataset, especially text data. As the embedding is numerical data, it can be processed further for any downstream tasks you have. For example, we can generate embedding with OpenAI using the following code. The code above will produce vector embedding, which you can use for further processing. LLMs can also help your data science project by acting as a classifier and assuming the model to classify data. For example, we can use Scikit-LLM, a Python package that enhances text data analytic tasks via LLM to classify text data. First, we will install the library with the following code. Then, we can try the library to create text prediction, such as sentiment analysis, with the following code. LLM can easily be used for the text classifier model without any additional model training. To improve the result, you can also extend it with a few shot examples. Another example of using synthetic data to support model building and training is generating synthetic data. LLM can produce a similar dataset but not an exact copy of the actual dataset. We can introduce more variation to the data using synthetic data and help the machine learning model generalize well. Here is an example code for generating synthetic datasets with LLM. A simple approach can improve your model. Try out synthetic data generation with your prompt to see if it helps your work. LLM has changed how we work, and it’s for the better. Integrating LLM into a data science project is one of the use cases that the model could do. In this article, we explore how we can incorporate LLM into your project, including: I hope this has helped! Cornellius Yudha Wijayais a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and data tips via social media and writing media. Cornellius writes on a variety of AI and machine learning topics. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
30 Must-Know Tools for Python Development,https://www.kdnuggets.com/2025/02/nettresults/30-must-know-tools-for-python-development,KDNuggets,2025-02-25T18:00:09,KDNuggets,https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"A structured overview of the essential tools developers can use across different aspects of Python development   Python development involves various stages and equally many tools to manage them: We gathered several such popular tools in the following visual:  The objective is to provide a structured overview of the essential tools developers can use across different aspects of Python development. Let's explore each category and its top tools in more detail.  Manage Python package installations and dependencies.  Optimize and analyze performance.  Ensure project isolation and manage dependencies efficiently.  Enforce coding standards and maintain code quality.  Ensure type correctness in Python codebases.  Monitor application behavior and track issues.  Automate testing for software reliability.  Identify and fix issues in your code.  Improve and restructure code efficiently.  Detect and mitigate security vulnerabilities. These tools are invaluable for any Python developer, helping with everything from virtual environments and dependency management to debugging, logging, and security. Incorporating them into your workflow can significantly improve your development experience and code quality. Over to you: Which tools do you regularly use from this landscape? Read the original article atDaily Dose of Data Science, a column for AI and ML professionals seeking clarity, depth, and practical insights to succeed in AI/ML roles—currently reaching 600k+ AI professionals every day. By,Avi Chawla- highly passionate about approaching and explaining data science problems with intuition. Avi has been working in the field of data science and machine learning for over 6 years, both across academia and industry.   Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","A structured overview of the essential tools developers can use across different aspects of Python development   Python development involves various stages and equally many tools to manage them: We gathered several such popular tools in the following visual:  The objective is to provide a structured overview of the essential tools developers can use across different aspects of Python development. Let's explore each category and its top tools in more detail.  Manage Python package installations and dependencies.  Optimize and analyze performance.  Ensure project isolation and manage dependencies efficiently.  Enforce coding standards and maintain code quality.  Ensure type correctness in Python codebases.  Monitor application behavior and track issues.  Automate testing for software reliability.  Identify and fix issues in your code.  Improve and restructure code efficiently.  Detect and mitigate security vulnerabilities. These tools are invaluable for any Python developer, helping with everything from virtual environments and dependency management to debugging, logging, and security. Incorporating them into your workflow can significantly improve your development experience and code quality. Over to you: Which tools do you regularly use from this landscape? Read the original article atDaily Dose of Data Science, a column for AI and ML professionals seeking clarity, depth, and practical insights to succeed in AI/ML roles—currently reaching 600k+ AI professionals every day. By,Avi Chawla- highly passionate about approaching and explaining data science problems with intuition. Avi has been working in the field of data science and machine learning for over 6 years, both across academia and industry.   Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",30 Must-Know Tools for Python Development,"

Key Points:
",Data Science,"A structured overview of the essential tools developers can use across different aspects of Python development   Python development involves various stages and equally many tools to manage them: We gathered several such popular tools in the following visual:  The objective is to provide a structured overview of the essential tools developers can use across different aspects of Python development. Let's explore each category and its top tools in more detail.  Manage Python package installations and dependencies.  Optimize and analyze performance.  Ensure project isolation and manage dependencies efficiently.  Enforce coding standards and maintain code quality.  Ensure type correctness in Python codebases.  Monitor application behavior and track issues.  Automate testing for software reliability.  Identify and fix issues in your code.  Improve and restructure code efficiently.  Detect and mitigate security vulnerabilities. These tools are invaluable for any Python developer, helping with everything from virtual environments and dependency management to debugging, logging, and security. Incorporating them into your workflow can significantly improve your development experience and code quality. Over to you: Which tools do you regularly use from this landscape? Read the original article atDaily Dose of Data Science, a column for AI and ML professionals seeking clarity, depth, and practical insights to succeed in AI/ML roles—currently reaching 600k+ AI professionals every day. By,Avi Chawla- highly passionate about approaching and explaining data science problems with intuition. Avi has been working in the field of data science and machine learning for over 6 years, both across academia and industry.   Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
Generative AI for Data Scientists in 2025: Beyond Text Generation,https://www.kdnuggets.com/generative-ai-for-data-scientists-in-2025-beyond-text-generation,KDNuggets,2025-02-25T17:00:03,KDNuggets,https://images.unsplash.com/photo-1551288049-bebda4e38f71?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Directions to become ""upgraded"" data scientists prepared to fully leverage generative AI technologies in the year ahead. Image by Editor (Kanwal Mehreen) | Canva There is little doubt at this point thatgenerative AI is transforming the daily work of data scientists and analysts. Traditionally, these roles have focused on delivering solutions such as data visualizations, reports, dashboards, machine learning models for predictive purposes, and analytical insights for storytelling. However, with the rise and spread of generative AI, data scientists are expected to expand their analytical capabilities to address more disparate forms of unstructured data, support business goals and teams, and even foster creativity in operational and strategic processes. This article explores how generative AI is being adopted by the data science community to enhance their skillsets, help achieve business goals, and in general to align them with current trends shaping the year 2025. The discussion provides a perspective beyond the most widespread generative AI use case of text generation via conversational tools like ChatGPT.  Generative AI is empowering data scientists not only to strengthen their technical expertise but also to elevate their creative skills.Automating routine coding taskslike data cleaning, feature engineering, and script optimization is now possible thanks to generative tools like OpenAI's Codex and GitHub Copilot, leaving more room to focus on high-impact tasks like developing advanced and interpretable AI models. Moreover, generative AI repositories like Hugging Face and cloud platforms like Google’s Vertex AI provide accessible frameworks forfine-tuning pre-trained generative AI modelson domain-specific datasets. A clear example is Vertex AI's model garden, which incorporates foundation models (pre-trained generative models for general-purpose scenarios) for use cases as diverse as long text summarization, image generation, and audio synthesis. These platforms make adoption simpler through APIs, pre-built tools, and tutorials, reducing the learning curve for new users. In the end, these solutions are a great pathway for data scientists to expand their skills repertoire beyond traditional analytics. From a more technical perspective,upskilling in generative AIand becoming more familiar with the intricacies of generative AI models and architectures also entails mastering emerging AI concepts such astransfer learning, multimodal AI models, reinforcement learning, and agentic AI. These advanced concepts are increasingly shaping the development and application of AI technologies and are indispensable for staying competitive in a rapidly evolving field.  Of course, staying abreast of all Generative AI capabilities not only benefits data scientists themselves but also the business organization they are part of. Generative AI is a crucial catalyst for data scientists aiming to align their work and skills with organizational objectives. By unlocking insights from unstructured data beyond just text -like images, video, audio, code, and even realistic-looking data- generative AI can broaden the horizons of business intelligence and conventional predictive analytics. For instance, image generation tools like DALL·E and MidJourney can automate creative design processes, making marketing campaigns more efficient, original, and personalized with innovative visuals. Language models can do a great job not only in generating text but also in advanced text analysis processes like summarizing large volumes of customer feedback to extract the key insights about what most people like or dislike in your products and express them in a synthesized yet meaningful fashion (Amazon just started doing this with their product reviews recently, check it out!). These kinds of solutions that rely to a significant degree on content generation are enabling businesses to make informed decisions more efficiently. Meanwhile,predictive modelingcan benefit from generative AI in multiple ways: one of them is byaugmenting datasetswith synthetic data that faithfully mimic the properties and patterns of reference datasets, thereby improving accuracy and reducing unwanted biases in machine learning models. Another impactful way to leverage generative AI in business contexts is by integrating existing enterprise tools like CRM and ERP systems: content generation here can certainly play its role by automating the creation of personalized communication resources tailored to specific customer segments. The result? Boosted customer engagement and satisfaction!  On a final remark, as 2025 unfolds several generative AI trends are set to shape data science processes and jobs in the year ahead, including the rise of multimodal AI for integrating diverse data sources into a single content generation task, edge AI for real-time and privacy-focused processing (this is very important due to generative AI systems frequently utilizing user-related inputs like portrait images), and advancements in code-less AI development tools like Google Vertex AI's AutoML to automate routine tasks. The prominence of ethical AI frameworks emphasizing transparency and fairness is likewise on the rise. A data scientist keen on staying competitive in 2025 must therefore integrate all these innovations into their skillset and workflows, and most of all, stay curious and informed about what is yet to come through industry resources and events. Iván Palomares Carrascosais a leader, writer, speaker, and adviser in AI, machine learning, deep learning & LLMs. He trains and guides others in harnessing AI in the real world. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Directions to become ""upgraded"" data scientists prepared to fully leverage generative AI technologies in the year ahead. Image by Editor (Kanwal Mehreen) | Canva There is little doubt at this point thatgenerative AI is transforming the daily work of data scientists and analysts. Traditionally, these roles have focused on delivering solutions such as data visualizations, reports, dashboards, machine learning models for predictive purposes, and analytical insights for storytelling. However, with the rise and spread of generative AI, data scientists are expected to expand their analytical capabilities to address more disparate forms of unstructured data, support business goals and teams, and even foster creativity in operational and strategic processes. This article explores how generative AI is being adopted by the data science community to enhance their skillsets, help achieve business goals, and in general to align them with current trends shaping the year 2025. The discussion provides a perspective beyond the most widespread generative AI use case of text generation via conversational tools like ChatGPT.  Generative AI is empowering data scientists not only to strengthen their technical expertise but also to elevate their creative skills.Automating routine coding taskslike data cleaning, feature engineering, and script optimization is now possible thanks to generative tools like OpenAI's Codex and GitHub Copilot, leaving more room to focus on high-impact tasks like developing advanced and interpretable AI models. Moreover, generative AI repositories like Hugging Face and cloud platforms like Google’s Vertex AI provide accessible frameworks forfine-tuning pre-trained generative AI modelson domain-specific datasets. A clear example is Vertex AI's model garden, which incorporates foundation models (pre-trained generative models for general-purpose scenarios) for use cases as diverse as long text summarization, image generation, and audio synthesis. These platforms make adoption simpler through APIs, pre-built tools, and tutorials, reducing the learning curve for new users. In the end, these solutions are a great pathway for data scientists to expand their skills repertoire beyond traditional analytics. From a more technical perspective,upskilling in generative AIand becoming more familiar with the intricacies of generative AI models and architectures also entails mastering emerging AI concepts such astransfer learning, multimodal AI models, reinforcement learning, and agentic AI. These advanced concepts are increasingly shaping the development and application of AI technologies and are indispensable for staying competitive in a rapidly evolving field.  Of course, staying abreast of all Generative AI capabilities not only benefits data scientists themselves but also the business organization they are part of. Generative AI is a crucial catalyst for data scientists aiming to align their work and skills with organizational objectives. By unlocking insights from unstructured data beyond just text -like images, video, audio, code, and even realistic-looking data- generative AI can broaden the horizons of business intelligence and conventional predictive analytics. For instance, image generation tools like DALL·E and MidJourney can automate creative design processes, making marketing campaigns more efficient, original, and personalized with innovative visuals. Language models can do a great job not only in generating text but also in advanced text analysis processes like summarizing large volumes of customer feedback to extract the key insights about what most people like or dislike in your products and express them in a synthesized yet meaningful fashion (Amazon just started doing this with their product reviews recently, check it out!). These kinds of solutions that rely to a significant degree on content generation are enabling businesses to make informed decisions more efficiently. Meanwhile,predictive modelingcan benefit from generative AI in multiple ways: one of them is byaugmenting datasetswith synthetic data that faithfully mimic the properties and patterns of reference datasets, thereby improving accuracy and reducing unwanted biases in machine learning models. Another impactful way to leverage generative AI in business contexts is by integrating existing enterprise tools like CRM and ERP systems: content generation here can certainly play its role by automating the creation of personalized communication resources tailored to specific customer segments. The result? Boosted customer engagement and satisfaction!  On a final remark, as 2025 unfolds several generative AI trends are set to shape data science processes and jobs in the year ahead, including the rise of multimodal AI for integrating diverse data sources into a single content generation task, edge AI for real-time and privacy-focused processing (this is very important due to generative AI systems frequently utilizing user-related inputs like portrait images), and advancements in code-less AI development tools like Google Vertex AI's AutoML to automate routine tasks. The prominence of ethical AI frameworks emphasizing transparency and fairness is likewise on the rise. A data scientist keen on staying competitive in 2025 must therefore integrate all these innovations into their skillset and workflows, and most of all, stay curious and informed about what is yet to come through industry resources and events. Iván Palomares Carrascosais a leader, writer, speaker, and adviser in AI, machine learning, deep learning & LLMs. He trains and guides others in harnessing AI in the real world. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",Generative AI for Data Scientists in 2025: Beyond Text Generation,"

Key Points:
",Data Science,"Directions to become ""upgraded"" data scientists prepared to fully leverage generative AI technologies in the year ahead. Image by Editor (Kanwal Mehreen) | Canva There is little doubt at this point thatgenerative AI is transforming the daily work of data scientists and analysts. Traditionally, these roles have focused on delivering solutions such as data visualizations, reports, dashboards, machine learning models for predictive purposes, and analytical insights for storytelling. However, with the rise and spread of generative AI, data scientists are expected to expand their analytical capabilities to address more disparate forms of unstructured data, support business goals and teams, and even foster creativity in operational and strategic processes. This article explores how generative AI is being adopted by the data science community to enhance their skillsets, help achieve business goals, and in general to align them with current trends shaping the year 2025. The discussion provides a perspective beyond the most widespread generative AI use case of text generation via conversational tools like ChatGPT.  Generative AI is empowering data scientists not only to strengthen their technical expertise but also to elevate their creative skills.Automating routine coding taskslike data cleaning, feature engineering, and script optimization is now possible thanks to generative tools like OpenAI's Codex and GitHub Copilot, leaving more room to focus on high-impact tasks like developing advanced and interpretable AI models. Moreover, generative AI repositories like Hugging Face and cloud platforms like Google’s Vertex AI provide accessible frameworks forfine-tuning pre-trained generative AI modelson domain-specific datasets. A clear example is Vertex AI's model garden, which incorporates foundation models (pre-trained generative models for general-purpose scenarios) for use cases as diverse as long text summarization, image generation, and audio synthesis. These platforms make adoption simpler through APIs, pre-built tools, and tutorials, reducing the learning curve for new users. In the end, these solutions are a great pathway for data scientists to expand their skills repertoire beyond traditional analytics. From a more technical perspective,upskilling in generative AIand becoming more familiar with the intricacies of generative AI models and architectures also entails mastering emerging AI concepts such astransfer learning, multimodal AI models, reinforcement learning, and agentic AI. These advanced concepts are increasingly shaping the development and application of AI technologies and are indispensable for staying competitive in a rapidly evolving field.  Of course, staying abreast of all Generative AI capabilities not only benefits data scientists themselves but also the business organization they are part of. Generative AI is a crucial catalyst for data scientists aiming to align their work and skills with organizational objectives. By unlocking insights from unstructured data beyond just text -like images, video, audio, code, and even realistic-looking data- generative AI can broaden the horizons of business intelligence and conventional predictive analytics. For instance, image generation tools like DALL·E and MidJourney can automate creative design processes, making marketing campaigns more efficient, original, and personalized with innovative visuals. Language models can do a great job not only in generating text but also in advanced text analysis processes like summarizing large volumes of customer feedback to extract the key insights about what most people like or dislike in your products and express them in a synthesized yet meaningful fashion (Amazon just started doing this with their product reviews recently, check it out!). These kinds of solutions that rely to a significant degree on content generation are enabling businesses to make informed decisions more efficiently. Meanwhile,predictive modelingcan benefit from generative AI in multiple ways: one of them is byaugmenting datasetswith synthetic data that faithfully mimic the properties and patterns of reference datasets, thereby improving accuracy and reducing unwanted biases in machine learning models. Another impactful way to leverage generative AI in business contexts is by integrating existing enterprise tools like CRM and ERP systems: content generation here can certainly play its role by automating the creation of personalized communication resources tailored to specific customer segments. The result? Boosted customer engagement and satisfaction!  On a final remark, as 2025 unfolds several generative AI trends are set to shape data science processes and jobs in the year ahead, including the rise of multimodal AI for integrating diverse data sources into a single content generation task, edge AI for real-time and privacy-focused processing (this is very important due to generative AI systems frequently utilizing user-related inputs like portrait images), and advancements in code-less AI development tools like Google Vertex AI's AutoML to automate routine tasks. The prominence of ethical AI frameworks emphasizing transparency and fairness is likewise on the rise. A data scientist keen on staying competitive in 2025 must therefore integrate all these innovations into their skillset and workflows, and most of all, stay curious and informed about what is yet to come through industry resources and events. Iván Palomares Carrascosais a leader, writer, speaker, and adviser in AI, machine learning, deep learning & LLMs. He trains and guides others in harnessing AI in the real world. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
Optimizing Memory Usage with NumPy Arrays,https://www.kdnuggets.com/optimizing-memory-usage-with-numpy-arrays,KDNuggets,2025-02-25T15:00:13,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Learn how to effectively optimize memory usage using NumPy arrays in Python. Image by Wesley Tingey | Unsplash Memory optimization is very important when working on a data science and machine learning project. Before digging deeper into this article, let's build muscle memory by first understanding what memory optimization means and how we can effectively use NumPy for this task. Managing and effectively distributing the computer memory resources so as to minimize memory usage while making sure that the computer system performance is at its peak is known asmemory optimization. When writing code, you need to use the appropriate data structures to maximize memory efficiency.This is because some data types consume less memory, and some consume more. You must also consider memory duplication and make sure to avoid it at all cost while freeing unused memory regularly. NumPy is very efficient in memory unlike Python lists. NumPy stores data in a with a contiguous memory block while Python lists stores element as separate objects. NumPy arrays have fixed data types, meaning all elements occupy the same amount of memory. This further reduces memory usage compared to Python lists, where each element's size can vary. This makes NumPy much more memory-efficient when handling large datasets.  NumPy arrays store their elements in contiguous(adjacent)blocks of memory, meaning that all the components are packed tightly together. This layout allows fast access and efficient operations on the array, as memory lookups are minimized. Since NumPy arrays are homogeneous, meaning all elements have the same data type, the memory space required for each element is identical. NumPy only needs to store the size of the array, the shape (i.e., dimensions), and the data type. Then it allows a direct access to elements via their index positions without following pointers. As a result, operations on NumPy arrays are much faster and require less memory overhead compared to Python lists.  There are two memory layouts in NumPy, namely,C-orderandFortran-order. The choice between C-order and Fortran-order can impact both the performance and memory access patterns of NumPy arrays.  In this section, we will cover the different methods and ways to optimize memory usage using NumPy arrays. Some of these methods include choosing the right data type, using views instead of copies, using broadcasting efficiently, reducing array size withnp.squeezeandnp.compress, and memory mapping withnp.memmap  Choosing the right data type(dtype)for your NumPy arrays is one of the main strategies to minimize memory utilization. The data type you select will determine the memory footprint of an array, since NumPy arrays are homogeneous, meaning that every element in an array has the same dtype. You can save memory by utilizing smaller data types that are still within the range of your data.  Code explanation:  A view in NumPy refers to a new array object that refers to the same data as the original array. This saves memory because no new data is created. On the other hand, a copy is a new array object with its own separate copy of the data. Modifying a copy will not affect the original array, as it occupies its own memory space.  Code explanation:  Broadcasting in NumPy is a powerful feature that allows arrays of different shapes to be used in arithmetic operations without explicitly reshaping them. It will enable NumPy to perform operations on arrays of different shapes without creating large temporary arrays, which saves memory by reusing existing data during operations instead of expanding arrays. Broadcasting works basically by automatically expanding smaller arrays along their dimensions to match the shape of larger arrays in an operation. This eliminates the need to manually reshape arrays or create unnecessary temporary arrays, saving memory.  Code explanation:  NumPy has operations such asnp.squeezeandnp.compress, which help minimize array sizes by eliminating unnecessary dimensions or filtering certain data.  Code explanation:  Memory mapping (np.memmap) allows you to work with large datasets that don’t fit into memory by storing data on disk and accessing only the necessary portions.  Code explanation:  In conclusion, in this article we have been able to learn how to optimize memory usage using NumPy arrays. If you are conveniently leverage the methods highlighted in this article such as choosing the right data types, using views instead of copies, and taking advantage of broadcasting, you can significantly reduce memory consumption without sacrificing performance. Shittu Olumideis a software engineer and technical writer passionate about leveraging cutting-edge technologies to craft compelling narratives, with a keen eye for detail and a knack for simplifying complex concepts. You can also find Shittu onTwitter. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Learn how to effectively optimize memory usage using NumPy arrays in Python. Image by Wesley Tingey | Unsplash Memory optimization is very important when working on a data science and machine learning project. Before digging deeper into this article, let's build muscle memory by first understanding what memory optimization means and how we can effectively use NumPy for this task. Managing and effectively distributing the computer memory resources so as to minimize memory usage while making sure that the computer system performance is at its peak is known asmemory optimization. When writing code, you need to use the appropriate data structures to maximize memory efficiency.This is because some data types consume less memory, and some consume more. You must also consider memory duplication and make sure to avoid it at all cost while freeing unused memory regularly. NumPy is very efficient in memory unlike Python lists. NumPy stores data in a with a contiguous memory block while Python lists stores element as separate objects. NumPy arrays have fixed data types, meaning all elements occupy the same amount of memory. This further reduces memory usage compared to Python lists, where each element's size can vary. This makes NumPy much more memory-efficient when handling large datasets.  NumPy arrays store their elements in contiguous(adjacent)blocks of memory, meaning that all the components are packed tightly together. This layout allows fast access and efficient operations on the array, as memory lookups are minimized. Since NumPy arrays are homogeneous, meaning all elements have the same data type, the memory space required for each element is identical. NumPy only needs to store the size of the array, the shape (i.e., dimensions), and the data type. Then it allows a direct access to elements via their index positions without following pointers. As a result, operations on NumPy arrays are much faster and require less memory overhead compared to Python lists.  There are two memory layouts in NumPy, namely,C-orderandFortran-order. The choice between C-order and Fortran-order can impact both the performance and memory access patterns of NumPy arrays.  In this section, we will cover the different methods and ways to optimize memory usage using NumPy arrays. Some of these methods include choosing the right data type, using views instead of copies, using broadcasting efficiently, reducing array size withnp.squeezeandnp.compress, and memory mapping withnp.memmap  Choosing the right data type(dtype)for your NumPy arrays is one of the main strategies to minimize memory utilization. The data type you select will determine the memory footprint of an array, since NumPy arrays are homogeneous, meaning that every element in an array has the same dtype. You can save memory by utilizing smaller data types that are still within the range of your data.  Code explanation:  A view in NumPy refers to a new array object that refers to the same data as the original array. This saves memory because no new data is created. On the other hand, a copy is a new array object with its own separate copy of the data. Modifying a copy will not affect the original array, as it occupies its own memory space.  Code explanation:  Broadcasting in NumPy is a powerful feature that allows arrays of different shapes to be used in arithmetic operations without explicitly reshaping them. It will enable NumPy to perform operations on arrays of different shapes without creating large temporary arrays, which saves memory by reusing existing data during operations instead of expanding arrays. Broadcasting works basically by automatically expanding smaller arrays along their dimensions to match the shape of larger arrays in an operation. This eliminates the need to manually reshape arrays or create unnecessary temporary arrays, saving memory.  Code explanation:  NumPy has operations such asnp.squeezeandnp.compress, which help minimize array sizes by eliminating unnecessary dimensions or filtering certain data.  Code explanation:  Memory mapping (np.memmap) allows you to work with large datasets that don’t fit into memory by storing data on disk and accessing only the necessary portions.  Code explanation:  In conclusion, in this article we have been able to learn how to optimize memory usage using NumPy arrays. If you are conveniently leverage the methods highlighted in this article such as choosing the right data types, using views instead of copies, and taking advantage of broadcasting, you can significantly reduce memory consumption without sacrificing performance. Shittu Olumideis a software engineer and technical writer passionate about leveraging cutting-edge technologies to craft compelling narratives, with a keen eye for detail and a knack for simplifying complex concepts. You can also find Shittu onTwitter. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",Optimizing Memory Usage with NumPy Arrays,"

Key Points:
",Data Science,"Learn how to effectively optimize memory usage using NumPy arrays in Python. Image by Wesley Tingey | Unsplash Memory optimization is very important when working on a data science and machine learning project. Before digging deeper into this article, let's build muscle memory by first understanding what memory optimization means and how we can effectively use NumPy for this task. Managing and effectively distributing the computer memory resources so as to minimize memory usage while making sure that the computer system performance is at its peak is known asmemory optimization. When writing code, you need to use the appropriate data structures to maximize memory efficiency.This is because some data types consume less memory, and some consume more. You must also consider memory duplication and make sure to avoid it at all cost while freeing unused memory regularly. NumPy is very efficient in memory unlike Python lists. NumPy stores data in a with a contiguous memory block while Python lists stores element as separate objects. NumPy arrays have fixed data types, meaning all elements occupy the same amount of memory. This further reduces memory usage compared to Python lists, where each element's size can vary. This makes NumPy much more memory-efficient when handling large datasets.  NumPy arrays store their elements in contiguous(adjacent)blocks of memory, meaning that all the components are packed tightly together. This layout allows fast access and efficient operations on the array, as memory lookups are minimized. Since NumPy arrays are homogeneous, meaning all elements have the same data type, the memory space required for each element is identical. NumPy only needs to store the size of the array, the shape (i.e., dimensions), and the data type. Then it allows a direct access to elements via their index positions without following pointers. As a result, operations on NumPy arrays are much faster and require less memory overhead compared to Python lists.  There are two memory layouts in NumPy, namely,C-orderandFortran-order. The choice between C-order and Fortran-order can impact both the performance and memory access patterns of NumPy arrays.  In this section, we will cover the different methods and ways to optimize memory usage using NumPy arrays. Some of these methods include choosing the right data type, using views instead of copies, using broadcasting efficiently, reducing array size withnp.squeezeandnp.compress, and memory mapping withnp.memmap  Choosing the right data type(dtype)for your NumPy arrays is one of the main strategies to minimize memory utilization. The data type you select will determine the memory footprint of an array, since NumPy arrays are homogeneous, meaning that every element in an array has the same dtype. You can save memory by utilizing smaller data types that are still within the range of your data.  Code explanation:  A view in NumPy refers to a new array object that refers to the same data as the original array. This saves memory because no new data is created. On the other hand, a copy is a new array object with its own separate copy of the data. Modifying a copy will not affect the original array, as it occupies its own memory space.  Code explanation:  Broadcasting in NumPy is a powerful feature that allows arrays of different shapes to be used in arithmetic operations without explicitly reshaping them. It will enable NumPy to perform operations on arrays of different shapes without creating large temporary arrays, which saves memory by reusing existing data during operations instead of expanding arrays. Broadcasting works basically by automatically expanding smaller arrays along their dimensions to match the shape of larger arrays in an operation. This eliminates the need to manually reshape arrays or create unnecessary temporary arrays, saving memory.  Code explanation:  NumPy has operations such asnp.squeezeandnp.compress, which help minimize array sizes by eliminating unnecessary dimensions or filtering certain data.  Code explanation:  Memory mapping (np.memmap) allows you to work with large datasets that don’t fit into memory by storing data on disk and accessing only the necessary portions.  Code explanation:  In conclusion, in this article we have been able to learn how to optimize memory usage using NumPy arrays. If you are conveniently leverage the methods highlighted in this article such as choosing the right data types, using views instead of copies, and taking advantage of broadcasting, you can significantly reduce memory consumption without sacrificing performance. Shittu Olumideis a software engineer and technical writer passionate about leveraging cutting-edge technologies to craft compelling narratives, with a keen eye for detail and a knack for simplifying complex concepts. You can also find Shittu onTwitter. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
10 Essential Docker Commands for Data Engineering,https://www.kdnuggets.com/10-essential-docker-commands-for-data-engineering,KDNuggets,2025-02-25T13:00:11,KDNuggets,https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?q=80&w=2947&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Tired of 'it works on my machine' problems? Learn the top 10 Docker commands every data engineer needs to build, deploy, and scale projects like a pro! Image by Author | Canva Docker is basically a tool that helps data engineers package, distribute, and run applications in a consistent environment.Instead of manually installing stuff (and praying it works everywhere), you just wrap your entire project—code, tools, dependencies into lightweight, portable, and self-sufficient environments called containers.These containers can run your code anywhere, whether on your laptop, a server, or the cloud. For example, if your project needs Python, Spark, and a bunch of specific libraries, instead of manually installing them on every machine, you can just spin up a Docker container with everything pre-configured. Share it with your team, and they’ll have the exact same setup running in no time. Before we discuss the essential commands, let’s go over some key Docker terminology to make sure we’re all on the same page. Before using Docker, you'll need to install:  Here are the essential Docker commands that every data engineer should know:  What It Does:Creates and starts a container from an image. Why It’s Important:Data engineers frequently launch databases, processing engines, or API services. Thedocker runcommand’s flags are critical: Without volumes, database data would vanish when the container stops—a disaster for production pipelines.  What It Does:Turn your Dockerfile into a reusable image.  Why It’s Important:Data engineers often need custom images preloaded with tools like Airflow, PySpark, or machine learning libraries. Thedocker buildcommand ensures teams use identical environments, eliminating ""works on my machine"" issues.  What It Does:Executes a command inside a running container. Why It’s Important:Data engineers use this to inspect databases, run ad-hoc queries, or test scripts without restarting containers. The-itflags lets you type commands interactively (without this, you’re stuck in read-only mode).  What It Does:Displays logs from a container. Why It’s Important:Debugging failed tasks (e.g., Airflow DAGs or Spark jobs) relies on logs. The-fflag streams logs in real-time, helping diagnose runtime issues.  What It Does:Live dashboard for CPU, memory, and network usage of containers. Why It’s Important:Efficient resource monitoring is important for maintaining optimal performance in data pipelines. For example, if a data pipeline experiences slow processing, checkingdocker statscan reveal whether PostgreSQL is overutilizing CPU resources or if a Spark worker is consuming excessive memory, allowing for timely optimization.  What It Does:Start multi-container applications using adocker-compose.ymlfile.  Why It’s Important:Data pipelines often involve interconnected services (e.g., Airflow + PostgreSQL + Redis). Compose simplifies defining and managing these dependencies in a single declarative file so you don’t run 10 commands manually. The d flag allows you to run containers in the background (detached mode).  What It Does:Manages persistent storage for containers. Why It’s Important:Volumes preserve critical data (e.g., CSV files, database tables) even if containers crash. They’re also used to share data between containers (e.g., Spark and Hadoop).  What It Does:Download an image from Docker Hub (or another registry). Why It’s Important:Pre-built images save hours of setup time. Official images for tools like Spark, Kafka, or Jupyter are regularly updated and optimized.  What It Does:Stop and remove containers. Why It’s Important:Data engineers test pipelines iteratively. Stopping and removing old containers prevents resource leaks and keeps environments clean.  What It Does:Clean up unused containers, images, and volumes to free resources. Why It’s Important:Over time, Docker environments accumulate unused images, stopped containers, and dangling volumes (Docker volume that is no longer associated with any container), which eats disk space and slow down performance. This command reclaims gigabytes after weeks of testing. Mastering these Docker commands empowers data engineers to deploy reproducible pipelines, streamline collaboration, and troubleshoot effectively. Do you have a favorite Docker command that you use in your daily workflow? Let us know in the comments! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!","Tired of 'it works on my machine' problems? Learn the top 10 Docker commands every data engineer needs to build, deploy, and scale projects like a pro! Image by Author | Canva Docker is basically a tool that helps data engineers package, distribute, and run applications in a consistent environment.Instead of manually installing stuff (and praying it works everywhere), you just wrap your entire project—code, tools, dependencies into lightweight, portable, and self-sufficient environments called containers.These containers can run your code anywhere, whether on your laptop, a server, or the cloud. For example, if your project needs Python, Spark, and a bunch of specific libraries, instead of manually installing them on every machine, you can just spin up a Docker container with everything pre-configured. Share it with your team, and they’ll have the exact same setup running in no time. Before we discuss the essential commands, let’s go over some key Docker terminology to make sure we’re all on the same page. Before using Docker, you'll need to install:  Here are the essential Docker commands that every data engineer should know:  What It Does:Creates and starts a container from an image. Why It’s Important:Data engineers frequently launch databases, processing engines, or API services. Thedocker runcommand’s flags are critical: Without volumes, database data would vanish when the container stops—a disaster for production pipelines.  What It Does:Turn your Dockerfile into a reusable image.  Why It’s Important:Data engineers often need custom images preloaded with tools like Airflow, PySpark, or machine learning libraries. Thedocker buildcommand ensures teams use identical environments, eliminating ""works on my machine"" issues.  What It Does:Executes a command inside a running container. Why It’s Important:Data engineers use this to inspect databases, run ad-hoc queries, or test scripts without restarting containers. The-itflags lets you type commands interactively (without this, you’re stuck in read-only mode).  What It Does:Displays logs from a container. Why It’s Important:Debugging failed tasks (e.g., Airflow DAGs or Spark jobs) relies on logs. The-fflag streams logs in real-time, helping diagnose runtime issues.  What It Does:Live dashboard for CPU, memory, and network usage of containers. Why It’s Important:Efficient resource monitoring is important for maintaining optimal performance in data pipelines. For example, if a data pipeline experiences slow processing, checkingdocker statscan reveal whether PostgreSQL is overutilizing CPU resources or if a Spark worker is consuming excessive memory, allowing for timely optimization.  What It Does:Start multi-container applications using adocker-compose.ymlfile.  Why It’s Important:Data pipelines often involve interconnected services (e.g., Airflow + PostgreSQL + Redis). Compose simplifies defining and managing these dependencies in a single declarative file so you don’t run 10 commands manually. The d flag allows you to run containers in the background (detached mode).  What It Does:Manages persistent storage for containers. Why It’s Important:Volumes preserve critical data (e.g., CSV files, database tables) even if containers crash. They’re also used to share data between containers (e.g., Spark and Hadoop).  What It Does:Download an image from Docker Hub (or another registry). Why It’s Important:Pre-built images save hours of setup time. Official images for tools like Spark, Kafka, or Jupyter are regularly updated and optimized.  What It Does:Stop and remove containers. Why It’s Important:Data engineers test pipelines iteratively. Stopping and removing old containers prevents resource leaks and keeps environments clean.  What It Does:Clean up unused containers, images, and volumes to free resources. Why It’s Important:Over time, Docker environments accumulate unused images, stopped containers, and dangling volumes (Docker volume that is no longer associated with any container), which eats disk space and slow down performance. This command reclaims gigabytes after weeks of testing. Mastering these Docker commands empowers data engineers to deploy reproducible pipelines, streamline collaboration, and troubleshoot effectively. Do you have a favorite Docker command that you use in your daily workflow? Let us know in the comments! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!",10 Essential Docker Commands for Data Engineering,"

Key Points:
",Data Science,"Tired of 'it works on my machine' problems? Learn the top 10 Docker commands every data engineer needs to build, deploy, and scale projects like a pro! Image by Author | Canva Docker is basically a tool that helps data engineers package, distribute, and run applications in a consistent environment.Instead of manually installing stuff (and praying it works everywhere), you just wrap your entire project—code, tools, dependencies into lightweight, portable, and self-sufficient environments called containers.These containers can run your code anywhere, whether on your laptop, a server, or the cloud. For example, if your project needs Python, Spark, and a bunch of specific libraries, instead of manually installing them on every machine, you can just spin up a Docker container with everything pre-configured. Share it with your team, and they’ll have the exact same setup running in no time. Before we discuss the essential commands, let’s go over some key Docker terminology to make sure we’re all on the same page. Before using Docker, you'll need to install:  Here are the essential Docker commands that every data engineer should know:  What It Does:Creates and starts a container from an image. Why It’s Important:Data engineers frequently launch databases, processing engines, or API services. Thedocker runcommand’s flags are critical: Without volumes, database data would vanish when the container stops—a disaster for production pipelines.  What It Does:Turn your Dockerfile into a reusable image.  Why It’s Important:Data engineers often need custom images preloaded with tools like Airflow, PySpark, or machine learning libraries. Thedocker buildcommand ensures teams use identical environments, eliminating ""works on my machine"" issues.  What It Does:Executes a command inside a running container. Why It’s Important:Data engineers use this to inspect databases, run ad-hoc queries, or test scripts without restarting containers. The-itflags lets you type commands interactively (without this, you’re stuck in read-only mode).  What It Does:Displays logs from a container. Why It’s Important:Debugging failed tasks (e.g., Airflow DAGs or Spark jobs) relies on logs. The-fflag streams logs in real-time, helping diagnose runtime issues.  What It Does:Live dashboard for CPU, memory, and network usage of containers. Why It’s Important:Efficient resource monitoring is important for maintaining optimal performance in data pipelines. For example, if a data pipeline experiences slow processing, checkingdocker statscan reveal whether PostgreSQL is overutilizing CPU resources or if a Spark worker is consuming excessive memory, allowing for timely optimization.  What It Does:Start multi-container applications using adocker-compose.ymlfile.  Why It’s Important:Data pipelines often involve interconnected services (e.g., Airflow + PostgreSQL + Redis). Compose simplifies defining and managing these dependencies in a single declarative file so you don’t run 10 commands manually. The d flag allows you to run containers in the background (detached mode).  What It Does:Manages persistent storage for containers. Why It’s Important:Volumes preserve critical data (e.g., CSV files, database tables) even if containers crash. They’re also used to share data between containers (e.g., Spark and Hadoop).  What It Does:Download an image from Docker Hub (or another registry). Why It’s Important:Pre-built images save hours of setup time. Official images for tools like Spark, Kafka, or Jupyter are regularly updated and optimized.  What It Does:Stop and remove containers. Why It’s Important:Data engineers test pipelines iteratively. Stopping and removing old containers prevents resource leaks and keeps environments clean.  What It Does:Clean up unused containers, images, and volumes to free resources. Why It’s Important:Over time, Docker environments accumulate unused images, stopped containers, and dangling volumes (Docker volume that is no longer associated with any container), which eats disk space and slow down performance. This command reclaims gigabytes after weeks of testing. Mastering these Docker commands empowers data engineers to deploy reproducible pipelines, streamline collaboration, and troubleshoot effectively. Do you have a favorite Docker command that you use in your daily workflow? Let us know in the comments! Kanwal MehreenKanwal is a machine learning engineer and a technical writer with a profound passion for data science and the intersection of AI with medicine. She co-authored the ebook ""Maximizing Productivity with ChatGPT"". As a Google Generation Scholar 2022 for APAC, she champions diversity and academic excellence. She's also recognized as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded FEMCodes to empower women in STEM fields. Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggetsPrivacy Policy No, thanks!"
Sunshine And March Vibes (2025 Wallpapers Edition),https://smashingmagazine.com/2025/02/desktop-wallpaper-calendars-march-2025/,Smashing Magazine,2025-02-28T13:00:00,Smashing Magazine,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Cosima has been an editor at SmashingMag since 2013. Whenever she’s not writing articles for the weeklySmashing Newsletter, she’s probably working on a …More about
Cosima ↬ Weekly tips on front-end & UX.Trusted by 200,000+ folks. With the days getting noticeably longer in the northern hemisphere, the sun coming out, and the flowers blooming,March fuels us with fresh energy. And even if spring is far away in your part of the world, you might feel that 2025 has gained full speed by now — the perfect opportunity to put all those plans you’ve made and ideas you’ve been carrying around to action! To cater for some extra inspiration this March, artists and designers from across the globe once againchallenged their creative skillsand designed a new batch of desktop wallpapers to accompany you through the month. As every month, you’ll find their artworks compiled below — together with some timeless March favorites from ourarchivesthat are just too good to be forgotten. This post wouldn’t exist without thekind support of our wonderful communitywho diligently contributes their designs each month anew to keep the steady stream of wallpapers flowing. So, a huge thank-you to everyone who shared their artwork with us this time around! If you, too, would like toget featuredin one of our upcoming wallpapers posts, please don’t hesitate tojoin in. We can’t wait to see what you’ll come up with! Happy March! Bee-utiful SmileDesigned byDoreen Bethgefrom Germany.previewwith calendar:640x480,800x600,1024x768,1152x864,1280x720,1280x800,1280x960,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3200x2000without calendar:640x480,800x600,1024x768,1152x864,1280x720,1280x800,1280x960,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3200x2000 Designed byDoreen Bethgefrom Germany. Coffee BreakDesigned byRicardo Gimenesfrom Spain.previewwith calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160without calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Rosa Parks“March, the month of transition between winter and spring, is dedicated to Rosa Parks and her great phrase: ‘You must never be fearful about what you are doing when it is right.’” — Designed byVeronica Valenzuelafrom Spain.previewwith calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440without calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “March, the month of transition between winter and spring, is dedicated to Rosa Parks and her great phrase: ‘You must never be fearful about what you are doing when it is right.’” — Designed byVeronica Valenzuelafrom Spain. So TireDesigned byRicardo Gimenesfrom Spain.previewwith calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160without calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Hey UXer, it’s time to sharpen your design leadership skills.Learn to strategize, influence, and drive cultural changeeven with limited resources. Time To Wake Up“Rays of sunlight had cracked into the bear’s cave. He slowly opened one eye and caught a glimpse of nature in blossom. Is it spring already? Oh, but he is so sleepy. He doesn’t want to wake up, not just yet. So he continues dreaming about those sweet sluggish days while everything around him is blooming.” — Designed byPopArt Studiofrom Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Rays of sunlight had cracked into the bear’s cave. He slowly opened one eye and caught a glimpse of nature in blossom. Is it spring already? Oh, but he is so sleepy. He doesn’t want to wake up, not just yet. So he continues dreaming about those sweet sluggish days while everything around him is blooming.” — Designed byPopArt Studiofrom Serbia. Music From The PastDesigned byRicardo Gimenesfrom Spain.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Northern Lights“Spring is getting closer, and we are waiting for it with open arms. This month, we want to enjoy discovering the northern lights. To do so, we are going to Alaska, where we have the faithful company of our friend White Fang.” — Designed byVeronica Valenzuela Jimenezfrom Spain.previewwithout calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “Spring is getting closer, and we are waiting for it with open arms. This month, we want to enjoy discovering the northern lights. To do so, we are going to Alaska, where we have the faithful company of our friend White Fang.” — Designed byVeronica Valenzuela Jimenezfrom Spain. Queen Bee“Spring is coming! Birds are singing, flowers are blooming, bees are flying… Enjoy this month!” — Designed by Melissa Bogemans from Belgium.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Spring is coming! Birds are singing, flowers are blooming, bees are flying… Enjoy this month!” — Designed by Melissa Bogemans from Belgium. BotanicaDesigned byVlad Gerasimovfrom Georgia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 Designed byVlad Gerasimovfrom Georgia. Let’s Spring“After some freezing months, it’s time to enjoy the sun and flowers. It’s party time, colours are coming, so let’s spring!” — Designed byColorsferafrom Spain.previewwithout calendar:320x480,1024x768,1024x1024,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “After some freezing months, it’s time to enjoy the sun and flowers. It’s party time, colours are coming, so let’s spring!” — Designed byColorsferafrom Spain. Spring BirdDesigned byNathalie Ouedernifrom France.previewwithout calendar:1024x768,1280x1024,1440x900,1680x1200,1920x1200,2560x1440 Designed byNathalie Ouedernifrom France. Explore The Forest“This month, I want to go to the woods and explore my new world in sunny weather.” — Designed by Zi-Cing Hong from Taiwan.previewwithout calendar:1024x768,1152x864,1280x720,1280x800,1280x960,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440 “This month, I want to go to the woods and explore my new world in sunny weather.” — Designed by Zi-Cing Hong from Taiwan. Tacos To The Moon And BackDesigned byRicardo Gimenesfrom Spain.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Daydreaming“Daydreaming of better things, of lovely things, of saddening things.” — Designed byBhabna Basakfrom India.previewwithout calendar:1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Daydreaming of better things, of lovely things, of saddening things.” — Designed byBhabna Basakfrom India. Ballet“A day, even a whole month, isn’t enough to show how much a woman should be appreciated. Dear ladies, any day or month are yours if you decide so.” — Designed byAna Masnikosafrom Belgrade, Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1040,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “A day, even a whole month, isn’t enough to show how much a woman should be appreciated. Dear ladies, any day or month are yours if you decide so.” — Designed byAna Masnikosafrom Belgrade, Serbia. Awakening“I am the kind of person who prefers the cold but I do love spring since it’s the magical time when flowers and trees come back to life and fill the landscape with beautiful colors.” — Designed byMaria Kellerfrom Mexico.previewwithout calendar:320x480,640x480,640x1136,750x1334,800x480,800x600,1024x768,1024x1024,1152x864,1242x2208,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “I am the kind of person who prefers the cold but I do love spring since it’s the magical time when flowers and trees come back to life and fill the landscape with beautiful colors.” — Designed byMaria Kellerfrom Mexico. MARCHing Forward“If all you want is a little orange dinosaur MARCHing (okay, I think you get the pun) across your monitor, this wallpaper was made just for you! This little guy is my design buddy at the office and sits by (and sometimes on top of) my monitor. This is what happens when you have designer’s block and a DSLR.” — Designed by Paul Bupe Jr from Statesboro, GA.previewwithout calendar:1024x768,1280x1024,1440x900,1920x1080,1920x1200,2560x1440 “If all you want is a little orange dinosaur MARCHing (okay, I think you get the pun) across your monitor, this wallpaper was made just for you! This little guy is my design buddy at the office and sits by (and sometimes on top of) my monitor. This is what happens when you have designer’s block and a DSLR.” — Designed by Paul Bupe Jr from Statesboro, GA. Jingzhe“Jīngzhé is the third of the 24 solar terms in the traditional East Asian calendars. The word 驚蟄 means ‘the awakening of hibernating insects’. 驚 is ‘to start’ and 蟄 means ‘hibernating insects’. Traditional Chinese folklore says that during Jingzhe, thunderstorms will wake up the hibernating insects, which implies that the weather is getting warmer.” — Designed bySunny Hongfrom Taiwan.previewwithout calendar:800x600,1280x720,1280x1024,1366x768,1400x1050,1680x1200,1920x1080,2560x1440 “Jīngzhé is the third of the 24 solar terms in the traditional East Asian calendars. The word 驚蟄 means ‘the awakening of hibernating insects’. 驚 is ‘to start’ and 蟄 means ‘hibernating insects’. Traditional Chinese folklore says that during Jingzhe, thunderstorms will wake up the hibernating insects, which implies that the weather is getting warmer.” — Designed bySunny Hongfrom Taiwan. Fresh LemonsDesigned byNathalie Ouedernifrom France.previewwithout calendar:320x480,1024x768,1280x1024,1440x900,1600x1200,1680x1200,1920x1200,2560x1440 Designed byNathalie Ouedernifrom France. Pizza Time“Who needs an excuse to look at pizza all month?” — Designed byJames Mitchellfrom the United Kingdom.previewwithout calendar:1280x720,1280x800,1366x768,1440x900,1680x1050,1920x1080,1920x1200,2560x1440,2880x1800 “Who needs an excuse to look at pizza all month?” — Designed byJames Mitchellfrom the United Kingdom. Questions“Doodles are slowly becoming my trademark, so I just had to use them to express this phrase I’m fond of recently. A bit enigmatic, philosophical. Inspiring, isn’t it?” — Designed byMarta Paderewskafrom Poland.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Doodles are slowly becoming my trademark, so I just had to use them to express this phrase I’m fond of recently. A bit enigmatic, philosophical. Inspiring, isn’t it?” — Designed byMarta Paderewskafrom Poland. The Unknown“I made a connection, between the dark side and the unknown lighted and catchy area.” — Designed byValentin Keletifrom Romania.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “I made a connection, between the dark side and the unknown lighted and catchy area.” — Designed byValentin Keletifrom Romania. Waiting For Spring“As days are getting longer again and the first few flowers start to bloom, we are all waiting for spring to finally arrive.” — Designed by Naioo from Germany.previewwithout calendar:1280x800,1366x768,1440x900,1680x1050,1920x1080,1920x1200 “As days are getting longer again and the first few flowers start to bloom, we are all waiting for spring to finally arrive.” — Designed by Naioo from Germany. St. Patrick’s Day“On the 17th March, raise a glass and toast St. Patrick on St. Patrick’s Day, the Patron Saint of Ireland.” — Designed byEver Increasing Circlesfrom the United Kingdom.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1080x1080,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “On the 17th March, raise a glass and toast St. Patrick on St. Patrick’s Day, the Patron Saint of Ireland.” — Designed byEver Increasing Circlesfrom the United Kingdom. Spring Is Coming“This March, our calendar design epitomizes the heralds of spring. Soon enough, you’ll be waking up to the singing of swallows, in a room full of sunshine, filled with the empowering smell of daffodil, the first springtime flowers. Spring is the time of rebirth and new beginnings, creativity and inspiration, self-awareness, and inner reflection. Have a budding, thriving spring!” — Designed byPopArt Studiofrom Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1440x900,1440x1050,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “This March, our calendar design epitomizes the heralds of spring. Soon enough, you’ll be waking up to the singing of swallows, in a room full of sunshine, filled with the empowering smell of daffodil, the first springtime flowers. Spring is the time of rebirth and new beginnings, creativity and inspiration, self-awareness, and inner reflection. Have a budding, thriving spring!” — Designed byPopArt Studiofrom Serbia. Happy Birthday Dr. Seuss!“March 2nd marks the birthday of the most creative and extraordinary author ever, Dr. Seuss! I have included an inspirational quote about learning to encourage everyone to continue learning new things every day.” — Designed bySafia Begumfrom the United Kingdom.previewwithout calendar:800x450,1280x720,1366x768,1440x810,1600x900,1680x945,1920x1080,2560x1440 “March 2nd marks the birthday of the most creative and extraordinary author ever, Dr. Seuss! I have included an inspirational quote about learning to encourage everyone to continue learning new things every day.” — Designed bySafia Begumfrom the United Kingdom. Wake Up!“Early spring in March is for me the time when the snow melts, everything isn’t very colorful. This is what I wanted to show. Everything comes to life slowly, as this bear. Flowers are banal, so instead of a purple crocus we have a purple bird-harbinger.” — Designed byMarek Kedzierskifrom Poland.previewwithout calendar:320x480,1024x768,1280x720,1280x800,1280x960,1400x1050,1600x1200,1680x1050,1920x1080,1920x1200,2560x1440 “Early spring in March is for me the time when the snow melts, everything isn’t very colorful. This is what I wanted to show. Everything comes to life slowly, as this bear. Flowers are banal, so instead of a purple crocus we have a purple bird-harbinger.” — Designed byMarek Kedzierskifrom Poland. Spring Is Inevitable“Spring is round the corner. And very soon plants will grow on some other planets too. Let’s be happy about a new cycle of life.” — Designed byIgor Izhikfrom Canada.previewwithout calendar:1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,2560x1600 “Spring is round the corner. And very soon plants will grow on some other planets too. Let’s be happy about a new cycle of life.” — Designed byIgor Izhikfrom Canada. Traveling To Neverland“This month we become children and we travel with Peter Pan. Let’s go to Neverland!” — Designed byVeronica Valenzuelafrom Spain.previewwithout calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “This month we become children and we travel with Peter Pan. Let’s go to Neverland!” — Designed byVeronica Valenzuelafrom Spain. Let’s Get OutsideDesigned byLívia Lénártfrom Hungary.previewwithout calendar:1024x768,1280x1024,1366x768,1600x1200,1680x1200,1920x1080,1920x1200,2560x1440 Designed byLívia Lénártfrom Hungary.  Useful front-end & UX bits, delivered once a week.With tools to help you get your work done better. Subscribe and get Vitaly’sSmart Interface Design Checklists PDFvia email. 🎁Your (smashing) emailOnfront-end & UX. Trusted by 207,000+ folks. Useful front-end & UX bits, delivered once a week. With tools to help you get your work done better. Subscribe and get Vitaly’sSmart Interface Design Checklists PDFvia email. 🎁 Onfront-end & UX. Trusted by 207,000+ folks. Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. With practical takeaways, live sessions, video recordings and a friendly Q&A. Everything TypeScript, with code walkthroughs and examples. And other printed books.","Cosima has been an editor at SmashingMag since 2013. Whenever she’s not writing articles for the weeklySmashing Newsletter, she’s probably working on a …More about
Cosima ↬ Weekly tips on front-end & UX.Trusted by 200,000+ folks. With the days getting noticeably longer in the northern hemisphere, the sun coming out, and the flowers blooming,March fuels us with fresh energy. And even if spring is far away in your part of the world, you might feel that 2025 has gained full speed by now — the perfect opportunity to put all those plans you’ve made and ideas you’ve been carrying around to action! To cater for some extra inspiration this March, artists and designers from across the globe once againchallenged their creative skillsand designed a new batch of desktop wallpapers to accompany you through the month. As every month, you’ll find their artworks compiled below — together with some timeless March favorites from ourarchivesthat are just too good to be forgotten. This post wouldn’t exist without thekind support of our wonderful communitywho diligently contributes their designs each month anew to keep the steady stream of wallpapers flowing. So, a huge thank-you to everyone who shared their artwork with us this time around! If you, too, would like toget featuredin one of our upcoming wallpapers posts, please don’t hesitate tojoin in. We can’t wait to see what you’ll come up with! Happy March! Bee-utiful SmileDesigned byDoreen Bethgefrom Germany.previewwith calendar:640x480,800x600,1024x768,1152x864,1280x720,1280x800,1280x960,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3200x2000without calendar:640x480,800x600,1024x768,1152x864,1280x720,1280x800,1280x960,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3200x2000 Designed byDoreen Bethgefrom Germany. Coffee BreakDesigned byRicardo Gimenesfrom Spain.previewwith calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160without calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Rosa Parks“March, the month of transition between winter and spring, is dedicated to Rosa Parks and her great phrase: ‘You must never be fearful about what you are doing when it is right.’” — Designed byVeronica Valenzuelafrom Spain.previewwith calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440without calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “March, the month of transition between winter and spring, is dedicated to Rosa Parks and her great phrase: ‘You must never be fearful about what you are doing when it is right.’” — Designed byVeronica Valenzuelafrom Spain. So TireDesigned byRicardo Gimenesfrom Spain.previewwith calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160without calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Hey UXer, it’s time to sharpen your design leadership skills.Learn to strategize, influence, and drive cultural changeeven with limited resources. Time To Wake Up“Rays of sunlight had cracked into the bear’s cave. He slowly opened one eye and caught a glimpse of nature in blossom. Is it spring already? Oh, but he is so sleepy. He doesn’t want to wake up, not just yet. So he continues dreaming about those sweet sluggish days while everything around him is blooming.” — Designed byPopArt Studiofrom Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Rays of sunlight had cracked into the bear’s cave. He slowly opened one eye and caught a glimpse of nature in blossom. Is it spring already? Oh, but he is so sleepy. He doesn’t want to wake up, not just yet. So he continues dreaming about those sweet sluggish days while everything around him is blooming.” — Designed byPopArt Studiofrom Serbia. Music From The PastDesigned byRicardo Gimenesfrom Spain.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Northern Lights“Spring is getting closer, and we are waiting for it with open arms. This month, we want to enjoy discovering the northern lights. To do so, we are going to Alaska, where we have the faithful company of our friend White Fang.” — Designed byVeronica Valenzuela Jimenezfrom Spain.previewwithout calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “Spring is getting closer, and we are waiting for it with open arms. This month, we want to enjoy discovering the northern lights. To do so, we are going to Alaska, where we have the faithful company of our friend White Fang.” — Designed byVeronica Valenzuela Jimenezfrom Spain. Queen Bee“Spring is coming! Birds are singing, flowers are blooming, bees are flying… Enjoy this month!” — Designed by Melissa Bogemans from Belgium.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Spring is coming! Birds are singing, flowers are blooming, bees are flying… Enjoy this month!” — Designed by Melissa Bogemans from Belgium. BotanicaDesigned byVlad Gerasimovfrom Georgia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 Designed byVlad Gerasimovfrom Georgia. Let’s Spring“After some freezing months, it’s time to enjoy the sun and flowers. It’s party time, colours are coming, so let’s spring!” — Designed byColorsferafrom Spain.previewwithout calendar:320x480,1024x768,1024x1024,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “After some freezing months, it’s time to enjoy the sun and flowers. It’s party time, colours are coming, so let’s spring!” — Designed byColorsferafrom Spain. Spring BirdDesigned byNathalie Ouedernifrom France.previewwithout calendar:1024x768,1280x1024,1440x900,1680x1200,1920x1200,2560x1440 Designed byNathalie Ouedernifrom France. Explore The Forest“This month, I want to go to the woods and explore my new world in sunny weather.” — Designed by Zi-Cing Hong from Taiwan.previewwithout calendar:1024x768,1152x864,1280x720,1280x800,1280x960,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1920x1080,1920x1200,1920x1440,2560x1440 “This month, I want to go to the woods and explore my new world in sunny weather.” — Designed by Zi-Cing Hong from Taiwan. Tacos To The Moon And BackDesigned byRicardo Gimenesfrom Spain.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,3840x2160 Designed byRicardo Gimenesfrom Spain. Daydreaming“Daydreaming of better things, of lovely things, of saddening things.” — Designed byBhabna Basakfrom India.previewwithout calendar:1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Daydreaming of better things, of lovely things, of saddening things.” — Designed byBhabna Basakfrom India. Ballet“A day, even a whole month, isn’t enough to show how much a woman should be appreciated. Dear ladies, any day or month are yours if you decide so.” — Designed byAna Masnikosafrom Belgrade, Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1040,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “A day, even a whole month, isn’t enough to show how much a woman should be appreciated. Dear ladies, any day or month are yours if you decide so.” — Designed byAna Masnikosafrom Belgrade, Serbia. Awakening“I am the kind of person who prefers the cold but I do love spring since it’s the magical time when flowers and trees come back to life and fill the landscape with beautiful colors.” — Designed byMaria Kellerfrom Mexico.previewwithout calendar:320x480,640x480,640x1136,750x1334,800x480,800x600,1024x768,1024x1024,1152x864,1242x2208,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “I am the kind of person who prefers the cold but I do love spring since it’s the magical time when flowers and trees come back to life and fill the landscape with beautiful colors.” — Designed byMaria Kellerfrom Mexico. MARCHing Forward“If all you want is a little orange dinosaur MARCHing (okay, I think you get the pun) across your monitor, this wallpaper was made just for you! This little guy is my design buddy at the office and sits by (and sometimes on top of) my monitor. This is what happens when you have designer’s block and a DSLR.” — Designed by Paul Bupe Jr from Statesboro, GA.previewwithout calendar:1024x768,1280x1024,1440x900,1920x1080,1920x1200,2560x1440 “If all you want is a little orange dinosaur MARCHing (okay, I think you get the pun) across your monitor, this wallpaper was made just for you! This little guy is my design buddy at the office and sits by (and sometimes on top of) my monitor. This is what happens when you have designer’s block and a DSLR.” — Designed by Paul Bupe Jr from Statesboro, GA. Jingzhe“Jīngzhé is the third of the 24 solar terms in the traditional East Asian calendars. The word 驚蟄 means ‘the awakening of hibernating insects’. 驚 is ‘to start’ and 蟄 means ‘hibernating insects’. Traditional Chinese folklore says that during Jingzhe, thunderstorms will wake up the hibernating insects, which implies that the weather is getting warmer.” — Designed bySunny Hongfrom Taiwan.previewwithout calendar:800x600,1280x720,1280x1024,1366x768,1400x1050,1680x1200,1920x1080,2560x1440 “Jīngzhé is the third of the 24 solar terms in the traditional East Asian calendars. The word 驚蟄 means ‘the awakening of hibernating insects’. 驚 is ‘to start’ and 蟄 means ‘hibernating insects’. Traditional Chinese folklore says that during Jingzhe, thunderstorms will wake up the hibernating insects, which implies that the weather is getting warmer.” — Designed bySunny Hongfrom Taiwan. Fresh LemonsDesigned byNathalie Ouedernifrom France.previewwithout calendar:320x480,1024x768,1280x1024,1440x900,1600x1200,1680x1200,1920x1200,2560x1440 Designed byNathalie Ouedernifrom France. Pizza Time“Who needs an excuse to look at pizza all month?” — Designed byJames Mitchellfrom the United Kingdom.previewwithout calendar:1280x720,1280x800,1366x768,1440x900,1680x1050,1920x1080,1920x1200,2560x1440,2880x1800 “Who needs an excuse to look at pizza all month?” — Designed byJames Mitchellfrom the United Kingdom. Questions“Doodles are slowly becoming my trademark, so I just had to use them to express this phrase I’m fond of recently. A bit enigmatic, philosophical. Inspiring, isn’t it?” — Designed byMarta Paderewskafrom Poland.previewwithout calendar:640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “Doodles are slowly becoming my trademark, so I just had to use them to express this phrase I’m fond of recently. A bit enigmatic, philosophical. Inspiring, isn’t it?” — Designed byMarta Paderewskafrom Poland. The Unknown“I made a connection, between the dark side and the unknown lighted and catchy area.” — Designed byValentin Keletifrom Romania.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “I made a connection, between the dark side and the unknown lighted and catchy area.” — Designed byValentin Keletifrom Romania. Waiting For Spring“As days are getting longer again and the first few flowers start to bloom, we are all waiting for spring to finally arrive.” — Designed by Naioo from Germany.previewwithout calendar:1280x800,1366x768,1440x900,1680x1050,1920x1080,1920x1200 “As days are getting longer again and the first few flowers start to bloom, we are all waiting for spring to finally arrive.” — Designed by Naioo from Germany. St. Patrick’s Day“On the 17th March, raise a glass and toast St. Patrick on St. Patrick’s Day, the Patron Saint of Ireland.” — Designed byEver Increasing Circlesfrom the United Kingdom.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1080x1080,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “On the 17th March, raise a glass and toast St. Patrick on St. Patrick’s Day, the Patron Saint of Ireland.” — Designed byEver Increasing Circlesfrom the United Kingdom. Spring Is Coming“This March, our calendar design epitomizes the heralds of spring. Soon enough, you’ll be waking up to the singing of swallows, in a room full of sunshine, filled with the empowering smell of daffodil, the first springtime flowers. Spring is the time of rebirth and new beginnings, creativity and inspiration, self-awareness, and inner reflection. Have a budding, thriving spring!” — Designed byPopArt Studiofrom Serbia.previewwithout calendar:320x480,640x480,800x480,800x600,1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1366x768,1440x900,1440x1050,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440 “This March, our calendar design epitomizes the heralds of spring. Soon enough, you’ll be waking up to the singing of swallows, in a room full of sunshine, filled with the empowering smell of daffodil, the first springtime flowers. Spring is the time of rebirth and new beginnings, creativity and inspiration, self-awareness, and inner reflection. Have a budding, thriving spring!” — Designed byPopArt Studiofrom Serbia. Happy Birthday Dr. Seuss!“March 2nd marks the birthday of the most creative and extraordinary author ever, Dr. Seuss! I have included an inspirational quote about learning to encourage everyone to continue learning new things every day.” — Designed bySafia Begumfrom the United Kingdom.previewwithout calendar:800x450,1280x720,1366x768,1440x810,1600x900,1680x945,1920x1080,2560x1440 “March 2nd marks the birthday of the most creative and extraordinary author ever, Dr. Seuss! I have included an inspirational quote about learning to encourage everyone to continue learning new things every day.” — Designed bySafia Begumfrom the United Kingdom. Wake Up!“Early spring in March is for me the time when the snow melts, everything isn’t very colorful. This is what I wanted to show. Everything comes to life slowly, as this bear. Flowers are banal, so instead of a purple crocus we have a purple bird-harbinger.” — Designed byMarek Kedzierskifrom Poland.previewwithout calendar:320x480,1024x768,1280x720,1280x800,1280x960,1400x1050,1600x1200,1680x1050,1920x1080,1920x1200,2560x1440 “Early spring in March is for me the time when the snow melts, everything isn’t very colorful. This is what I wanted to show. Everything comes to life slowly, as this bear. Flowers are banal, so instead of a purple crocus we have a purple bird-harbinger.” — Designed byMarek Kedzierskifrom Poland. Spring Is Inevitable“Spring is round the corner. And very soon plants will grow on some other planets too. Let’s be happy about a new cycle of life.” — Designed byIgor Izhikfrom Canada.previewwithout calendar:1024x768,1024x1024,1152x864,1280x720,1280x800,1280x960,1280x1024,1400x1050,1440x900,1600x1200,1680x1050,1680x1200,1920x1080,1920x1200,1920x1440,2560x1440,2560x1600 “Spring is round the corner. And very soon plants will grow on some other planets too. Let’s be happy about a new cycle of life.” — Designed byIgor Izhikfrom Canada. Traveling To Neverland“This month we become children and we travel with Peter Pan. Let’s go to Neverland!” — Designed byVeronica Valenzuelafrom Spain.previewwithout calendar:640x480,800x480,1024x768,1280x720,1280x800,1440x900,1600x1200,1920x1080,1920x1440,2560x1440 “This month we become children and we travel with Peter Pan. Let’s go to Neverland!” — Designed byVeronica Valenzuelafrom Spain. Let’s Get OutsideDesigned byLívia Lénártfrom Hungary.previewwithout calendar:1024x768,1280x1024,1366x768,1600x1200,1680x1200,1920x1080,1920x1200,2560x1440 Designed byLívia Lénártfrom Hungary.  Useful front-end & UX bits, delivered once a week.With tools to help you get your work done better. Subscribe and get Vitaly’sSmart Interface Design Checklists PDFvia email. 🎁Your (smashing) emailOnfront-end & UX. Trusted by 207,000+ folks. Useful front-end & UX bits, delivered once a week. With tools to help you get your work done better. Subscribe and get Vitaly’sSmart Interface Design Checklists PDFvia email. 🎁 Onfront-end & UX. Trusted by 207,000+ folks. Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. With practical takeaways, live sessions, video recordings and a friendly Q&A. Everything TypeScript, with code walkthroughs and examples. And other printed books.",Sunshine And March Vibes (2025 Wallpapers Edition),"

Key Points:
",UI/UX,"<p>With the days getting noticeably longer in the northern hemisphere, the sun coming out, and the flowers blooming, <strong>March fuels us with fresh energy</strong>. And even if spring is far away in your part of the world, you might feel that 2025 has gained full speed by now — the perfect opportunity to put all those plans you’ve made and ideas you’ve been carrying around to action!</p>

<p>To cater for some extra inspiration this March, artists and designers from across the globe once again <strong>challenged their creative skills</strong> and designed a new batch of desktop wallpapers to accompany you through the month. As every month, you’ll find their artworks compiled below — together with some timeless March favorites from our <a href=""https://www.smashingmagazine.com/category/wallpapers"">archives</a> that are just too good to be forgotten.</p>

<p>This post wouldn’t exist without the <strong>kind support of our wonderful community</strong> who diligently contributes their designs each month anew to keep the steady stream of wallpapers flowing. So, a huge thank-you to everyone who shared their artwork with us this time around! If you, too, would like to <strong>get featured</strong> in one of our upcoming wallpapers posts, please don’t hesitate to <a href=""https://www.smashingmagazine.com/desktop-wallpaper-calendars-join-in/"">join in</a>. We can’t wait to see what you’ll come up with! Happy March!</p>

<ul>
<li>You can <strong>click on every image to see a larger preview</strong>.</li>
<li>We respect and carefully consider the ideas and motivation behind each and every artist’s work. This is why we give all artists the <strong>full freedom to explore their creativity</strong> and express emotions and experience through their works. This is also why the themes of the wallpapers weren’t anyhow influenced by us but rather designed from scratch by the artists themselves.</li>
<li><strong><a href=""https://www.smashingmagazine.com/desktop-wallpaper-calendars-join-in/"">Submit your wallpaper design!</a></strong> 👩‍🎨<br />Feeling inspired? We are always <strong>looking for creative talent</strong> and would love to feature <em>your</em> desktop wallpaper in one of our upcoming posts. <a href=""https://www.smashingmagazine.com/desktop-wallpaper-calendars-join-in/"">Join in ↬</a></li>
</ul>

<p></p>Bee-utiful Smile<p></p>
<p></p><p>Designed by <a href=""https://www.doreenbethge.de/"">Doreen Bethge</a> from Germany.</p><p></p>
<p></p><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/mar-25-bee-utiful-smile-full.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-25-bee-utiful-smile-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/mar-25-bee-utiful-smile-preview.png"">preview</a></li>
<li>with calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/cal/mar-25-bee-utiful-smile-cal-3200x2000.png"">3200x2000</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/bee-utiful-smile/nocal/mar-25-bee-utiful-smile-nocal-3200x2000.png"">3200x2000</a></li>
</ul>

<p></p>Coffee Break<p></p>
<p></p><p>Designed by <a href=""https://www.ricardogimenes.com/"">Ricardo Gimenes</a> from Spain.</p><p></p>
<p></p><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/mar-25-coffee-break-full.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-25-coffee-break-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/mar-25-coffee-break-preview.png"">preview</a></li>
<li>with calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1024x1024.png"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1280x1024.png"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1366x768.png"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1680x1200.png"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/cal/mar-25-coffee-break-cal-3840x2160.png"">3840x2160</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1366x768.png"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/coffee-break/nocal/mar-25-coffee-break-nocal-3840x2160.png"">3840x2160</a></li>
</ul>

<p></p>Rosa Parks<p></p>
<p></p><p>“March, the month of transition between winter and spring, is dedicated to Rosa Parks and her great phrase: ‘You must never be fearful about what you are doing when it is right.’” — Designed by <a href=""https://www.silocreativo.com/en/"">Veronica Valenzuela</a> from Spain.</p><p></p>
<p></p><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/mar-25-rosa-parks-full.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-25-rosa-parks-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/mar-25-rosa-parks-preview.png"">preview</a></li>
<li>with calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/cal/mar-25-rosa-parks-cal-2560x1440.png"">2560x1440</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/rosa-parks/nocal/mar-25-rosa-parks-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>So Tire<p></p>
<p></p><p>Designed by <a href=""https://www.ricardogimenes.com/"">Ricardo Gimenes</a> from Spain.</p><p></p>
<p></p><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/mar-25-so-tire-full.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-25-so-tire-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/mar-25-so-tire-preview.png"">preview</a></li>
<li>with calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1024x1024.png"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1280x1024.png"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1366x768.png"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1680x1200.png"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/cal/mar-25-so-tire-cal-3840x2160.png"">3840x2160</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1366x768.png"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-25/so-tire/nocal/mar-25-so-tire-nocal-3840x2160.png"">3840x2160</a></li>
</ul>



<p></p>Time To Wake Up<p></p>
<p></p><p>“Rays of sunlight had cracked into the bear’s cave. He slowly opened one eye and caught a glimpse of nature in blossom. Is it spring already? Oh, but he is so sleepy. He doesn’t want to wake up, not just yet. So he continues dreaming about those sweet sluggish days while everything around him is blooming.” — Designed by <a href=""https://www.popwebdesign.net/index_eng.html"">PopArt Studio</a> from Serbia.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/79516b2e-d9c6-4ab3-985b-e45eac96be42/mar-19-time-to-wake-up-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/595f9f16-ceb3-4d9b-8284-fc7a3f493295/mar-19-time-to-wake-up-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/595f9f16-ceb3-4d9b-8284-fc7a3f493295/mar-19-time-to-wake-up-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-640x480.jpg"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-800x480.jpg"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-800x600.jpg"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/time-to-wake-up/nocal/mar-19-time-to-wake-up-nocal-2560x1440.jpg"">2560x1440</a></li></ul>

<p></p>Music From The Past<p></p>
<p></p><p>Designed by <a href=""https://www.ricardogimenes.com/"">Ricardo Gimenes</a> from Spain.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-music-from-the-past-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-music-from-the-past-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-music-from-the-past-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-800x600.png"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1152x864.png"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1280x960.png"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1366x768.png"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/music-from-the-past/nocal/mar-24-music-from-the-past-nocal-3840x2160.png"">3840x2160</a></li>
</ul>

<p></p>Northern Lights<p></p>
<p></p><p>“Spring is getting closer, and we are waiting for it with open arms. This month, we want to enjoy discovering the northern lights. To do so, we are going to Alaska, where we have the faithful company of our friend White Fang.” — Designed by <a href=""https://www.silocreativo.com/en"">Veronica Valenzuela Jimenez</a> from Spain.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-northern-lights-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-northern-lights-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-24-northern-lights-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-24/northern-lights/nocal/mar-24-northern-lights-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Queen Bee<p></p>
<p></p><p>“Spring is coming! Birds are singing, flowers are blooming, bees are flying… Enjoy this month!” — Designed by Melissa Bogemans from Belgium.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/9c43d932-e478-4d77-95d8-c4ae8d8c80f0/mar-19-queen-bee-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/885ef955-c69b-4d99-b5fc-286f35b6e0e3/mar-19-queen-bee-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/885ef955-c69b-4d99-b5fc-286f35b6e0e3/mar-19-queen-bee-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-320x480.png"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-640x480.png"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-800x480.png"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-800x600.png"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1152x864.png"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-19/queen-bee/nocal/mar-19-queen-bee-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Botanica<p></p>
<p></p><p>Designed by <a href=""https://vlad.studio/"">Vlad Gerasimov</a> from Georgia.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/98a1c385-d14e-49bc-91c5-df5551b37799/mar-21-botanica-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/74e88326-57b4-4d3e-a31a-dc718b75c3a4/mar-21-botanica-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/74e88326-57b4-4d3e-a31a-dc718b75c3a4/mar-21-botanica-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-640x480.jpg"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-800x480.jpg"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-800x600.jpg"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/botanica/nocal/mar-21-botanica-nocal-2560x1440.jpg"">2560x1440</a></li>
</ul>

<p></p>Let’s Spring<p></p>
<p></p><p>“After some freezing months, it’s time to enjoy the sun and flowers. It’s party time, colours are coming, so let’s spring!” — Designed by <a href=""https://www.colorsfera.com"">Colorsfera</a> from Spain.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/5aedaccc-20d2-4802-9796-ae246644813f/march-15-lets-spring-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/46e37736-fa97-4d04-9ac1-921571a5d844/march-15-lets-spring-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/46e37736-fa97-4d04-9ac1-921571a5d844/march-15-lets-spring-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-320x480.png"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/lets-spring/nocal/march-15-lets-spring-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Spring Bird<p></p>
<p></p><p>Designed by <a href=""https://www.nathalieouederni.com"">Nathalie Ouederni</a> from France.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4992d67d-5607-46fb-afc5-edb09be2cc30/mar-17-spring-bird-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/062168cd-738b-4836-bc6f-26e8e49d917b/mar-17-spring-bird-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/062168cd-738b-4836-bc6f-26e8e49d917b/mar-17-spring-bird-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/spring-bird/nocal/mar-17-spring-bird-nocal-2560x1440.jpg"">2560x1440</a></li>
</ul>

<p></p>Explore The Forest<p></p>
<p></p><p>“This month, I want to go to the woods and explore my new world in sunny weather.” — Designed by Zi-Cing Hong from Taiwan.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8cd5d4f3-bbf7-44cf-823c-a3ea3a1ad477/mar-18-explore-the-forest-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b9f6c238-b0dc-4c83-8608-225fbc102c53/mar-18-explore-the-forest-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b9f6c238-b0dc-4c83-8608-225fbc102c53/mar-18-explore-the-forest-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1152x864.png"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/explore-the-forest/nocal/mar-18-explore-the-forest-nocal-2560x1440.png"">2560x1440</a></li> </ul>

<p></p>Tacos To The Moon And Back<p></p>
<p></p><p>Designed by <a href=""https://www.ricardogimenes.com/"">Ricardo Gimenes</a> from Spain.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/a8d8604c-db94-4a00-876b-d5bb83481777/mar-21-to-the-moon-and-back-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b015acab-cb24-41bf-95a9-2c48db73bb65/mar-21-to-the-moon-and-back-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b015acab-cb24-41bf-95a9-2c48db73bb65/mar-21-to-the-moon-and-back-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-640x480.png"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-800x480.png"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-800x600.png"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1152x864.png"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/to-the-moon-and-back/nocal/mar-21-to-the-moon-and-back-nocal-3840x2160.png"">3840x2160</a></li>
</ul>



<p></p>Daydreaming<p></p>
<p></p><p>“Daydreaming of better things, of lovely things, of saddening things.” — Designed by <a href=""https://design-studio.io/"">Bhabna Basak</a> from India.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-23-daydreaming-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-23-daydreaming-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-23-daydreaming-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-23/daydreaming/nocal/mar-23-daydreaming-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Ballet<p></p>
<p></p><p>“A day, even a whole month, isn’t enough to show how much a woman should be appreciated. Dear ladies, any day or month are yours if you decide so.” — Designed by <a href=""https://www.creitive.com/"">Ana Masnikosa</a> from Belgrade, Serbia.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/3644cf1c-73ff-4407-b5ad-9107f25220cc/mar-17-ballet-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/c18c06c4-3981-490d-addc-e91f24ae0ccd/mar-17-ballet-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/c18c06c4-3981-490d-addc-e91f24ae0ccd/mar-17-ballet-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-320x480.png"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-640x480.png"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-800x480.png"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-800x600.png"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1152x864.png"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1280x1040.png"">1280x1040</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/ballet/nocal/mar-17-ballet-nocal-2560x1440.png"">2560x1440</a></li></ul>

<p></p>Awakening<p></p>
<p></p><p>“I am the kind of person who prefers the cold but I do love spring since it’s the magical time when flowers and trees come back to life and fill the landscape with beautiful colors.” — Designed by <a href=""https://www.mariakellerac.com"">Maria Keller</a> from Mexico.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0a961483-4838-4bea-99f5-8939ffdff62e/mar-15-wake-up-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/a44be9ea-24bd-4614-aa0d-c45b03012f43/mar-15-wake-up-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/a44be9ea-24bd-4614-aa0d-c45b03012f43/mar-15-wake-up-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-640x480.jpg"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-640x1136.jpg"">640x1136</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-750x1334.jpg"">750x1334</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-800x480.jpg"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-800x600.jpg"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1242x2208.jpg"">1242x2208</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/wake-up/nocal/mar-15-wake-up-nocal-2560x1440.jpg"">2560x1440</a></li>
</ul>

<p></p>MARCHing Forward<p></p>
<p></p><p>“If all you want is a little orange dinosaur MARCHing (okay, I think you get the pun) across your monitor, this wallpaper was made just for you! This little guy is my design buddy at the office and sits by (and sometimes on top of) my monitor. This is what happens when you have designer’s block and a DSLR.” — Designed by Paul Bupe Jr from Statesboro, GA.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/280c36ae-ad54-439e-8fee-1708a25b6a81/mar-14-marching-forward-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/44a39397-6c4b-4c6c-96ee-b7ca060c3e1e/mar-14-marching-forward-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/44a39397-6c4b-4c6c-96ee-b7ca060c3e1e/mar-14-marching-forward-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-14/marching-forward/nocal/mar-14-marching-forward-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Jingzhe<p></p>
<p></p><p>“Jīngzhé is the third of the 24 solar terms in the traditional East Asian calendars. The word 驚蟄 means ‘the awakening of hibernating insects’. 驚 is ‘to start’ and 蟄 means ‘hibernating insects’. Traditional Chinese folklore says that during Jingzhe, thunderstorms will wake up the hibernating insects, which implies that the weather is getting warmer.” — Designed by <a href=""https://www.pixiv.net/member.php?id=8059765"">Sunny Hong</a> from Taiwan.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-17-jingzhe-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-17-jingzhe-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-17-jingzhe-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-800x600.png"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/jingzhe/nocal/mar-17-jingzhe-nocal-2560x1440.png"">2560x1440</a></li></ul>

<p></p>Fresh Lemons<p></p>
<p></p><p>Designed by <a href=""https://www.nathalieouederni.com/"">Nathalie Ouederni</a> from France.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4de34052-8b80-4845-b76c-2ae71e6d94b4/mar-15-fresh-lemons-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/1eb6ae3d-a3a0-4107-8b61-1995b717749c/mar-15-fresh-lemons-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/1eb6ae3d-a3a0-4107-8b61-1995b717749c/mar-15-fresh-lemons-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-15/fresh-lemons/nocal/mar-15-fresh-lemons-nocal-2560x1440.jpg"">2560x1440</a></li></ul>

<p></p>Pizza Time<p></p>
<p></p><p>“Who needs an excuse to look at pizza all month?” — Designed by <a href=""https://www.behance.net/jamesmitchell23"">James Mitchell</a> from the United Kingdom.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4049a0bc-303d-4a8a-8bc5-028fc93d6643/mar-17-pizza-time-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/08e46754-3ebf-436e-b870-7738acf14494/mar-17-pizza-time-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/08e46754-3ebf-436e-b870-7738acf14494/mar-17-pizza-time-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-2560x1440.png"">2560x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/pizza-time/nocal/mar-17-pizza-time-nocal-2880x1800.png"">2880x1800</a></li></ul>

<p></p>Questions<p></p>
<p></p><p>“Doodles are slowly becoming my trademark, so I just had to use them to express this phrase I’m fond of recently. A bit enigmatic, philosophical. Inspiring, isn’t it?” — Designed by <a href=""https://www.behance.net/marpad"">Marta Paderewska</a> from Poland.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/50511280-eddb-4e2c-98e1-51cf72219eca/mar-16-questions-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/55440170-e0d7-427c-b21f-d943b8ee7df0/mar-16-questions-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/55440170-e0d7-427c-b21f-d943b8ee7df0/mar-16-questions-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-640x480.png"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-800x480.png"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-800x600.png"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1024x1024.png"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1152x864.png"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1280x1024.png"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1440x900.png"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1680x1200.png"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/questions/nocal/mar-16-questions-nocal-2560x1440.png"">2560x1440</a></li></ul>

<p></p>The Unknown<p></p>
<p></p><p>“I made a connection, between the dark side and the unknown lighted and catchy area.” — Designed by <a href=""https://www.behance.net/valentinkeleti"">Valentin Keleti</a> from Romania.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/42049be6-738a-4751-89a3-c661805f0c1a/mar-18-the-unknown-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/55fd74da-c62f-4c39-a0ad-faa8b649df8a/mar-18-the-unknown-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/55fd74da-c62f-4c39-a0ad-faa8b649df8a/mar-18-the-unknown-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-640x480.jpg"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-800x480.jpg"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-800x600.jpg"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/the-unknown/nocal/mar-18-the-unknown-nocal-2560x1440.jpg"">2560x1440</a></li> </ul>



<p></p>Waiting For Spring<p></p>
<p></p><p>“As days are getting longer again and the first few flowers start to bloom, we are all waiting for spring to finally arrive.” — Designed by Naioo from Germany.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8e54c2a3-a50c-4624-a055-0f8edfe8e203/march-12-waiting-for-spring-15-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/80ae8715-1dcc-4a97-9fcf-f39f2374b148/march-12-waiting-for-spring-15-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/80ae8715-1dcc-4a97-9fcf-f39f2374b148/march-12-waiting-for-spring-15-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/march-12/march-12-waiting_for_spring__15-nocal-1920x1200.jpg"">1920x1200</a></li></ul>

<p></p>St. Patrick’s Day<p></p>
<p></p><p>“On the 17th March, raise a glass and toast St. Patrick on St. Patrick’s Day, the Patron Saint of Ireland.” — Designed by <a href=""https://www.everincreasingcircles.com/"">Ever Increasing Circles</a> from the United Kingdom.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/b2ceb983-8187-411d-aa06-8773b397e5f3/mar-21-st-patricks-day-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/eecc7027-ca1c-4dc9-bc69-eafa3d6ed56f/mar-21-st-patricks-day-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/eecc7027-ca1c-4dc9-bc69-eafa3d6ed56f/mar-21-st-patricks-day-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-320x480.jpg"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-640x480.jpg"">640x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-800x480.jpg"">800x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-800x600.jpg"">800x600</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1080x1080.jpg"">1080x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-21/st-patricks-day/nocal/mar-21-st-patricks-day-nocal-2560x1440.jpg"">2560x1440</a></li>
</ul>

<p></p>Spring Is Coming<p></p>
<p></p><p>“This March, our calendar design epitomizes the heralds of spring. Soon enough, you’ll be waking up to the singing of swallows, in a room full of sunshine, filled with the empowering smell of daffodil, the first springtime flowers. Spring is the time of rebirth and new beginnings, creativity and inspiration, self-awareness, and inner reflection. Have a budding, thriving spring!” — Designed by <a href=""https://www.popwebdesign.net/illustrations.html"">PopArt Studio</a> from Serbia.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-22-spring-is-coming-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-22-spring-is-coming-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2023/mar-22-spring-is-coming-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-320x480.jpg"">320x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-640x480.jpg"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-800x480.jpg"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-800x600.jpg"">800x600</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1440x1050.jpg"">1440x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/spring-is-coming/nocal/mar-22-spring-is-coming-nocal-2560x1440.jpg"">2560x1440</a></li>
</ul>

<p></p>Happy Birthday Dr. Seuss!<p></p>
<p></p><p>“March 2nd marks the birthday of the most creative and extraordinary author ever, Dr. Seuss! I have included an inspirational quote about learning to encourage everyone to continue learning new things every day.” — Designed by <a href=""https://www.safiabegum.com/"">Safia Begum</a> from the United Kingdom.&lt;/p</p>
<blockquote>
<a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/c44c6a7a-2ab2-4a54-bf9a-d7ca5a616066/mar-17-happy-birthday-dr-seuss-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/d3c8f6c7-dfce-4108-bbfe-b9f4ada9ec56/mar-17-happy-birthday-dr-seuss-preview-opt.png"" /></a>
</blockquote>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/d3c8f6c7-dfce-4108-bbfe-b9f4ada9ec56/mar-17-happy-birthday-dr-seuss-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-800x450.png"">800x450</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1366x768.png"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1440x810.png"">1440x810</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1600x900.png"">1600x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1680x945.png"">1680x945</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/happy-birthday-dr-seuss/nocal/mar-17-happy-birthday-dr-seuss-nocal-2560x1440.png"">2560x1440</a></li></ul>

<p></p>Wake Up!<p></p>
<p></p><p>“Early spring in March is for me the time when the snow melts, everything isn’t very colorful. This is what I wanted to show. Everything comes to life slowly, as this bear. Flowers are banal, so instead of a purple crocus we have a purple bird-harbinger.” — Designed by <a href=""https://www.behance.net/rustbeard"">Marek Kedzierski</a> from Poland.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/c70e4782-c406-4443-94aa-99fa636bd8ac/mar-17-wake-up-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0755cfd9-a45d-4767-b93f-b0b9f5411c94/mar-17-wake-up-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/0755cfd9-a45d-4767-b93f-b0b9f5411c94/mar-17-wake-up-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-320x480.png"">320x480</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1024x768.png"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1280x720.png"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1280x800.png"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1280x960.png"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1400x1050.png"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1680x1050.png"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-1920x1200.png"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-17/wake-up/nocal/mar-17-wake-up-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Spring Is Inevitable<p></p>
<p></p><p>“Spring is round the corner. And very soon plants will grow on some other planets too. Let’s be happy about a new cycle of life.” — Designed by <a href=""https://izhik.com"">Igor Izhik</a> from Canada.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/301954ac-cca9-449b-aa18-f55fac6acd3f/mar-16-spring-is-inevitable-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/97e11614-864e-4ee3-8b3b-5686d165ffd8/mar-16-spring-is-inevitable-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/97e11614-864e-4ee3-8b3b-5686d165ffd8/mar-16-spring-is-inevitable-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1024x1024.jpg"">1024x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1152x864.jpg"">1152x864</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1280x720.jpg"">1280x720</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1280x800.jpg"">1280x800</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1280x960.jpg"">1280x960</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1400x1050.jpg"">1400x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1440x900.jpg"">1440x900</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1680x1050.jpg"">1680x1050</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-1920x1440.jpg"">1920x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-2560x1440.jpg"">2560x1440</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-16/spring-is-inevitable/nocal/mar-16-spring-is-inevitable-nocal-2560x1600.jpg"">2560x1600</a></li>
</ul>

<p></p>Traveling To Neverland<p></p>
<p></p><p>“This month we become children and we travel with Peter Pan. Let’s go to Neverland!” — Designed by <a href=""https://www.silocreativo.com/en"">Veronica Valenzuela</a> from Spain.</p><p></p>
<p></p><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-22-traveling-to-neverland-full-opt.png""><img src=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-22-traveling-to-neverland-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://files.smashing.media/articles/desktop-wallpaper-calendars-march-2025/mar-22-traveling-to-neverland-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-640x480.png"">640x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-800x480.png"">800x480</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1024x768.png"">1024x768</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1280x720.png"">1280x720</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1280x800.png"">1280x800</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1440x900.png"">1440x900</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1600x1200.png"">1600x1200</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1920x1080.png"">1920x1080</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-1920x1440.png"">1920x1440</a>, <a href=""https://www.smashingmagazine.com/files/wallpapers/mar-22/traveling-to-neverland/nocal/mar-22-traveling-to-neverland-nocal-2560x1440.png"">2560x1440</a></li>
</ul>

<p></p>Let’s Get Outside<p></p>
<p></p><p>Designed by <a href=""https://pathlove.com/"">Lívia Lénárt</a> from Hungary.</p><p></p>
<p></p><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/dfe3c753-896e-4bb1-915f-53adb712ef7d/mar-18-lets-get-outside-full-opt.png""><img src=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/7596279f-3fdb-41d6-937f-1a8cc0434092/mar-18-lets-get-outside-preview-opt.png"" /></a><p></p>
<ul>
<li><a href=""https://archive.smashing.media/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/7596279f-3fdb-41d6-937f-1a8cc0434092/mar-18-lets-get-outside-preview-opt.png"">preview</a></li>
<li>without calendar: <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1024x768.jpg"">1024x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1280x1024.jpg"">1280x1024</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1366x768.jpg"">1366x768</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1600x1200.jpg"">1600x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1680x1200.jpg"">1680x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1920x1080.jpg"">1920x1080</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-1920x1200.jpg"">1920x1200</a>, <a href=""https://smashingmagazine.com/files/wallpapers/mar-18/lets-get-outside/nocal/mar-18-lets-get-outside-nocal-2560x1440.jpg"">2560x1440</a></li> </ul>"
The Human Element: Using Research And Psychology To Elevate Data Storytelling,https://smashingmagazine.com/2025/02/human-element-using-research-psychology-elevate-data-storytelling/,Smashing Magazine,2025-02-26T10:00:00,Smashing Magazine,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Victor Yocco & Angelica Lo Duca Feb 26, 2025 0 comments The Human Element: Using Research And Psychology To Elevate Data Storytelling 23 min read UX , Design , Storytelling Share on Twitter , LinkedIn About The Authors Victor is a Philadelphia-based researcher, author, and speaker. His book Design for the Mind is available from Manning Publications. Victor frequently writes … More about
Victor & Angelica ↬ Email Newsletter Your (smashing) email Weekly tips on front-end & UX . Trusted by 200,000+ folks. Get a Free Trial How To Measure UX and Design Impact, 8h video + UX training Building Modern HTML Emails, with Rémi Parmentier Smart Interface Design Patterns, 10h video + UX training UX Design Leadership Masterclass, with Paul Boag JavaScript Form Builder — Create JSON-driven forms without coding. Try if for free! Effective data storytelling isn’t a black box. By integrating UX research & psychology, you can craft more impactful and persuasive narratives. Victor Yocco and Angelica Lo Duca outline a five-step framework that provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level. Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for user-centered improvements , and ultimately create more impactful and persuasive presentations . This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs. For instance, The New York Times’ Snow Fall data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while The Guardian’s The Counted (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions. Figure 1: The NYT Snow Fall displays data visualizations alongside a narrative of the events preceding and during a deadly avalanche. ( Large preview ) Figure 2: The Guardian The Counted tells a compelling data story of the facts behind people killed by the police in the US. ( Large preview ) The importance of data storytelling lies in its ability to: Simplify complexity It makes data understandable and actionable. Engage and persuade Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act. Bridge gaps Data storytelling connects the dots between information and human experience, making the data relevant and relatable. While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: Narrative Structures : Traditional storytelling models like the hero’s journey ( Vogler, 1992 ) or the Freytag pyramid (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged. Figure 3: Freytag’s Pyramid provides a narrative structure for storytellers. (Image source: scribophile.com) ( Large preview ) Data Visualization : Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics ( Cairo, 2016 ) transform raw data into digestible visuals, making complex information easier to understand and remember. Narrative Structures For Data Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories: The Three-Act Structure This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion ( Few, 2005 ). The Hero’s Journey (Data Edition) We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution. Example: Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement. Problems With Widely Used Data Storytelling Models Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the presentation of data over the experience of the audience, neglecting how different individuals perceive and process information. Figure 4: The traditional flow for creating a data-driven story. ( Large preview ) While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact. Cognitive Overload Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks. Emotional Disconnect Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values. Lack of Personalization Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. Over-Reliance on Visuals While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap, Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage. “ Improving On Traditional Models Traditional models can be improved by focusing more on the following two critical components: Audience understanding : A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading. Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion. Psychological principles : These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles. By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes. Persuasion In Data Storytelling All storytelling involves persuasion . Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience. Applying Psychological Principles To Data Storytelling Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive. Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. The Theory of Planned Behavior While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. The Theory of Planned Behavior (TPB) is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it. The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components: Attitude This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity. Subjective Norms These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. Perceived Behavioral Control This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior? As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991). Figure 5: The factors of the TPB interact with each other, collectively shaping an individual's behavioral intentions, which, in turn, are the most proximal determinant of human social behavior. ( Large preview ) UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article. How To Understand Your Audience And Apply Psychological Principles OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore how to integrate them into your storytelling process. Introducing The Audience Research Informed Data Storytelling Model (ARIDSM) At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories: Figure 6: The 5 steps of the Audience Research Informed Data Storytelling Model (ARIDSM). ( Large preview ) Step 1: Define Clear Objectives Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away? Why it matters : Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact. How to execute Step 1 : Start by asking yourself: What is the core message I want to convey? What do I want my audience to think, feel, or do after experiencing this story? How will I measure the success of my data story? Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.” Example: Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months. Step 2: Conduct UX Research To Understand Your Audience This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information. Why it matters : Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood. How to execute Step 2 : Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members. Example: If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story. Step 3: Analyze and Select Relevant Audience Data This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience. Why it matters : Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive. How to execute Step 3 : Clean and organize your data. Ensure accuracy and consistency before analysis. Identify key variables and metrics. This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items. Example: If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group. Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3] In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power. Why it matters : The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective. How to execute Step 4 : Here’s how to leverage the TPB in your data story: Influence Attitudes : Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. Shape Subjective Norms : Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise. Enhance Perceived Behavioral Control : Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome. Step 5: Craft A Balanced And Persuasive Narrative This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act. Why it matters : A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it. How to execute Step 5 : Structure your story strategically : Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you’ve cultivated throughout the narrative. Example: In a data story about promoting exercise, you could: Determine what stories might be available using the data you have collected or obtained. In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners. Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes). Integrate TPB elements : Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control). Use infographics to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling. Call to action : Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities. Evaluating The Method Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion. We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change. We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate. Step 1 : We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the hottest month ever recorded . Step 2 : We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research. Step 3 : We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a factor analysis to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change. Step 4 [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do. Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are no standardized questions (PDF) validated to measure the concepts for each individual topic. Item Measures Scale How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change? Attitude 1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial” How much do you think the people you care about (family, friends, community) expect you to take action against climate change? Subjective Norms 1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action” How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact? Perceived Behavioral Control 1 to 5 with 1 being “not at all confident” and 5 being “extremely confident” Table 1: Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component. Step 5 : We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created. Figure 7: Control data story. For the control story, we displayed the data around daily surface air temperature with some additional information explaining the chart. We did not attempt to influence behavior or tap into psychology to suggest there was urgency or persuade the participant to want to learn more. The color used in the chart comes from the initial chart generated in the source. We acknowledge that color is likely to present some psychological influence, given the use of red to represent extreme heat and cooler colors like blue to represent cooler time periods. ( Large preview ) Figure 8: Group 1 data story. Our measurements suggested that the participants in Group 1 had a higher level of awareness of climate change and the related negative impacts of more extreme temperatures. Therefore, we didn’t call out the potential negatives of climate change and instead focused on a more positive message of how we might make a positive impact. Group one had higher levels of subjective norms, suggesting that language promoting how others engage in certain behaviors might align with what they believe to be true. We focused on the community aspect of the message, encouraging them to act. ( Large preview ) Step 6 : We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group. Step 7 : We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message. Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic. For example, you might feel Maslow’s hierarchy of needs is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs. Elevate Your Data Storytelling Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as audience understanding and the application of psychological principles . By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives. The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level . This approach ensures that data is not merely presented but is transformed into a meaningful experience that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform. Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience. Please feel free to use our method and provide any feedback on your experience to the author. (yk) Explore more on UX Design Storytelling Smashing Newsletter Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. Front-End & UX Workshops, Online With practical takeaways, live sessions, video recordings and a friendly Q&A. TypeScript in 50 Lessons Everything TypeScript, with code walkthroughs and examples. And other printed books.","Victor Yocco & Angelica Lo Duca Feb 26, 2025 0 comments The Human Element: Using Research And Psychology To Elevate Data Storytelling 23 min read UX , Design , Storytelling Share on Twitter , LinkedIn About The Authors Victor is a Philadelphia-based researcher, author, and speaker. His book Design for the Mind is available from Manning Publications. Victor frequently writes … More about
Victor & Angelica ↬ Email Newsletter Your (smashing) email Weekly tips on front-end & UX . Trusted by 200,000+ folks. Get a Free Trial How To Measure UX and Design Impact, 8h video + UX training Building Modern HTML Emails, with Rémi Parmentier Smart Interface Design Patterns, 10h video + UX training UX Design Leadership Masterclass, with Paul Boag JavaScript Form Builder — Create JSON-driven forms without coding. Try if for free! Effective data storytelling isn’t a black box. By integrating UX research & psychology, you can craft more impactful and persuasive narratives. Victor Yocco and Angelica Lo Duca outline a five-step framework that provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level. Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for user-centered improvements , and ultimately create more impactful and persuasive presentations . This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs. For instance, The New York Times’ Snow Fall data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while The Guardian’s The Counted (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions. Figure 1: The NYT Snow Fall displays data visualizations alongside a narrative of the events preceding and during a deadly avalanche. ( Large preview ) Figure 2: The Guardian The Counted tells a compelling data story of the facts behind people killed by the police in the US. ( Large preview ) The importance of data storytelling lies in its ability to: Simplify complexity It makes data understandable and actionable. Engage and persuade Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act. Bridge gaps Data storytelling connects the dots between information and human experience, making the data relevant and relatable. While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: Narrative Structures : Traditional storytelling models like the hero’s journey ( Vogler, 1992 ) or the Freytag pyramid (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged. Figure 3: Freytag’s Pyramid provides a narrative structure for storytellers. (Image source: scribophile.com) ( Large preview ) Data Visualization : Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics ( Cairo, 2016 ) transform raw data into digestible visuals, making complex information easier to understand and remember. Narrative Structures For Data Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories: The Three-Act Structure This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion ( Few, 2005 ). The Hero’s Journey (Data Edition) We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution. Example: Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement. Problems With Widely Used Data Storytelling Models Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the presentation of data over the experience of the audience, neglecting how different individuals perceive and process information. Figure 4: The traditional flow for creating a data-driven story. ( Large preview ) While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact. Cognitive Overload Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks. Emotional Disconnect Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values. Lack of Personalization Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. Over-Reliance on Visuals While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap, Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage. “ Improving On Traditional Models Traditional models can be improved by focusing more on the following two critical components: Audience understanding : A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading. Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion. Psychological principles : These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles. By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes. Persuasion In Data Storytelling All storytelling involves persuasion . Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience. Applying Psychological Principles To Data Storytelling Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive. Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. The Theory of Planned Behavior While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. The Theory of Planned Behavior (TPB) is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it. The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components: Attitude This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity. Subjective Norms These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. Perceived Behavioral Control This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior? As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991). Figure 5: The factors of the TPB interact with each other, collectively shaping an individual's behavioral intentions, which, in turn, are the most proximal determinant of human social behavior. ( Large preview ) UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article. How To Understand Your Audience And Apply Psychological Principles OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore how to integrate them into your storytelling process. Introducing The Audience Research Informed Data Storytelling Model (ARIDSM) At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories: Figure 6: The 5 steps of the Audience Research Informed Data Storytelling Model (ARIDSM). ( Large preview ) Step 1: Define Clear Objectives Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away? Why it matters : Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact. How to execute Step 1 : Start by asking yourself: What is the core message I want to convey? What do I want my audience to think, feel, or do after experiencing this story? How will I measure the success of my data story? Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.” Example: Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months. Step 2: Conduct UX Research To Understand Your Audience This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information. Why it matters : Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood. How to execute Step 2 : Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members. Example: If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story. Step 3: Analyze and Select Relevant Audience Data This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience. Why it matters : Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive. How to execute Step 3 : Clean and organize your data. Ensure accuracy and consistency before analysis. Identify key variables and metrics. This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items. Example: If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group. Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3] In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power. Why it matters : The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective. How to execute Step 4 : Here’s how to leverage the TPB in your data story: Influence Attitudes : Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. Shape Subjective Norms : Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise. Enhance Perceived Behavioral Control : Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome. Step 5: Craft A Balanced And Persuasive Narrative This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act. Why it matters : A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it. How to execute Step 5 : Structure your story strategically : Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you’ve cultivated throughout the narrative. Example: In a data story about promoting exercise, you could: Determine what stories might be available using the data you have collected or obtained. In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners. Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes). Integrate TPB elements : Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control). Use infographics to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling. Call to action : Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities. Evaluating The Method Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion. We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change. We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate. Step 1 : We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the hottest month ever recorded . Step 2 : We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research. Step 3 : We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a factor analysis to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change. Step 4 [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do. Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are no standardized questions (PDF) validated to measure the concepts for each individual topic. Item Measures Scale How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change? Attitude 1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial” How much do you think the people you care about (family, friends, community) expect you to take action against climate change? Subjective Norms 1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action” How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact? Perceived Behavioral Control 1 to 5 with 1 being “not at all confident” and 5 being “extremely confident” Table 1: Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component. Step 5 : We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created. Figure 7: Control data story. For the control story, we displayed the data around daily surface air temperature with some additional information explaining the chart. We did not attempt to influence behavior or tap into psychology to suggest there was urgency or persuade the participant to want to learn more. The color used in the chart comes from the initial chart generated in the source. We acknowledge that color is likely to present some psychological influence, given the use of red to represent extreme heat and cooler colors like blue to represent cooler time periods. ( Large preview ) Figure 8: Group 1 data story. Our measurements suggested that the participants in Group 1 had a higher level of awareness of climate change and the related negative impacts of more extreme temperatures. Therefore, we didn’t call out the potential negatives of climate change and instead focused on a more positive message of how we might make a positive impact. Group one had higher levels of subjective norms, suggesting that language promoting how others engage in certain behaviors might align with what they believe to be true. We focused on the community aspect of the message, encouraging them to act. ( Large preview ) Step 6 : We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group. Step 7 : We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message. Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic. For example, you might feel Maslow’s hierarchy of needs is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs. Elevate Your Data Storytelling Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as audience understanding and the application of psychological principles . By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives. The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level . This approach ensures that data is not merely presented but is transformed into a meaningful experience that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform. Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience. Please feel free to use our method and provide any feedback on your experience to the author. (yk) Explore more on UX Design Storytelling Smashing Newsletter Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. Front-End & UX Workshops, Online With practical takeaways, live sessions, video recordings and a friendly Q&A. TypeScript in 50 Lessons Everything TypeScript, with code walkthroughs and examples. And other printed books.",The Human Element: Using Research And Psychology To Elevate Data Storytelling,"

Key Points:
",UI/UX,"<p>Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. </p>
<p>When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for <strong>user-centered improvements</strong>, and ultimately create <strong>more impactful and persuasive presentations</strong>. This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs.</p>
<p>For instance, <a href=""https://www.nytimes.com/projects/2012/snow-fall/index.html#/?part=tunnel-creek"">The New York Times’ <em>Snow Fall</em></a> data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while <a href=""https://www.theguardian.com/us-news/ng-interactive/2015/jun/01/the-counted-police-killings-us-database"">The Guardian’s <em>The Counted</em></a> (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/1-nyt-snow-fall-data-visualization.png"" /></p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/2-guardian-data-story.png"" /></p>
<p>The importance of data storytelling lies in its ability to:</p>
<ul>
<li><strong>Simplify complexity</strong><br />It makes data understandable and actionable.</li>
<li><strong>Engage and persuade</strong><br />Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act.</li>
<li><strong>Bridge gaps</strong><br />Data storytelling connects the dots between information and human experience, making the data relevant and relatable.</li>
</ul>
<p>While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: </p>
<p><strong>Narrative Structures</strong>: Traditional storytelling models like the <strong>hero’s journey</strong> (<a href=""https://www.amazon.com/Writers-Journey-Storytellers-Screenwriters-Christopher/dp/B00EKYS38S"">Vogler, 1992</a>) or the <strong><a href=""https://www.masterclass.com/articles/freytags-pyramid"">Freytag pyramid</a></strong> (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/3-freytag-pyramid.png"" /></p>
<p><strong>Data Visualization</strong>: Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics (<a href=""https://www.semanticscholar.org/paper/The-Truthful-Art%3A-Data%2C-Charts%2C-and-Maps-for-by-Marchese/60ec847558e415d72e1ed10ec2f32bd97486a510?p2df"">Cairo, 2016</a>) transform raw data into digestible visuals, making complex information easier to understand and remember.</p>
<h4>Narrative Structures For Data</h4>
<p>Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories:</p>
<ul>
<li><strong>The Three-Act Structure</strong><br />This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion (<a href=""https://www.amazon.com/Screenplay-Foundations-Screenwriting-Syd-Field/dp/0385339038"">Few, 2005</a>).</li>
<li><strong>The Hero’s Journey (Data Edition)</strong><br />We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution.</li>
</ul>
<blockquote>Example:<br />Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement.</blockquote>

Problems With Widely Used Data Storytelling Models
<p>Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the <em>presentation</em> of data over the <em>experience</em> of the audience, neglecting how different individuals perceive and process information.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/4-data-storytelling-structure.jpg"" /></p>
<p>While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact.</p>
<ul>
<li><strong>Cognitive Overload</strong><br />Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks.</li>
<li><strong>Emotional Disconnect</strong><br />Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values.</li>
<li><strong>Lack of Personalization</strong><br />Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. </li>
<li><strong>Over-Reliance on Visuals</strong><br />While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. </li>
</ul>
<p>These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap,</p>
<p>Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage.</p>
<h3>Improving On Traditional Models</h3>
<p>Traditional models can be improved by focusing more on the following two critical components:</p>
<p><strong>Audience understanding</strong>: A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading.</p>
<p>Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion.</p>
<p><strong>Psychological principles</strong>: These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles.</p>
<p>By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes.</p>
<h4>Persuasion In Data Storytelling</h4>
<p>All storytelling involves <strong>persuasion</strong>. Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience.</p>
<h3>Applying Psychological Principles To Data Storytelling</h3>
<p>Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive.</p>
<p>Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. </p>
<p><strong>The Theory of Planned Behavior</strong></p>
<p>While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. <strong>The Theory of Planned Behavior (TPB)</strong> is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it.</p>
<p>The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components:</p>
<ol>
<li><strong>Attitude</strong><br />This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity.</li>
<li><strong>Subjective Norms</strong><br />These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. </li>
<li><strong>Perceived Behavioral Control</strong><br />This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior?</li>
</ol>
<p>As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991).</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/5-theory-planned-behaviour.png"" /></p>
<p>UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article.</p>
How To Understand Your Audience And Apply Psychological Principles
<p>OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore <em>how</em> to integrate them into your storytelling process.</p>
<h3>Introducing The Audience Research Informed Data Storytelling Model (ARIDSM)</h3>
<p>At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories:</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/6-five-step-aridsm.png"" /></p>
<h4>Step 1: Define Clear Objectives</h4>
<p>Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away?</p>
<p><strong>Why it matters</strong>: Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact.</p>
<p><strong>How to execute Step 1</strong>: Start by asking yourself:</p>
<ul>
<li>What is the core message I want to convey?</li>
<li>What do I want my audience to think, feel, or do after experiencing this story?</li>
<li>How will I measure the success of my data story?</li>
</ul>
<p>Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.”</p>
<blockquote>Example:<br />Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months.</blockquote>

<h4>Step 2: Conduct UX Research To Understand Your Audience</h4>
<p>This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information.</p>
<p><strong>Why it matters</strong>: Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood.</p>
<p><strong>How to execute Step 2</strong>: Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members.</p>
<blockquote>Example:<br />If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story.</blockquote>

<h4>Step 3: Analyze and Select Relevant Audience Data</h4>
<p>This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience.</p>
<p><strong>Why it matters</strong>: Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive.</p>
<p><strong>How to execute Step 3</strong>:</p>
<ul>
<li><strong>Clean and organize your data.</strong><br />Ensure accuracy and consistency before analysis.</li>
<li><strong>Identify key variables and metrics.</strong><br />This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items.</li>
</ul>
<blockquote>Example:<br />If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group.</blockquote>

<h4>Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3]</h4>
<p>In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power.</p>
<p><strong>Why it matters</strong>: The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective.</p>
<p><strong>How to execute Step 4</strong>:</p>
<p>Here’s how to leverage the TPB in your data story:</p>
<p><strong>Influence Attitudes</strong>: Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. </p>
<p>This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. </p>
<p><strong>Shape Subjective Norms</strong>: Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. </p>
<p>If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise.</p>
<p><strong>Enhance Perceived Behavioral Control</strong>: Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome.</p>
<h4>Step 5: Craft A Balanced And Persuasive Narrative</h4>
<p>This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act.</p>
<p><strong>Why it matters</strong>: A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it.</p>
<p><strong>How to execute Step 5</strong>:</p>
<p><strong>Structure your story strategically</strong>: Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you've cultivated throughout the narrative.</p>
<blockquote>Example:<br />In a data story about promoting exercise, you could:<ul><li><strong>Determine what stories might be available using the data you have collected or obtained.</strong> In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners.</li><li>Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes).</li><li><strong>Integrate TPB elements</strong>: Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control).</li><li><strong>Use infographics</strong> to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling.</li><li><strong>Call to action</strong>: Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities.</li></ul></blockquote>

<h3>Evaluating The Method</h3>
<p>Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion.</p>
<p>We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change.</p>
<p>We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate.</p>
<p><strong>Step 1</strong>: We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the <a href=""https://climate.copernicus.eu/surface-air-temperature-july-2023"">hottest month ever recorded</a>.</p>
<p><strong>Step 2</strong>: We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. </p>
<p>We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research.</p>
<p><strong>Step 3</strong>: We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a <a href=""https://en.wikipedia.org/wiki/Factor_analysis"">factor analysis</a> to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change.</p>
<p><strong>Step 4</strong> [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do.</p>
<p>Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are <a href=""https://people.umass.edu/aizen/pdf/tpb.measurement.pdf"">no standardized questions</a> (PDF) validated to measure the concepts for each individual topic. </p>
<table>
    <thead>
        <tr>
            <th>Item</th>
            <th>Measures</th>
      <th>Scale</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change?</td>
            <td>Attitude</td>
      <td>1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial”</td>
        </tr>
        <tr>
            <td>How much do you think the people you care about (family, friends, community) expect you to take action against climate change?</td>
            <td>Subjective Norms</td>
      <td>1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action”</td>
        </tr>
        <tr>
            <td>How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact?</td>
            <td>Perceived Behavioral Control</td>
      <td>1 to 5 with 1 being “not at all confident” and 5 being “extremely confident”</td>
        </tr>
    </tbody>
</table>

<p><strong><em>Table 1:</em></strong> <em>Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component.</em></p>
<p><strong>Step 5</strong>: We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/7-control-data-story.jpg"" /></p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/8-customized-data-stories.jpg"" /></p>
<p><strong>Step 6</strong>: We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group.</p>
<p><strong>Step 7</strong>: We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message.</p>
<p>Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic.</p>
<p>For example, you might feel <a href=""https://www.simplypsychology.org/maslow.html"">Maslow’s hierarchy of needs</a> is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs.</p>
Elevate Your Data Storytelling
<p>Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as <strong>audience understanding</strong> and <strong>the application of psychological principles</strong>. By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives.</p>
<p>The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides <strong>a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level</strong>. This approach ensures that data is not merely presented but is transformed into <strong>a meaningful experience</strong> that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform.</p>
<p>Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience.</p>
<p>Please feel free to use our method and provide any feedback on your experience to the author.</p>"
ChatGPT vs DeepSeek: suggestions of diverging data color schemes,https://uxdesign.cc/chatgpt-vs-deepseek-suggestions-of-diverging-data-color-schemes-bb2e58e6c374?source=rss----138adf9c44c---4,UX Collective,2025-03-01T17:56:34,UX Collective,https://plus.unsplash.com/premium_photo-1661412938808-a0f7be3c8cf1?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Member-only story ChatGPT vs DeepSeek: suggestions of diverging data color schemes Theresa-Marie Rhyne · Follow Published in UX Collective · 9 min read · 12 hours ago -- 1 Share Comparing the ChatGPT and DeepSeek suggestions for a Pantone 2025 Color of the Year Diverging Color Scheme. Here, I compare ChatGPT and DeepSeek approaches to generating a customized diverging data color scheme that includes Mocha Mousse, the Pantone 2025 Color of the Year . Pantone provides suggested color harmonies but does not build data color schemes for its hues. I specifically asked both Gen AI systems to “Specify a five class diverging color scheme for Mocha Mousse with a neutral — white midpoint and color hex codes that passes color deficiency tests.”. Let’s begin this writing by reviewing the diverging data color scheme, color deficiency, and the Pantone Color of the Year concepts. A Diverging Data Color Scheme: Diverging data color schemes are created by joining two sequential color sequences together with a neutral midpoint. The data color scheme is often used for data that include a critical midpoint value (the mean, median or zero value) and a data distribution with two ends of importance. Below I show two listings of generic diverging color schemes, one from ChatGPT and the other from DeepSeek. Both Gen AI systems provided a series of color Hex code solutions based on my prompt: “Create various diverging color scheme suggestions”. For those unfamiliar with the concept, a color Hex code is a hexadecimal way to represent a color in RGB format by combining amounts of Red, Green, and Blue to note that…","Member-only story ChatGPT vs DeepSeek: suggestions of diverging data color schemes Theresa-Marie Rhyne · Follow Published in UX Collective · 9 min read · 12 hours ago -- 1 Share Comparing the ChatGPT and DeepSeek suggestions for a Pantone 2025 Color of the Year Diverging Color Scheme. Here, I compare ChatGPT and DeepSeek approaches to generating a customized diverging data color scheme that includes Mocha Mousse, the Pantone 2025 Color of the Year . Pantone provides suggested color harmonies but does not build data color schemes for its hues. I specifically asked both Gen AI systems to “Specify a five class diverging color scheme for Mocha Mousse with a neutral — white midpoint and color hex codes that passes color deficiency tests.”. Let’s begin this writing by reviewing the diverging data color scheme, color deficiency, and the Pantone Color of the Year concepts. A Diverging Data Color Scheme: Diverging data color schemes are created by joining two sequential color sequences together with a neutral midpoint. The data color scheme is often used for data that include a critical midpoint value (the mean, median or zero value) and a data distribution with two ends of importance. Below I show two listings of generic diverging color schemes, one from ChatGPT and the other from DeepSeek. Both Gen AI systems provided a series of color Hex code solutions based on my prompt: “Create various diverging color scheme suggestions”. For those unfamiliar with the concept, a color Hex code is a hexadecimal way to represent a color in RGB format by combining amounts of Red, Green, and Blue to note that…",ChatGPT vs DeepSeek: suggestions of diverging data color schemes,"

Key Points:
",UI/UX,"Member-only story ChatGPT vs DeepSeek: suggestions of diverging data color schemes Theresa-Marie Rhyne · Follow Published in UX Collective · 9 min read · 12 hours ago -- 1 Share Comparing the ChatGPT and DeepSeek suggestions for a Pantone 2025 Color of the Year Diverging Color Scheme. Here, I compare ChatGPT and DeepSeek approaches to generating a customized diverging data color scheme that includes Mocha Mousse, the Pantone 2025 Color of the Year . Pantone provides suggested color harmonies but does not build data color schemes for its hues. I specifically asked both Gen AI systems to “Specify a five class diverging color scheme for Mocha Mousse with a neutral — white midpoint and color hex codes that passes color deficiency tests.”. Let’s begin this writing by reviewing the diverging data color scheme, color deficiency, and the Pantone Color of the Year concepts. A Diverging Data Color Scheme: Diverging data color schemes are created by joining two sequential color sequences together with a neutral midpoint. The data color scheme is often used for data that include a critical midpoint value (the mean, median or zero value) and a data distribution with two ends of importance. Below I show two listings of generic diverging color schemes, one from ChatGPT and the other from DeepSeek. Both Gen AI systems provided a series of color Hex code solutions based on my prompt: “Create various diverging color scheme suggestions”. For those unfamiliar with the concept, a color Hex code is a hexadecimal way to represent a color in RGB format by combining amounts of Red, Green, and Blue to note that…"
"Growth begins where comfort ends. Yes, it’s easier said than done.",https://uxdesign.cc/growth-begins-where-comfort-ends-yes-its-easier-said-than-done-23657790f94e?source=rss----138adf9c44c---4,UX Collective,2025-02-28T21:33:49,UX Collective,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Growth begins where comfort ends. Yes, it’s easier said than done. Ok, we know growth stems from discomfort… but now what? I still don’t know what my next step will be… Pascal Potvin · Follow Published in UX Collective · 9 min read · 3 days ago -- Listen Share Let’s face it, growth isn’t this linear, magical path we often wish it was. Sure, discomfort pushes us forward, but that doesn’t mean we have all the answers now that we’ve stepped into that uncertain space. I’m right there with you. I’ve been standing at the edge of “ what’s next? ” wondering if I’ve learned all the lessons, tackled all the demons of discomfort, or if there’s some cosmic, obvious next move waiting to slap me in the face. The truth? No, there’s no clear-cut next step. And that’s exactly why it’s hard, and that’s the point. ( I’ve read that from some wise human in a book :/ ) Personal wallpaper concept Life always gives us what we need, not always what we want. I’ve you’re like me, you live a comfortable life and career, yet somehow you feel stuck at the same time. Weird isn’t it? The world asks us to be comfortable, to have it figured out, to stick to the script. We inhabit a society that conditions us to avoid failure at all costs, a lesson ingrained in us from the moment we’re young. The education system rewards getting the right answers while punishing mistakes with bad grades, turning the learning process into a game of perfection rather than progress. Parents (often with good intentions) shield their children from failure, stepping in too soon to fix problems. Achievements are praised, but effort is overlooked, reinforcing the belief that success is about results rather than resilience. This mindset carries into adulthood, shaping workplace culture. Employees learn quickly that taking risks can backfire, and that companies celebrate flawless execution but rarely reward bold attempts that don’t succeed. Playing it safe becomes the norm, stifling innovation before it even has a chance. The fear of judgment keeps many from trying new things, from sharing imperfect work, and from showing the messy reality of growth. This fear of failure holds people back even in design, where creativity should thrive. Designers stick to best practices, lean on established frameworks, and chase pixel perfection, but rarely are they encouraged to experiment openly, to fail in public, to iterate with uncertainty. The unspoken rule? Don’t risk looking like you don’t know what you’re doing. And yet, that’s exactly where real growth happens — outside the polished, predictable, and safe. It whispers that discomfort is dangerous, that mistakes define us, that we should remain within our safe, predictable boundaries and never venture beyond. For years, I surrendered to this narrative — shy, paralyzed by the fear of failure, ignoring that persistent inner voice urging me toward a greater purpose. And it wasn’t just in life; it was in my design work, too. I stuck to what was familiar. I relied on tried-and-true methods. I stuck with companies because they offered a steady paycheck over enjoying my work. At a certain point, I feared going back to freelance or joining startups because what if it didn’t work? What if it looked ridiculous? What if I failed in front of everyone? That’s the exact moment we usually hit that wall. Your comfort zone is the space (real, or in your mind) that you have carved out for yourself that makes you feel safe. We usually have two choices: stay where we are or lean into the discomfort and grow. Personal design for a wallpaper series But here’s the ugly truth: comfort kills ambition. The very moment we’ve crossed over into growth, the rules change. Now, it’s not about chasing comfort or being seen as a “pro” — it’s about getting comfortable with not knowing what comes next. It’s about accepting that your growth doesn’t have to look like anyone else’s and that sometimes, it’s a series of small, messy leaps, not grand, calculated steps. So how do we deal with that? By facing it . Oh boy, is that hard! So how do we deal with that ? Accept that uncertainty is your new best friend. When you’re feeling that pull to play it safe, recognize it as an invitation to do something bold. The world won’t hand you a roadmap. There’s no master plan, no “perfect timing.” Growth is about making decisions in spite of uncertainty, not waiting for all the stars to align. 2. Rewire your thinking about what it means to “arrive.” We’ve been conditioned to think that one day, we’ll hit this magic “ I’ve made it ” moment. But the truth is, you don’t arrive. You evolve . You keep pushing, shifting, and transforming. Stop waiting for permission to take that next step. Stop expecting that it’ll all make sense. It won’t. But it will be worth it. 3. Make discomfort your ally. When you get that feeling of “ What now? I don’t know what’s next ,” instead of freezing, lean into it. This is where your most important next step is hiding. I used to fight that feeling, but now, I’ve learned that it’s my signal to dig deeper, to try something wildly new, or to refine something I thought was already done. Growth doesn’t always look like forward movement — it often looks like reworking what you’ve already started. 4. Lead with courage, not certainty. Here’s the kicker: You don’t need to have every piece of the puzzle figured out to lead. Courage beats certainty every time. I’ve watched my most successful colleagues, not because they had all the answers, but because they made bold decisions in the face of the unknown and inspired others to do the same. That is leadership. 16 UX and UI Design Tips That Always Deliver Growth — Stef Ivanov Growth — beyond learning new tools Growth, especially in the fast-evolving world of design, requires more than just mastering new tools or trends. It’s about navigating the uncomfortable spaces of leadership, collaboration, and influence, where discomfort pushes you to stretch beyond the familiar. In design, as in life, growth comes from friction. It’s in those moments where you feel completely out of your depth that you evolve. But most of us don’t want to go there. We tend to avoid projects that feel too complex. We often resist learning new tools because it makes us feel like a beginner again. And let’s face it, that sucks. We cling to what we know instead of exploring what could be. In his article “ Reflections on leadership, growth, and design: Lessons from my journey at xccelerate ,” Kaleb cardenas Z shares insights from his tenure as Service Design Lead and UX Instructor, highlighting the continuous growth he experienced in these roles. For a long time, I was no different. Until I wasn’t. I haven’t fully solved it yet, but I’m trying hard to have faith in what life throws at me. I intentionally put myself into uncomfortable situations so that I get better at adapting… not coping. My journey of transformation began when I deliberately sought the unknown: Challenging my limits 30 feet underwater, holding my breath Sharing vulnerable content before millions (including haters) Embracing new sports despite initial incompetence Venturing into unfamiliar territory — moving my family to Mexico, learning a new language, opening a dropshipping store with my kids… Testing my physical and mental boundaries through ancestral medicine, cold exposure, breathwork practices, and rigorous training regimen Trying to abandon the “safe” path and follow my intuition Doing daily ice baths And, just as importantly, I began taking risks in my design career: Designing in styles I had no prior experience with Starting having different conversations Pitching bold ideas in rooms where I felt unqualified Adopting new design tools forced me back into a beginner mindset Saying yes to projects before I felt “ready” (this is big) Challenging industry norms instead of playing it safe These experiences taught me something fundamental: growth is never comfortable. But comfort is a slow death for creativity and innovation. The greatest insight Fear isn’t concrete reality — it’s a protective narrative generated by your ego. But here’s the paradox: safety and growth cannot coexist. If evolution is your goal, discomfort must become your compass. This is exactly what’s shaking many of us right now. The design game has changed and I sometimes don’t know where I fit anymore. With the speed at which tech evolves, AI, the great unknown in front of us is challenging us all to rethink our next step and move. This applies to design just as much as it applies to life. If you want to stand out in your field, stop playing it safe. Safe ideas are forgettable Safe designs blend into the noise Safe careers stagnate Safe gets you knowhere And here’s what most people don’t tell you: advancing in your career isn’t just about perfecting your craft. The difference between junior and senior designers — or senior and principal — isn’t just skill. It’s soft skills. It’s how well you navigate relationships, how you influence decisions, and how you bridge the gap between design, engineering, and product. Your pixel-perfect design doesn’t matter if you can’t get buy-in from developers and PMs to bring it to life. Once I realized that, the game changed. The best designers aren’t just great at designing. They’re great at leading, persuading, and collaborating. That’s what sets them apart. Personal podcast template Framer design exploration The formula for transformation The breakthrough equation is simpler than you might think: Self-awareness + Self-compassion Self-awareness demands brutal honesty: What patterns keep you trapped? What truths are you avoiding? Where are you choosing comfort over potential? Self-compassion recognizes that perfection isn’t the goal. Growth isn’t about self-criticism — it’s about acknowledging your limitations while moving forward regardless. This shift in mindset changed everything for me. Instead of beating myself up for not being “as good as” other designers, I started asking myself, “ What can I learn from them? ” Instead of dreading the feeling of being a beginner again, I started embracing it. Instead of viewing imposter syndrome as a sign to stop, I started seeing it as proof I was in the right place — at the edge of my current abilities, where real growth happens. Demystifying success Stop idealizing others. Those you admire encountered obstacles too. The only meaningful distinction? Persistence through difficulty . The best designers, the most successful creators, the people who make a lasting impact — they don’t have some magical talent that others don’t. They simply keep pushing past the discomfort that stops most people in their tracks. Your invitation Today, I challenge you: Step deliberately into discomfort. Dine alone in a crowded restaurant. Embrace the shock of a cold shower. Publish that controversial post you’ve been holding back. Experiment with a completely new design style. Apply for that dream job, even if you don’t feel ready. Schedule a 1:1 with a developer or PM and build that bridge. Do something that accelerates your heartbeat. Because your authentic life — and your most impactful work — begins precisely where your comfort zone ends. The design game has changed and I don’t know where I fit anymore… maybe that’s exactly it. We don’t fit in anymore, and that could be a sign to create our own reality and dare to do that thing we’ve always put off. The question is: Will you choose growth? Life Begins At the End of Your Comfort Zone. Here's Why. There are benefits to living a life of comfort, but your most exciting & purposeful life begins at the end of your… www.joyfulthroughitall.com https://positivepsychology.com/comfort-zone/ Essential Soft Skills for UX Designers Discover actionable strategies to enhance your soft skills, crucial for thriving in UX design. This guide is tailored… uxplaybook.org Reflections on Leadership, Growth, and Design: Lessons from My Journey as Chief Experience Officer As I’ve recently moved on from my role at Xccelerate, I wanted to take a moment to reflect on the lessons I’ve learned… medium.com Leading Beyond Authority: The Power of Influence in Design Leadership What turns a design leader into a true catalyst for change? It’s not just about title or authority — it’s about leading… medium.com Breathing techniques | Wim Hof Method Learn how the Wim Hof Method can help you with breathing techniques. www.wimhofmethod.com Fear is the Usual State of the Ego, But We Can Overcome The ego’s job is to protect us. So, it’s always looking for danger. Even if the situation isn’t hazardous, it still has… medium.com","Growth begins where comfort ends. Yes, it’s easier said than done. Ok, we know growth stems from discomfort… but now what? I still don’t know what my next step will be… Pascal Potvin · Follow Published in UX Collective · 9 min read · 3 days ago -- Listen Share Let’s face it, growth isn’t this linear, magical path we often wish it was. Sure, discomfort pushes us forward, but that doesn’t mean we have all the answers now that we’ve stepped into that uncertain space. I’m right there with you. I’ve been standing at the edge of “ what’s next? ” wondering if I’ve learned all the lessons, tackled all the demons of discomfort, or if there’s some cosmic, obvious next move waiting to slap me in the face. The truth? No, there’s no clear-cut next step. And that’s exactly why it’s hard, and that’s the point. ( I’ve read that from some wise human in a book :/ ) Personal wallpaper concept Life always gives us what we need, not always what we want. I’ve you’re like me, you live a comfortable life and career, yet somehow you feel stuck at the same time. Weird isn’t it? The world asks us to be comfortable, to have it figured out, to stick to the script. We inhabit a society that conditions us to avoid failure at all costs, a lesson ingrained in us from the moment we’re young. The education system rewards getting the right answers while punishing mistakes with bad grades, turning the learning process into a game of perfection rather than progress. Parents (often with good intentions) shield their children from failure, stepping in too soon to fix problems. Achievements are praised, but effort is overlooked, reinforcing the belief that success is about results rather than resilience. This mindset carries into adulthood, shaping workplace culture. Employees learn quickly that taking risks can backfire, and that companies celebrate flawless execution but rarely reward bold attempts that don’t succeed. Playing it safe becomes the norm, stifling innovation before it even has a chance. The fear of judgment keeps many from trying new things, from sharing imperfect work, and from showing the messy reality of growth. This fear of failure holds people back even in design, where creativity should thrive. Designers stick to best practices, lean on established frameworks, and chase pixel perfection, but rarely are they encouraged to experiment openly, to fail in public, to iterate with uncertainty. The unspoken rule? Don’t risk looking like you don’t know what you’re doing. And yet, that’s exactly where real growth happens — outside the polished, predictable, and safe. It whispers that discomfort is dangerous, that mistakes define us, that we should remain within our safe, predictable boundaries and never venture beyond. For years, I surrendered to this narrative — shy, paralyzed by the fear of failure, ignoring that persistent inner voice urging me toward a greater purpose. And it wasn’t just in life; it was in my design work, too. I stuck to what was familiar. I relied on tried-and-true methods. I stuck with companies because they offered a steady paycheck over enjoying my work. At a certain point, I feared going back to freelance or joining startups because what if it didn’t work? What if it looked ridiculous? What if I failed in front of everyone? That’s the exact moment we usually hit that wall. Your comfort zone is the space (real, or in your mind) that you have carved out for yourself that makes you feel safe. We usually have two choices: stay where we are or lean into the discomfort and grow. Personal design for a wallpaper series But here’s the ugly truth: comfort kills ambition. The very moment we’ve crossed over into growth, the rules change. Now, it’s not about chasing comfort or being seen as a “pro” — it’s about getting comfortable with not knowing what comes next. It’s about accepting that your growth doesn’t have to look like anyone else’s and that sometimes, it’s a series of small, messy leaps, not grand, calculated steps. So how do we deal with that? By facing it . Oh boy, is that hard! So how do we deal with that ? Accept that uncertainty is your new best friend. When you’re feeling that pull to play it safe, recognize it as an invitation to do something bold. The world won’t hand you a roadmap. There’s no master plan, no “perfect timing.” Growth is about making decisions in spite of uncertainty, not waiting for all the stars to align. 2. Rewire your thinking about what it means to “arrive.” We’ve been conditioned to think that one day, we’ll hit this magic “ I’ve made it ” moment. But the truth is, you don’t arrive. You evolve . You keep pushing, shifting, and transforming. Stop waiting for permission to take that next step. Stop expecting that it’ll all make sense. It won’t. But it will be worth it. 3. Make discomfort your ally. When you get that feeling of “ What now? I don’t know what’s next ,” instead of freezing, lean into it. This is where your most important next step is hiding. I used to fight that feeling, but now, I’ve learned that it’s my signal to dig deeper, to try something wildly new, or to refine something I thought was already done. Growth doesn’t always look like forward movement — it often looks like reworking what you’ve already started. 4. Lead with courage, not certainty. Here’s the kicker: You don’t need to have every piece of the puzzle figured out to lead. Courage beats certainty every time. I’ve watched my most successful colleagues, not because they had all the answers, but because they made bold decisions in the face of the unknown and inspired others to do the same. That is leadership. 16 UX and UI Design Tips That Always Deliver Growth — Stef Ivanov Growth — beyond learning new tools Growth, especially in the fast-evolving world of design, requires more than just mastering new tools or trends. It’s about navigating the uncomfortable spaces of leadership, collaboration, and influence, where discomfort pushes you to stretch beyond the familiar. In design, as in life, growth comes from friction. It’s in those moments where you feel completely out of your depth that you evolve. But most of us don’t want to go there. We tend to avoid projects that feel too complex. We often resist learning new tools because it makes us feel like a beginner again. And let’s face it, that sucks. We cling to what we know instead of exploring what could be. In his article “ Reflections on leadership, growth, and design: Lessons from my journey at xccelerate ,” Kaleb cardenas Z shares insights from his tenure as Service Design Lead and UX Instructor, highlighting the continuous growth he experienced in these roles. For a long time, I was no different. Until I wasn’t. I haven’t fully solved it yet, but I’m trying hard to have faith in what life throws at me. I intentionally put myself into uncomfortable situations so that I get better at adapting… not coping. My journey of transformation began when I deliberately sought the unknown: Challenging my limits 30 feet underwater, holding my breath Sharing vulnerable content before millions (including haters) Embracing new sports despite initial incompetence Venturing into unfamiliar territory — moving my family to Mexico, learning a new language, opening a dropshipping store with my kids… Testing my physical and mental boundaries through ancestral medicine, cold exposure, breathwork practices, and rigorous training regimen Trying to abandon the “safe” path and follow my intuition Doing daily ice baths And, just as importantly, I began taking risks in my design career: Designing in styles I had no prior experience with Starting having different conversations Pitching bold ideas in rooms where I felt unqualified Adopting new design tools forced me back into a beginner mindset Saying yes to projects before I felt “ready” (this is big) Challenging industry norms instead of playing it safe These experiences taught me something fundamental: growth is never comfortable. But comfort is a slow death for creativity and innovation. The greatest insight Fear isn’t concrete reality — it’s a protective narrative generated by your ego. But here’s the paradox: safety and growth cannot coexist. If evolution is your goal, discomfort must become your compass. This is exactly what’s shaking many of us right now. The design game has changed and I sometimes don’t know where I fit anymore. With the speed at which tech evolves, AI, the great unknown in front of us is challenging us all to rethink our next step and move. This applies to design just as much as it applies to life. If you want to stand out in your field, stop playing it safe. Safe ideas are forgettable Safe designs blend into the noise Safe careers stagnate Safe gets you knowhere And here’s what most people don’t tell you: advancing in your career isn’t just about perfecting your craft. The difference between junior and senior designers — or senior and principal — isn’t just skill. It’s soft skills. It’s how well you navigate relationships, how you influence decisions, and how you bridge the gap between design, engineering, and product. Your pixel-perfect design doesn’t matter if you can’t get buy-in from developers and PMs to bring it to life. Once I realized that, the game changed. The best designers aren’t just great at designing. They’re great at leading, persuading, and collaborating. That’s what sets them apart. Personal podcast template Framer design exploration The formula for transformation The breakthrough equation is simpler than you might think: Self-awareness + Self-compassion Self-awareness demands brutal honesty: What patterns keep you trapped? What truths are you avoiding? Where are you choosing comfort over potential? Self-compassion recognizes that perfection isn’t the goal. Growth isn’t about self-criticism — it’s about acknowledging your limitations while moving forward regardless. This shift in mindset changed everything for me. Instead of beating myself up for not being “as good as” other designers, I started asking myself, “ What can I learn from them? ” Instead of dreading the feeling of being a beginner again, I started embracing it. Instead of viewing imposter syndrome as a sign to stop, I started seeing it as proof I was in the right place — at the edge of my current abilities, where real growth happens. Demystifying success Stop idealizing others. Those you admire encountered obstacles too. The only meaningful distinction? Persistence through difficulty . The best designers, the most successful creators, the people who make a lasting impact — they don’t have some magical talent that others don’t. They simply keep pushing past the discomfort that stops most people in their tracks. Your invitation Today, I challenge you: Step deliberately into discomfort. Dine alone in a crowded restaurant. Embrace the shock of a cold shower. Publish that controversial post you’ve been holding back. Experiment with a completely new design style. Apply for that dream job, even if you don’t feel ready. Schedule a 1:1 with a developer or PM and build that bridge. Do something that accelerates your heartbeat. Because your authentic life — and your most impactful work — begins precisely where your comfort zone ends. The design game has changed and I don’t know where I fit anymore… maybe that’s exactly it. We don’t fit in anymore, and that could be a sign to create our own reality and dare to do that thing we’ve always put off. The question is: Will you choose growth? Life Begins At the End of Your Comfort Zone. Here's Why. There are benefits to living a life of comfort, but your most exciting & purposeful life begins at the end of your… www.joyfulthroughitall.com https://positivepsychology.com/comfort-zone/ Essential Soft Skills for UX Designers Discover actionable strategies to enhance your soft skills, crucial for thriving in UX design. This guide is tailored… uxplaybook.org Reflections on Leadership, Growth, and Design: Lessons from My Journey as Chief Experience Officer As I’ve recently moved on from my role at Xccelerate, I wanted to take a moment to reflect on the lessons I’ve learned… medium.com Leading Beyond Authority: The Power of Influence in Design Leadership What turns a design leader into a true catalyst for change? It’s not just about title or authority — it’s about leading… medium.com Breathing techniques | Wim Hof Method Learn how the Wim Hof Method can help you with breathing techniques. www.wimhofmethod.com Fear is the Usual State of the Ego, But We Can Overcome The ego’s job is to protect us. So, it’s always looking for danger. Even if the situation isn’t hazardous, it still has… medium.com","Growth begins where comfort ends. Yes, it’s easier said than done.","

Key Points:
",UI/UX,"<h4>Ok, we know growth stems from discomfort… but now what? I still don’t know what my next step will be…</h4><p>Let’s face it, growth isn’t this linear, magical path we often wish it was. Sure, discomfort pushes us forward, but that doesn’t mean we have all the answers now that we’ve stepped into that uncertain space.</p><p>I’m right there with you. I’ve been standing at the edge of “<a href=""https://medium.com/user-experience-design-1/the-design-game-has-changed-and-i-dont-know-where-i-fit-anymore-d1e414c2ea9c""><em>what’s next?</em></a>” wondering if I’ve learned all the lessons, tackled all the demons of discomfort, or if there’s some cosmic, obvious next move waiting to slap me in the face.</p><p>The truth? No, there’s no clear-cut next step. And that’s exactly why it’s hard, and that’s the point. (<em>I’ve read that from some wise human in a book :/</em>)</p><figure><img alt=""Desktop wallpaper design with text “It’s you against yourself”. A motivational concept"" src=""https://cdn-images-1.medium.com/max/1024/1*2rbRT8sGcycRO5Xjeyz7ww.png"" /><figcaption>Personal wallpaper concept</figcaption></figure><h3>Life always gives us what we need, not always what we want.</h3><p>I’ve you’re like me, you live a comfortable life and career, yet somehow you feel stuck at the same time. Weird isn’t it?</p><p>The world asks us to be comfortable, to have it figured out, to stick to the script. We inhabit a society that conditions us to avoid failure at all costs, a lesson ingrained in us from the moment we’re young. The education system rewards getting the right answers while punishing mistakes with bad grades, turning the learning process into a game of perfection rather than progress. Parents (often with good intentions) shield their children from failure, stepping in too soon to fix problems. Achievements are praised, but effort is overlooked, reinforcing the belief that success is about results rather than resilience.</p><p>This mindset carries into adulthood, shaping workplace culture. Employees learn quickly that taking risks can backfire, and that companies celebrate flawless execution but rarely reward bold attempts that don’t succeed. Playing it safe becomes the norm, stifling innovation before it even has a chance. The fear of judgment keeps many from trying new things, from sharing imperfect work, and from showing the messy reality of growth.</p><p>This fear of failure holds people back even in design, where creativity should thrive. Designers stick to best practices, lean on established frameworks, and chase pixel perfection, but rarely are they encouraged to experiment openly, to fail in public, to iterate with uncertainty. The unspoken rule? Don’t risk looking like you don’t know what you’re doing.</p><p>And yet, that’s exactly where real growth happens — outside the polished, predictable, and safe.</p><p>It whispers that discomfort is dangerous, that mistakes define us, that we should remain within our safe, predictable boundaries and never venture beyond. For years, I surrendered to this narrative — shy, paralyzed by the fear of failure, ignoring that persistent inner voice urging me toward a greater purpose.</p><p>And it wasn’t just in life; it was in my design work, too. I stuck to what was familiar. I relied on tried-and-true methods. I stuck with companies because they offered a steady paycheck over enjoying my work. At a certain point, I feared going back to freelance or joining startups because what if it didn’t work? What if it looked ridiculous? What if I failed in front of everyone?</p><blockquote>That’s the exact moment we usually hit that wall. Your <a href=""https://www.joyfulthroughitall.com/life-begins-at-the-end-of-your-comfort-zone/"">comfort zone</a> is the space (real, or in your mind) that you have carved out for yourself that makes you feel safe.</blockquote><p>We usually have two choices: stay where we are or lean into the discomfort and grow.</p><figure><img alt=""Desktop wallpaper of a flower with text over it “If you’re growing slow, you’re still growing.”"" src=""https://cdn-images-1.medium.com/max/1024/1*o1cTrGAxxFJWqVfMUgTeVg.jpeg"" /><figcaption>Personal design for a wallpaper series</figcaption></figure><h4><strong>But here’s the ugly truth: comfort kills ambition.</strong></h4><p>The very moment we’ve crossed over into growth, the rules change. Now, it’s not about chasing comfort or being seen as a “pro” — it’s about getting comfortable with not knowing what comes next. It’s about accepting that your growth doesn’t have to look like anyone else’s and that sometimes, it’s a series of small, messy leaps, not grand, calculated steps.</p><p>So how do we deal with that?</p><p><strong>By facing it</strong>. Oh boy, is that hard!</p><p>So how do we deal with <em>that</em>?</p><ol><li><strong>Accept that uncertainty is your new best friend.</strong></li></ol><p>When you’re feeling that pull to play it safe, recognize it as an invitation to do something bold. The world won’t hand you a roadmap. There’s no master plan, no “perfect timing.” Growth is about making decisions in spite of uncertainty, not waiting for all the stars to align.</p><p><strong>2. Rewire your thinking about what it means to “arrive.”</strong></p><p>We’ve been conditioned to think that one day, we’ll hit this magic “<em>I’ve made it</em>” moment. <strong>But the truth is, you don’t arrive. You evolve</strong>. You keep pushing, shifting, and transforming. Stop waiting for permission to take that next step. Stop expecting that it’ll all make sense. It won’t. But it will be worth it.</p><p><strong>3. Make discomfort your ally.</strong></p><p>When you get that feeling of “<em>What now? I don’t know what’s next</em>,” instead of freezing, lean into it. This is where your most important next step is hiding. I used to fight that feeling, but now, I’ve learned that it’s my signal to dig deeper, to try something wildly new, or to refine something I thought was already done. Growth doesn’t always look like forward movement — it often looks like reworking what you’ve already started.</p><p><strong>4. Lead with courage, not certainty.</strong></p><p>Here’s the kicker: You don’t need to have every piece of the puzzle figured out to lead. Courage beats certainty every time. I’ve watched my most successful colleagues, not because they had all the answers, but because they made bold decisions in the face of the unknown and inspired others to do the same. That is leadership.</p><figure><img alt=""Drawing of barcharts with a man jumping to the highest bar (growth)"" src=""https://cdn-images-1.medium.com/max/1024/0*FEhIe6QC2qLsYShq"" /><figcaption>16 UX and UI Design Tips That Always Deliver Growth — <a href=""https://medium.com/@growthdesigner?source=post_page---byline--6bacd9fd25fe---------------------------------------"">Stef Ivanov</a></figcaption></figure><h3>Growth — beyond learning new tools</h3><p>Growth, especially in the fast-evolving world of design, requires more than just mastering new tools or trends. It’s about navigating the uncomfortable spaces of leadership, collaboration, and influence, where discomfort pushes you to stretch beyond the familiar.</p><p>In design, as in life, growth comes from friction. It’s in those moments where you feel completely out of your depth that you evolve. But most of us don’t want to go there. We tend to avoid projects that feel too complex. We often resist learning new tools because it makes us feel like a beginner again. And let’s face it, that sucks. We cling to what we know instead of exploring what could be.</p><p>In his article “<a href=""https://medium.com/%40kalebjcardenasz/reflections-on-leadership-growth-and-design-lessons-from-my-journey-at-xccelerate-c43bd4dece60"">Reflections on leadership, growth, and design: Lessons from my journey at xccelerate</a>,” Kaleb cardenas Z shares insights from his tenure as Service Design Lead and UX Instructor, highlighting the continuous growth he experienced in these roles.</p><p>For a long time, I was no different. Until I wasn’t. I haven’t fully solved it yet, but I’m trying hard to have faith in what life throws at me. I intentionally put myself into uncomfortable situations so that I get better at adapting… not coping.</p><p>My journey of transformation began when I deliberately sought the unknown:</p><ul><li>Challenging my limits 30 feet underwater, holding my breath</li><li>Sharing vulnerable content before millions (including haters)</li><li>Embracing new sports despite initial incompetence</li><li>Venturing into unfamiliar territory — moving my family to Mexico, learning a new language, opening a dropshipping store with my kids…</li><li>Testing my physical and mental boundaries through ancestral medicine, cold exposure, breathwork practices, and rigorous training regimen</li><li>Trying to abandon the “safe” path and follow my intuition</li><li>Doing daily <a href=""https://www.wimhofmethod.com/breathing-techniques"">ice baths</a></li></ul><p>And, just as importantly, I began taking risks in my design career:</p><ul><li>Designing in styles I had no prior experience with</li><li>Starting having different conversations</li><li>Pitching bold ideas in rooms where I felt unqualified</li><li>Adopting new design tools forced me back into a beginner mindset</li><li>Saying yes to projects before I felt “ready” (this is big)</li><li>Challenging industry norms instead of playing it safe</li></ul><p>These experiences taught me something fundamental: growth is never comfortable. But comfort is a slow death for creativity and innovation.</p><h3>The greatest insight</h3><p><a href=""https://medium.com/%40TerriMKozlowski/fear-is-the-usual-state-of-the-ego-but-we-can-overcome-48e4ec47f419"">Fear isn’t concrete reality</a> — it’s a protective narrative generated by your ego.</p><blockquote>But here’s the paradox: safety and growth cannot coexist.</blockquote><p><strong>If evolution is your goal, discomfort must become your compass.</strong></p><p>This is exactly what’s shaking many of us right now. The design game has changed and I sometimes don’t know where I fit anymore. With the speed at which tech evolves, AI, the great unknown in front of us is challenging us all to rethink our next step and move.</p><p>This applies to design just as much as it applies to life. If you want to stand out in your field, stop playing it safe.</p><ul><li>Safe ideas are forgettable</li><li>Safe designs blend into the noise</li><li>Safe careers stagnate</li><li>Safe gets you knowhere</li></ul><p>And here’s what most people don’t tell you: advancing in your career isn’t just about perfecting your craft. The difference between junior and senior designers — or senior and principal — isn’t just skill. It’s soft skills. It’s how well you navigate relationships, how you influence decisions, and how you bridge the gap between design, engineering, and product.</p><p><strong><em>Your pixel-perfect design doesn’t matter if you can’t get buy-in from developers and PMs to bring it to life. </em></strong>Once I realized that, the game changed.</p><p>The best designers aren’t just great at designing. They’re great at leading, persuading, and collaborating. That’s what sets them apart.</p><figure><img alt=""Website animation for a Framer template concept"" src=""https://cdn-images-1.medium.com/max/1024/1*iP-mUm1j4sHMUoQjasGKUw.gif"" /><figcaption>Personal podcast template Framer design exploration</figcaption></figure><h4><strong>The formula for transformation</strong></h4><p>The <a href=""https://medium.com/age-of-empathy/balancing-self-awareness-and-self-compassion-a-journey-to-emotional-well-being-74a7f5397ab1"">breakthrough equation</a> is simpler than you might think:</p><blockquote><strong>Self-awareness + Self-compassion</strong></blockquote><p>Self-awareness demands brutal honesty:</p><ul><li>What patterns keep you trapped?</li><li>What truths are you avoiding?</li><li>Where are you choosing comfort over potential?</li></ul><p>Self-compassion recognizes that perfection isn’t the goal. Growth isn’t about self-criticism — it’s about acknowledging your limitations while moving forward regardless.</p><p>This shift in mindset changed everything for me. Instead of beating myself up for not being “as good as” other designers, I started asking myself, “<em>What can I learn from them?</em>” Instead of dreading the feeling of being a beginner again, I started embracing it. Instead of viewing imposter syndrome as a sign to stop, I started seeing it as proof I was in the right place — at the edge of my current abilities, where real growth happens.</p><h3>Demystifying success</h3><p>Stop idealizing others. Those you admire encountered obstacles too. The only meaningful distinction? <strong>Persistence through difficulty</strong>.</p><p>The best designers, the most successful creators, the people who make a lasting impact — they don’t have some magical talent that others don’t. They simply keep pushing past the discomfort that stops most people in their tracks.</p><h4>Your invitation</h4><p>Today, I challenge you: Step deliberately into discomfort.</p><ul><li>Dine alone in a crowded restaurant.</li><li>Embrace the shock of a cold shower.</li><li>Publish that controversial post you’ve been holding back.</li><li>Experiment with a completely new design style.</li><li>Apply for that dream job, even if you don’t feel ready.</li><li>Schedule a 1:1 with a developer or PM and build that bridge.</li></ul><p>Do something that accelerates your heartbeat. Because your authentic life — and your most impactful work — begins precisely where your comfort zone ends.</p><p>The design game has changed and I don’t know where I fit anymore… maybe that’s exactly it. We don’t fit in anymore, and that could be a sign to create our own reality and dare to do that thing we’ve always put off.</p><p>The question is: <strong><em>Will you choose growth?</em></strong></p><p><a href=""https://www.joyfulthroughitall.com/life-begins-at-the-end-of-your-comfort-zone/"">Life Begins At the End of Your Comfort Zone. Here's Why.</a></p><p><a href=""https://positivepsychology.com/comfort-zone/"">https://positivepsychology.com/comfort-zone/</a></p><ul><li><a href=""https://uxplaybook.org/articles/essential-soft-skills-for-ux-designers"">Essential Soft Skills for UX Designers</a></li><li><a href=""https://medium.com/@kalebjcardenasz/reflections-on-leadership-growth-and-design-lessons-from-my-journey-at-xccelerate-c43bd4dece60"">Reflections on Leadership, Growth, and Design: Lessons from My Journey as Chief Experience Officer</a></li><li><a href=""https://medium.com/defining-experience/leading-beyond-authority-the-power-of-influence-in-design-leadership-7661b8e8023d"">Leading Beyond Authority: The Power of Influence in Design Leadership</a></li><li><a href=""https://www.wimhofmethod.com/breathing-techniques"">Breathing techniques | Wim Hof Method</a></li><li><a href=""https://medium.com/@TerriMKozlowski/fear-is-the-usual-state-of-the-ego-but-we-can-overcome-48e4ec47f419"">Fear is the Usual State of the Ego, But We Can Overcome</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=23657790f94e"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/growth-begins-where-comfort-ends-yes-its-easier-said-than-done-23657790f94e"">Growth begins where comfort ends. Yes, it’s easier said than done.</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
A study of gatcha games: the UX of the Pokemon TCG Pocket app,https://uxdesign.cc/a-study-of-gatcha-games-the-ux-of-the-pokemon-tcg-pocket-app-b291c78db86f?source=rss----138adf9c44c---4,UX Collective,2025-02-28T10:28:16,UX Collective,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Member-only story A study of gatcha games: the UX of the Pokemon TCG Pocket app Has Pokemon given in to the evils of gatcha gaming? Is gatcha gaming evil? What is gatcha gaming? Daley Wilhelm · Follow Published in UX Collective · 9 min read · 1 day ago -- Share Pokemon cards are more accessible than ever. The closest I’ve ever been to gambling is ripping into Pokemon card packs, a vice that started in childhood and continues to eat into my disposable income today. Pokemon TCG Pocket, an app that digitally recreates the experience of ripping open booster packs and collecting cards, has proven to be a wildly successful facsimile. The app, which released in October 2024, earned approximately $400 million by January 2025. How did a free-to-play app manage such a feat? The psychological pull of gatcha gaming. What is gatcha gaming? The Genshin Impact character banner — in the top right, 160 Primogems can be exchanged for one Intertwined Fate, which in turn can be used to “wish” for a character. The Standard banner, the rightmost box, uses a different currency called Acquaint Fate. The wild success of the Pokemon TCG Pocket app, a way to digitally collect Pokemon cards, is arguably an indication that gatcha gaming is more mainstream than ever. The word “ gatcha ” comes from “ gashapon ” which references the Japanese toy dispensers that pop out random, often collectable toys for a couple hundred yen.","Member-only story A study of gatcha games: the UX of the Pokemon TCG Pocket app Has Pokemon given in to the evils of gatcha gaming? Is gatcha gaming evil? What is gatcha gaming? Daley Wilhelm · Follow Published in UX Collective · 9 min read · 1 day ago -- Share Pokemon cards are more accessible than ever. The closest I’ve ever been to gambling is ripping into Pokemon card packs, a vice that started in childhood and continues to eat into my disposable income today. Pokemon TCG Pocket, an app that digitally recreates the experience of ripping open booster packs and collecting cards, has proven to be a wildly successful facsimile. The app, which released in October 2024, earned approximately $400 million by January 2025. How did a free-to-play app manage such a feat? The psychological pull of gatcha gaming. What is gatcha gaming? The Genshin Impact character banner — in the top right, 160 Primogems can be exchanged for one Intertwined Fate, which in turn can be used to “wish” for a character. The Standard banner, the rightmost box, uses a different currency called Acquaint Fate. The wild success of the Pokemon TCG Pocket app, a way to digitally collect Pokemon cards, is arguably an indication that gatcha gaming is more mainstream than ever. The word “ gatcha ” comes from “ gashapon ” which references the Japanese toy dispensers that pop out random, often collectable toys for a couple hundred yen.",A study of gatcha games: the UX of the Pokemon TCG Pocket app,"

Key Points:
",UI/UX,"Member-only story A study of gatcha games: the UX of the Pokemon TCG Pocket app Has Pokemon given in to the evils of gatcha gaming? Is gatcha gaming evil? What is gatcha gaming? Daley Wilhelm · Follow Published in UX Collective · 9 min read · 1 day ago -- Share Pokemon cards are more accessible than ever. The closest I’ve ever been to gambling is ripping into Pokemon card packs, a vice that started in childhood and continues to eat into my disposable income today. Pokemon TCG Pocket, an app that digitally recreates the experience of ripping open booster packs and collecting cards, has proven to be a wildly successful facsimile. The app, which released in October 2024, earned approximately $400 million by January 2025. How did a free-to-play app manage such a feat? The psychological pull of gatcha gaming. What is gatcha gaming? The Genshin Impact character banner — in the top right, 160 Primogems can be exchanged for one Intertwined Fate, which in turn can be used to “wish” for a character. The Standard banner, the rightmost box, uses a different currency called Acquaint Fate. The wild success of the Pokemon TCG Pocket app, a way to digitally collect Pokemon cards, is arguably an indication that gatcha gaming is more mainstream than ever. The word “ gatcha ” comes from “ gashapon ” which references the Japanese toy dispensers that pop out random, often collectable toys for a couple hundred yen."
The iPhone 16e: a “just enough tech” innovation for value,https://uxdesign.cc/the-iphone-16e-a-just-enough-tech-innovation-for-value-8b4ce63e5635?source=rss----138adf9c44c---4,UX Collective,2025-02-26T23:17:22,UX Collective,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"The iPhone 16e: a “just enough tech” innovation for value Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. Ian Batterbee · Follow Published in UX Collective · 9 min read · 3 days ago -- 4 Listen Share iPhone 16e. Credit: Apple Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the viral meme of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough. Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the iPhone 16e . Tim Cook unveiling the newest member of the family. Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025. That said, there is something intriguing about Apple’s latest offering: an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack. The “Just Enough Tech” New Family Member On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³ Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access. As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag. Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200! OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture? Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play: Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.” “Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose. This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s Just Enough Design .⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess. Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective. Zooming Out to View the Bigger Picture The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience. Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶ Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶ “A camera so advanced, it’s like getting two in one.” — Apple. A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses. The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: Apple Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess. When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷ Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap. Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features. “Just Enough Tech” in Other Fields Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs. Let’s explore three examples proving this beyond Apple. DeepSeek: The AI Underdog Built with Less DeepSeek launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market. DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design . The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry. Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game. Notion: Productivity Without the Bloat The Notion app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity. By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.” Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰ Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value. Be My Eyes: Vision Assistance, Human Touch Be My Eyes is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs. “Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes. Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹ Be My Eyes. Credit: Be My Eyes With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users. “Just Enough Tech” is a Strategic Choice Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most. Instead of asking, “What else can we add?” we focus on, “What do people need most?” “Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out. Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions: Who are we innovating for, and what problem are we solving? How do our audience’s needs, circumstances, and cultural contexts shape their priorities? What are the essential features that provide real value without unnecessary complexity? How do we balance diverse needs while keeping the experience simple and intuitive? How do we measure whether our technology delivers “enough” value or is missing the mark? How do we decide which features to keep, refine, or remove to maximise impact? Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters. The Takeaway The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability. Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. “Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined. What’s your version of “enough”? Tell me in the comments — I’m curious! References [1]: Apple. (February 24th, 2025). Apple will spend more than $500 billion in the U.S. over the next four years https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/ [2]: The Verge. (February 21st, 2025). Apple pulls encryption feature from UK over government spying demands https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor [3], [7]: Reuters. (February 19th, 2025). Apple reveals its version of budget AI: the $599 iPhone 16e https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19 [4]: Bora — UX Collective. (October 7th, 2022). Dieter Rams and 10 principles for good design https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6 [5]: Taku Satoh. (Published October 4th, 2022). Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo https://www.goodreads.com/en/book/show/60420633-just-enough-design [6]: Business Insider. (February 19th, 2025). Apple’s new $599 iPhone with AI is the Hail Mary it needs https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2 [8]: TechTarget. (February 6th, 2025). DeepSeek explained: Everything you need to know https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know [9]: Notion. (September 3rd, 2024). 100 Million of You https://www.notion.com/blog/100-million-of-you [10]: Wayne Yap. Notion’s three philosophies https://x.com/wayneyap/status/1894337810813128795 [11]: Be My Eyes. Bringing Sight To Blind and Low-vision People https://www.bemyeyes.com [12]: Be My Eyes. Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation https://www.bemyeyes.com/blog/10-years-of-be-my-eyes","The iPhone 16e: a “just enough tech” innovation for value Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. Ian Batterbee · Follow Published in UX Collective · 9 min read · 3 days ago -- 4 Listen Share iPhone 16e. Credit: Apple Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the viral meme of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough. Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the iPhone 16e . Tim Cook unveiling the newest member of the family. Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025. That said, there is something intriguing about Apple’s latest offering: an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack. The “Just Enough Tech” New Family Member On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³ Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access. As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag. Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200! OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture? Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play: Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.” “Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose. This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s Just Enough Design .⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess. Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective. Zooming Out to View the Bigger Picture The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience. Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶ Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶ “A camera so advanced, it’s like getting two in one.” — Apple. A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses. The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: Apple Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess. When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷ Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap. Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features. “Just Enough Tech” in Other Fields Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs. Let’s explore three examples proving this beyond Apple. DeepSeek: The AI Underdog Built with Less DeepSeek launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market. DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design . The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry. Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game. Notion: Productivity Without the Bloat The Notion app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity. By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.” Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰ Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value. Be My Eyes: Vision Assistance, Human Touch Be My Eyes is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs. “Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes. Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹ Be My Eyes. Credit: Be My Eyes With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users. “Just Enough Tech” is a Strategic Choice Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most. Instead of asking, “What else can we add?” we focus on, “What do people need most?” “Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out. Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions: Who are we innovating for, and what problem are we solving? How do our audience’s needs, circumstances, and cultural contexts shape their priorities? What are the essential features that provide real value without unnecessary complexity? How do we balance diverse needs while keeping the experience simple and intuitive? How do we measure whether our technology delivers “enough” value or is missing the mark? How do we decide which features to keep, refine, or remove to maximise impact? Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters. The Takeaway The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability. Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. “Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined. What’s your version of “enough”? Tell me in the comments — I’m curious! References [1]: Apple. (February 24th, 2025). Apple will spend more than $500 billion in the U.S. over the next four years https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/ [2]: The Verge. (February 21st, 2025). Apple pulls encryption feature from UK over government spying demands https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor [3], [7]: Reuters. (February 19th, 2025). Apple reveals its version of budget AI: the $599 iPhone 16e https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19 [4]: Bora — UX Collective. (October 7th, 2022). Dieter Rams and 10 principles for good design https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6 [5]: Taku Satoh. (Published October 4th, 2022). Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo https://www.goodreads.com/en/book/show/60420633-just-enough-design [6]: Business Insider. (February 19th, 2025). Apple’s new $599 iPhone with AI is the Hail Mary it needs https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2 [8]: TechTarget. (February 6th, 2025). DeepSeek explained: Everything you need to know https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know [9]: Notion. (September 3rd, 2024). 100 Million of You https://www.notion.com/blog/100-million-of-you [10]: Wayne Yap. Notion’s three philosophies https://x.com/wayneyap/status/1894337810813128795 [11]: Be My Eyes. Bringing Sight To Blind and Low-vision People https://www.bemyeyes.com [12]: Be My Eyes. Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation https://www.bemyeyes.com/blog/10-years-of-be-my-eyes",The iPhone 16e: a “just enough tech” innovation for value,"

Key Points:
",UI/UX,"<h4><strong>Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose.</strong></h4><figure><img alt=""Close-up of the iPhone 16e’s bottom edge, showcasing the USB-C port, speaker grilles, and a sleek metallic frame. Credit: Apple."" src=""https://cdn-images-1.medium.com/max/1024/1*TWm2M5KQ7J5WRR6xTrFvsg.jpeg"" /><figcaption>iPhone 16e. Credit: <a href=""https://www.apple.com/uk/iphone-16e/"">Apple</a></figcaption></figure><p>Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the <a href=""https://www.reddit.com/r/Wellthatsucks/comments/efgcbv/not_only_did_grandpa_already_have_this_shirt_he/"">viral meme</a> of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough.</p><p>Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the <a href=""https://www.apple.com/uk/iphone-16e/?afid=p238%7Cshln14Dmp-dc_mtid_20925ukn39931_pcrid_733787381714_pgrid_182022441584_pexid__ptid_kwd-2466237873037_&amp;cid=wwa-uk-kwgo-iphone-slid---productid--NonCore--Announce-"">iPhone 16e</a>.</p><a href=""https://medium.com/media/6e916140935ead96fa497019607aff4f/href"">https://medium.com/media/6e916140935ead96fa497019607aff4f/href</a><p>Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025.</p><p>That said, there is something intriguing about Apple’s latest offering: <strong>an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack.</strong></p><h3>The “Just Enough Tech” New Family Member</h3><p>On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³</p><p>Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access.</p><p>As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag.</p><p>Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200!</p><p>OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture?</p><p>Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play:</p><blockquote><strong>Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.”</strong></blockquote><p>“Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose.</p><p>This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s <em>Just Enough Design</em>.⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess.</p><p>Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective.</p><h3>Zooming Out to View the Bigger Picture</h3><p>The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience.</p><p>Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶</p><p>Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶</p><blockquote><strong>“A camera so advanced, it’s like getting two in one.” — Apple.</strong></blockquote><p>A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses.</p><figure><img alt=""A woman in a red outfit leans on a tree branch, posing against a backdrop of vibrant yellow autumn leaves. Captured using the iPhone 16e’s camera. Credit: Apple."" src=""https://cdn-images-1.medium.com/max/1024/1*8fOAnsHQ_Zd94sCulbHZLA.jpeg"" /><figcaption>The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: <a href=""https://www.apple.com/uk/iphone-16e/"">Apple</a></figcaption></figure><p>Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess.</p><p>When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷</p><p>Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap.</p><p>Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features.</p><h3>“Just Enough Tech” in Other Fields</h3><p>Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs.</p><p>Let’s explore three examples proving this beyond Apple.</p><h4>DeepSeek: The AI Underdog Built with Less</h4><p><a href=""https://www.deepseek.com/"">DeepSeek</a> launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market.</p><blockquote>DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design<strong>.</strong></blockquote><p>The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry.</p><p>Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game.</p><h4>Notion: Productivity Without the Bloat</h4><p>The <a href=""https://www.notion.com/"">Notion</a> app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity.</p><blockquote><strong>By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.”</strong></blockquote><p>Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰</p><p>Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value.</p><h4>Be My Eyes: Vision Assistance, Human Touch</h4><p><a href=""https://www.bemyeyes.com/"">Be My Eyes </a>is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs.</p><blockquote><strong>“Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes.</strong></blockquote><p>Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹</p><figure><img alt=""A woman in a white shirt holds a smartphone frame over her eyes, aligning them with the screen cutout against a solid blue background."" src=""https://cdn-images-1.medium.com/max/811/0*LvmtyxBd6YGFVGSZ.jpg"" /><figcaption>Be My Eyes. Credit: Be My Eyes</figcaption></figure><p>With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users.</p><h3>“Just Enough Tech” is a Strategic Choice</h3><p>Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most.</p><blockquote><strong>Instead of asking, “What else can we add?” we focus on, “What do people need most?”</strong></blockquote><p>“Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out.</p><p>Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions:</p><ul><li>Who are we innovating for, and what problem are we solving?</li><li>How do our audience’s needs, circumstances, and cultural contexts shape their priorities?</li><li>What are the essential features that provide real value without unnecessary complexity?</li><li>How do we balance diverse needs while keeping the experience simple and intuitive?</li><li>How do we measure whether our technology delivers “enough” value or is missing the mark?</li><li>How do we decide which features to keep, refine, or remove to maximise impact?</li></ul><p>Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters.</p><h3>The Takeaway</h3><p>The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability.</p><blockquote><strong>Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose.</strong></blockquote><p>“Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined.</p><p>What’s your version of “enough”? Tell me in the comments — I’m curious!</p><h3>References</h3><p>[1]: Apple. (February 24th, 2025). <em>Apple will spend more than $500 billion in the U.S. over the next four years<br /></em><a href=""https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/"">https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/</a></p><p>[2]: The Verge. (February 21st, 2025). <em>Apple pulls encryption feature from UK over government spying demands<br /></em><a href=""https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor"">https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor</a></p><p>[3], [7]: Reuters. (February 19th, 2025). <em>Apple reveals its version of budget AI: the $599 iPhone 16e<br /></em><a href=""https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19"">https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19</a></p><p>[4]: Bora — UX Collective. (October 7th, 2022).<em> Dieter Rams and 10 principles for good design<br /></em><a href=""https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6"">https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6</a></p><p>[5]: Taku Satoh. (Published October 4th, 2022). <em>Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo<br /></em><a href=""https://www.goodreads.com/en/book/show/60420633-just-enough-design"">https://www.goodreads.com/en/book/show/60420633-just-enough-design</a></p><p>[6]: Business Insider. (February 19th, 2025). <em>Apple’s new $599 iPhone with AI is the Hail Mary it needs<br /></em><a href=""https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2"">https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2</a></p><p>[8]: TechTarget. (February 6th, 2025). <em>DeepSeek explained: Everything you need to know<br /></em><a href=""https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know"">https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know</a></p><p>[9]: Notion. (September 3rd, 2024). <em>100 Million of You<br /></em><a href=""https://www.notion.com/blog/100-million-of-you"">https://www.notion.com/blog/100-million-of-you</a></p><p>[10]: Wayne Yap. <em>Notion’s three philosophies<br /></em><a href=""https://x.com/wayneyap/status/1894337810813128795"">https://x.com/wayneyap/status/1894337810813128795</a></p><p>[11]: Be My Eyes. <em>Bringing Sight To Blind and Low-vision People<br /></em><a href=""https://www.bemyeyes.com/language/english"">https://www.bemyeyes.com</a></p><p>[12]: Be My Eyes. <em>Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation<br /></em><a href=""https://www.bemyeyes.com/blog/10-years-of-be-my-eyes"">https://www.bemyeyes.com/blog/10-years-of-be-my-eyes</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8b4ce63e5635"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-iphone-16e-a-just-enough-tech-innovation-for-value-8b4ce63e5635"">The iPhone 16e: a “just enough tech” innovation for value</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
"UX, how can I trust you?",https://uxdesign.cc/ux-how-can-i-trust-you-47f1c8a71213?source=rss----138adf9c44c---4,UX Collective,2025-02-26T12:32:45,UX Collective,https://plus.unsplash.com/premium_photo-1661412938808-a0f7be3c8cf1?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening.","Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening.","UX, how can I trust you?","

Key Points:
",UI/UX,"Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening."
Thinking past the cliche of LLM’s AI design patterns,https://uxdesign.cc/thinking-past-the-cliche-of-llms-ai-design-patterns-c9b849fce9e8?source=rss----138adf9c44c---4,UX Collective,2025-02-26T12:30:20,UX Collective,https://plus.unsplash.com/premium_photo-1661412938808-a0f7be3c8cf1?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Member-only story Thinking past the cliche of LLM’s AI design patterns Nowadays, each time I see AI tools I often see a copy of OpenAI’s UX framework — sidebar on the left and chat in the center right. I love patterns, but for God’s sake, can we start acting like Product Designers again? Matt Jedraszczyk · Follow Published in UX Collective · 7 min read · 3 days ago -- 2 Share Horizon AI is one of the free interface you can get without even thinking about UI Let me explain myself. I understand the need to use patterns and normalize design, and I don’t know if anyone in 2025 will be still designing sign-in / sign-up pages from the ground up. I am not talking about reinventing the wheel. Still, I use different target group’s mental models to build “familiar” designs for them, for example — if my target group has a lot of experience working with Microsoft tools — I will design something for them that would fit what they imagine a “good” design looks like, even if I am not a huge fan of the Microsoft ecosystem. I am a Product Designer, I want to explore new ways of working technology. I am not a designer who works from 9 to 5 and suddenly stops thinking about the digital world that surrounds me — I love to explore new opportunities — and that's why I am so disappointed with the experiences we have with different AI Tools so far — they are repetitive and each new tool try to …","Member-only story Thinking past the cliche of LLM’s AI design patterns Nowadays, each time I see AI tools I often see a copy of OpenAI’s UX framework — sidebar on the left and chat in the center right. I love patterns, but for God’s sake, can we start acting like Product Designers again? Matt Jedraszczyk · Follow Published in UX Collective · 7 min read · 3 days ago -- 2 Share Horizon AI is one of the free interface you can get without even thinking about UI Let me explain myself. I understand the need to use patterns and normalize design, and I don’t know if anyone in 2025 will be still designing sign-in / sign-up pages from the ground up. I am not talking about reinventing the wheel. Still, I use different target group’s mental models to build “familiar” designs for them, for example — if my target group has a lot of experience working with Microsoft tools — I will design something for them that would fit what they imagine a “good” design looks like, even if I am not a huge fan of the Microsoft ecosystem. I am a Product Designer, I want to explore new ways of working technology. I am not a designer who works from 9 to 5 and suddenly stops thinking about the digital world that surrounds me — I love to explore new opportunities — and that's why I am so disappointed with the experiences we have with different AI Tools so far — they are repetitive and each new tool try to …",Thinking past the cliche of LLM’s AI design patterns,"

Key Points:
",UI/UX,"Member-only story Thinking past the cliche of LLM’s AI design patterns Nowadays, each time I see AI tools I often see a copy of OpenAI’s UX framework — sidebar on the left and chat in the center right. I love patterns, but for God’s sake, can we start acting like Product Designers again? Matt Jedraszczyk · Follow Published in UX Collective · 7 min read · 3 days ago -- 2 Share Horizon AI is one of the free interface you can get without even thinking about UI Let me explain myself. I understand the need to use patterns and normalize design, and I don’t know if anyone in 2025 will be still designing sign-in / sign-up pages from the ground up. I am not talking about reinventing the wheel. Still, I use different target group’s mental models to build “familiar” designs for them, for example — if my target group has a lot of experience working with Microsoft tools — I will design something for them that would fit what they imagine a “good” design looks like, even if I am not a huge fan of the Microsoft ecosystem. I am a Product Designer, I want to explore new ways of working technology. I am not a designer who works from 9 to 5 and suddenly stops thinking about the digital world that surrounds me — I love to explore new opportunities — and that's why I am so disappointed with the experiences we have with different AI Tools so far — they are repetitive and each new tool try to …"
User Research is not optional: arguing like Socrates will help you prove it,https://uxdesign.cc/user-research-is-not-optional-arguing-like-socrates-will-help-you-prove-it-6af555db12be?source=rss----138adf9c44c---4,UX Collective,2025-02-26T12:28:53,UX Collective,https://images.unsplash.com/photo-1629752187687-3d3c7ea3a21b?q=80&w=3571&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"User Research is not optional: arguing like Socrates will help you prove it User Research (UR) is essential for building great digital products — yet many leaders like Henry Ford and Steve Jobs said it’s unnecessary. This article reexamines outdated beliefs while making the case for user research using the Socratic Method. Joe Smiley · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Listen Share Illustration: João Casaca User Research (UR) should be something we all agree on that is critical to building the best digital products. But it’s not, and it seems to get more contentious every year. I’ve had millions of arguments in my career about user research, and I’ve actually argued for and against it. But now that I’m older and wiser, I’m 100% aligned that user research is critical for product development. You should be performing User Research for all digital products — new and existing — especially if those products are externally facing with clients, customers, or the general public. Research provides the critical data you need to make informed product decisions around features, usability, and the wants/needs of your target audience. You’re flying blind without doing any research. Let’s start by dissecting some old arguments of brilliant innovators who were against doing User Research, and then I’ll share my fool proof argument to help educate and prepare you for the inevitable conversation with leaders who will want to remove User Research to move “faster” and “save money.” Henry Ford’s argument against user research: revolutionary vs evolutionary products Henry Ford, the CEO of the Ford Motor Company, guided the production of the Model T car back in 1908 to be the first affordable, easy to operate, and mass-produced car in America. It was an instant success, and Ford often spoke about developing the Model T without research, saying “If I had asked people what they wanted, they would have said faster horses.” What he was saying is that most people can’t see beyond the products they utilize day-to-day, except the issues they have with them. Customers would likely ask for improvements to existing technology if you ask them about it (i.e. a faster horse) instead of asking for revolutionary new products. Ford Model T and a horse. Generated using Amazon Titan. Ford’s argument kinda makes sense, right? The problem with Ford’s argument is that most of us don’t work on revolutionary new products that ordinary people would have a hard time imagining, let alone understanding. Most product companies and teams are incrementally improving or evolving current digital products into something slightly better, but rarely working on anything that could be called “revolutionary.” Ford happened to live in an era where everything was revolutionary, so his argument made more sense when he said it in the early 1900’s. There really wasn’t any “technology” prior to that other than the telephone and steam engine, so every invention was considered “revolutionary.” source: IDEO Human Centered Design Handbook Even for people who do have incredible jobs that are building extraordinary innovations, there is Design Thinking that will help guide this development with Human Centered Design. They obviously didn’t have this back in the early 1900’s, but it exists today. And it involves lots of user research. Steve Jobs’ argument against user research: consumers don’t know what they want The other argument is the famous Steve Jobs quote that “People don’t know what they want until you show it to them. That’s why I never rely on market research. Our task is to read things that are not yet on the page.” This argument also seems pretty straightforward, and is pretty similar to the evolutionary vs revolutionary argument. But the difference here is that this one, is well, uh, utterly idiotic. Of course people don’t know what you want until you show it to them! That’s exactly what Agile product development and User Research is for, to quickly show your target audience new product ideas that they hopefully want and need. But to ignore User Research and just launch things into the market blindly is essentially the old Waterfall development method, which left a vast graveyard of failed products in it’s wake. Most innovation happens within startups, and if you look at the success rate of startups then you’ll realize why innovation is almost impossible without User Research. The fact is 90% of all startups fail, and the #1 reason is because they fail to identify a market for their product and/or service! source: CBInsights That is, startups don’t do enough market or User Research prior to designing, developing, and deploying their product. They dive in, spend a lot of time and money creating a product without talking to users, and then launch it into the market only to watch it fail miserably. The only argument you need for user research: buying a house The best way to win arguments about anything is to help people understand it at their level. For example, it’s a lot easier to prove how extreme the spending is in Congress by simply comparing the federal budget to that of a typical household budget. source: Congressional Budget Office This tactic is really valuable, because most of the time you’ll be arguing with a SVP or VP, or maybe even a product manager (PM) who is trying to speed up the process or save money. So you already have an uphill battle trying to prove the value of research. The argument for User Research is simple, and it makes it even more powerful if you utilize the Socratic method, which is named after the Athenian philosopher Socrates who used simple questions to challenges ideas and beliefs. Elon Musk (CEO of Tesla, SpaceX, X, Neuralink, and Boring Hole) often uses first principles thinking — which is a form of the Socratic method — by breaking problems down into fundamental truths and then reasoning up from there. This has helped to propel him to become one of the premier innovators of the 21st century in electric vehicles, space travel, and AI. Here’s a simplified view of how to utilize the Socratic Method: Wonder: Receive what the other person says, listen to their view or premise. Reflect: Sum up the other person’s view and clarify your understanding. Refine & Cross Examine: Ask the person to provide evidence that supports their view. Discover the thoughts, assumptions, and facts underlying their beliefs. Challenge these assumptions to test their validity. Restate: Note the new assumption resulting from your inquiry. Repeat: Start back at the beginning with the new assumptions. Illustration: Joe Smiley To help illustrate this, imagine your boss is asking you to remove User Research from a project to save time and money, where you’ll start by listening to their perspective and determining their assumptions as to why they would prefer to remove User Research. Once you’re at Step 3 of the Socratic Method of Redefine and Cross-Examine, you’ll want to challenge their assumptions that User Research is a waste of time and money. Start by asking your boss if they’ve ever lived in house before? Unless you’re reading this from the Amazon rain forest, I’m hoping that 99% of people you’re going to argue with will say yes. Next, ask them how many houses they’ve lived in? Just an estimate is fine. Again, most people will say 2–3 houses or more. Continue diving in further, where you’ll ask them “have you ever bought a house before?” Most people have bought at least one house, if not a few in their lifetime. source: Bankrate Then ask them “How did you buy the house? Did you walk up and buy the first house you saw, or did you do some research first on neighborhoods and then look at a lot of houses within a desired neighborhood? Did you use a realtor to help with this research or did you do it all by yourself?” And once they admit to getting a realtor, this is where you really dig in… “Wait, you told me you’ve lived in houses your entire life and even bought a home before, which means you’re a certifiable expert on houses, so why did you pay a lot of money to hire a realtor? Why not walk up to the first one you saw and just buy it? Seems like you wasted a lot of time and money on a realtor when you could have done it yourself, riiiiiight?” Hopefully your boss is having an “aha” moment. They should understand that while our product design and development team is highly experienced, we still need User Researchers to ensure our products meet customer needs while lowering our risk of developing ineffective and/or unusable features. source: Vecteezy My final key point in the home buying analogy is that many small and mid-sized companies spend the equivalent of a home purchase ($500,000–$1,000,000) each sprint on product development salaries and overhead. Larger companies like Google invest billions annually in product development! You wouldn’t risk your life’s savings when buying a house, and so why would companies blindly bet millions or billions every sprint on the chance that their product ideas are successful? It’s clear that User Research is not up for debate — it’s a foundational practice that ensures digital products are built with purpose, insight, and a clear understanding of your users’ needs. Dismissing it to save time or money is a short-sighted strategy that ultimately leads to wasted resources and failed products. I’ve always loved the Socratic Method because it provides an invaluable tool in advocating for User Research. By guiding skeptics through their own reasoning — using relatable analogies like buying a house — you can help them realize that research is not an impediment but a catalyst for building better products. Just as no one blindly purchases a home without research and/or a realtor, no company should blindly develop digital products without understanding their users. Always remember the Nielsen Norman formula, UX — U = X , where “X” now means “don’t do it .” source: Neilsen Norman Group So the next time someone challenges the need for User Research, don’t just argue — utilize the Socratic Method to ask questions, lead them to the logical conclusion, and let them see for themselves why research is not optional. Ultimately, the companies that invest in User Research are the ones that create products with real impact while saving time and money in the long run by avoiding unnecessary risks.","User Research is not optional: arguing like Socrates will help you prove it User Research (UR) is essential for building great digital products — yet many leaders like Henry Ford and Steve Jobs said it’s unnecessary. This article reexamines outdated beliefs while making the case for user research using the Socratic Method. Joe Smiley · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Listen Share Illustration: João Casaca User Research (UR) should be something we all agree on that is critical to building the best digital products. But it’s not, and it seems to get more contentious every year. I’ve had millions of arguments in my career about user research, and I’ve actually argued for and against it. But now that I’m older and wiser, I’m 100% aligned that user research is critical for product development. You should be performing User Research for all digital products — new and existing — especially if those products are externally facing with clients, customers, or the general public. Research provides the critical data you need to make informed product decisions around features, usability, and the wants/needs of your target audience. You’re flying blind without doing any research. Let’s start by dissecting some old arguments of brilliant innovators who were against doing User Research, and then I’ll share my fool proof argument to help educate and prepare you for the inevitable conversation with leaders who will want to remove User Research to move “faster” and “save money.” Henry Ford’s argument against user research: revolutionary vs evolutionary products Henry Ford, the CEO of the Ford Motor Company, guided the production of the Model T car back in 1908 to be the first affordable, easy to operate, and mass-produced car in America. It was an instant success, and Ford often spoke about developing the Model T without research, saying “If I had asked people what they wanted, they would have said faster horses.” What he was saying is that most people can’t see beyond the products they utilize day-to-day, except the issues they have with them. Customers would likely ask for improvements to existing technology if you ask them about it (i.e. a faster horse) instead of asking for revolutionary new products. Ford Model T and a horse. Generated using Amazon Titan. Ford’s argument kinda makes sense, right? The problem with Ford’s argument is that most of us don’t work on revolutionary new products that ordinary people would have a hard time imagining, let alone understanding. Most product companies and teams are incrementally improving or evolving current digital products into something slightly better, but rarely working on anything that could be called “revolutionary.” Ford happened to live in an era where everything was revolutionary, so his argument made more sense when he said it in the early 1900’s. There really wasn’t any “technology” prior to that other than the telephone and steam engine, so every invention was considered “revolutionary.” source: IDEO Human Centered Design Handbook Even for people who do have incredible jobs that are building extraordinary innovations, there is Design Thinking that will help guide this development with Human Centered Design. They obviously didn’t have this back in the early 1900’s, but it exists today. And it involves lots of user research. Steve Jobs’ argument against user research: consumers don’t know what they want The other argument is the famous Steve Jobs quote that “People don’t know what they want until you show it to them. That’s why I never rely on market research. Our task is to read things that are not yet on the page.” This argument also seems pretty straightforward, and is pretty similar to the evolutionary vs revolutionary argument. But the difference here is that this one, is well, uh, utterly idiotic. Of course people don’t know what you want until you show it to them! That’s exactly what Agile product development and User Research is for, to quickly show your target audience new product ideas that they hopefully want and need. But to ignore User Research and just launch things into the market blindly is essentially the old Waterfall development method, which left a vast graveyard of failed products in it’s wake. Most innovation happens within startups, and if you look at the success rate of startups then you’ll realize why innovation is almost impossible without User Research. The fact is 90% of all startups fail, and the #1 reason is because they fail to identify a market for their product and/or service! source: CBInsights That is, startups don’t do enough market or User Research prior to designing, developing, and deploying their product. They dive in, spend a lot of time and money creating a product without talking to users, and then launch it into the market only to watch it fail miserably. The only argument you need for user research: buying a house The best way to win arguments about anything is to help people understand it at their level. For example, it’s a lot easier to prove how extreme the spending is in Congress by simply comparing the federal budget to that of a typical household budget. source: Congressional Budget Office This tactic is really valuable, because most of the time you’ll be arguing with a SVP or VP, or maybe even a product manager (PM) who is trying to speed up the process or save money. So you already have an uphill battle trying to prove the value of research. The argument for User Research is simple, and it makes it even more powerful if you utilize the Socratic method, which is named after the Athenian philosopher Socrates who used simple questions to challenges ideas and beliefs. Elon Musk (CEO of Tesla, SpaceX, X, Neuralink, and Boring Hole) often uses first principles thinking — which is a form of the Socratic method — by breaking problems down into fundamental truths and then reasoning up from there. This has helped to propel him to become one of the premier innovators of the 21st century in electric vehicles, space travel, and AI. Here’s a simplified view of how to utilize the Socratic Method: Wonder: Receive what the other person says, listen to their view or premise. Reflect: Sum up the other person’s view and clarify your understanding. Refine & Cross Examine: Ask the person to provide evidence that supports their view. Discover the thoughts, assumptions, and facts underlying their beliefs. Challenge these assumptions to test their validity. Restate: Note the new assumption resulting from your inquiry. Repeat: Start back at the beginning with the new assumptions. Illustration: Joe Smiley To help illustrate this, imagine your boss is asking you to remove User Research from a project to save time and money, where you’ll start by listening to their perspective and determining their assumptions as to why they would prefer to remove User Research. Once you’re at Step 3 of the Socratic Method of Redefine and Cross-Examine, you’ll want to challenge their assumptions that User Research is a waste of time and money. Start by asking your boss if they’ve ever lived in house before? Unless you’re reading this from the Amazon rain forest, I’m hoping that 99% of people you’re going to argue with will say yes. Next, ask them how many houses they’ve lived in? Just an estimate is fine. Again, most people will say 2–3 houses or more. Continue diving in further, where you’ll ask them “have you ever bought a house before?” Most people have bought at least one house, if not a few in their lifetime. source: Bankrate Then ask them “How did you buy the house? Did you walk up and buy the first house you saw, or did you do some research first on neighborhoods and then look at a lot of houses within a desired neighborhood? Did you use a realtor to help with this research or did you do it all by yourself?” And once they admit to getting a realtor, this is where you really dig in… “Wait, you told me you’ve lived in houses your entire life and even bought a home before, which means you’re a certifiable expert on houses, so why did you pay a lot of money to hire a realtor? Why not walk up to the first one you saw and just buy it? Seems like you wasted a lot of time and money on a realtor when you could have done it yourself, riiiiiight?” Hopefully your boss is having an “aha” moment. They should understand that while our product design and development team is highly experienced, we still need User Researchers to ensure our products meet customer needs while lowering our risk of developing ineffective and/or unusable features. source: Vecteezy My final key point in the home buying analogy is that many small and mid-sized companies spend the equivalent of a home purchase ($500,000–$1,000,000) each sprint on product development salaries and overhead. Larger companies like Google invest billions annually in product development! You wouldn’t risk your life’s savings when buying a house, and so why would companies blindly bet millions or billions every sprint on the chance that their product ideas are successful? It’s clear that User Research is not up for debate — it’s a foundational practice that ensures digital products are built with purpose, insight, and a clear understanding of your users’ needs. Dismissing it to save time or money is a short-sighted strategy that ultimately leads to wasted resources and failed products. I’ve always loved the Socratic Method because it provides an invaluable tool in advocating for User Research. By guiding skeptics through their own reasoning — using relatable analogies like buying a house — you can help them realize that research is not an impediment but a catalyst for building better products. Just as no one blindly purchases a home without research and/or a realtor, no company should blindly develop digital products without understanding their users. Always remember the Nielsen Norman formula, UX — U = X , where “X” now means “don’t do it .” source: Neilsen Norman Group So the next time someone challenges the need for User Research, don’t just argue — utilize the Socratic Method to ask questions, lead them to the logical conclusion, and let them see for themselves why research is not optional. Ultimately, the companies that invest in User Research are the ones that create products with real impact while saving time and money in the long run by avoiding unnecessary risks.",User Research is not optional: arguing like Socrates will help you prove it,"

Key Points:
",UI/UX,"<h4>User Research (UR) is essential for building great digital products — yet many leaders like Henry Ford and Steve Jobs said it’s unnecessary. This article reexamines outdated beliefs while making the case for user research using the Socratic Method.</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*YgCvoReJIxhAFyf7Iq-dKQ.png"" /><figcaption>Illustration:<a href=""https://dribbble.com/jtcasaca""> João Casaca</a></figcaption></figure><p><strong>User Research (UR) should be something we all agree on that is critical to building the best digital products. But it’s not, and it seems to get more contentious every year. I’ve had millions of arguments in my career about user research, and I’ve actually argued for and against it.</strong></p><p><em>But now that I’m older and wiser, I’m 100% aligned that user research is critical for product development.</em></p><figure><img alt=""graphic of a Don Norman quote that says “If you want to create a great product, you have to start by understanding the people who will use it.”"" src=""https://cdn-images-1.medium.com/max/977/0*OtpHvaimPGlRWLtj"" /></figure><p>You should be performing User Research for all digital products — new and existing — especially if those products are externally facing with clients, customers, or the general public. Research provides the critical data you need to make informed product decisions around features, usability, and the wants/needs of your target audience.</p><p><em>You’re flying blind without doing any research.</em></p><p>Let’s start by dissecting some old arguments of brilliant innovators who were against doing User Research, and then I’ll share my fool proof argument to help educate and prepare you for the inevitable conversation with leaders who will want to remove User Research to move “faster” and “save money.”</p><h3>Henry Ford’s argument against user research: revolutionary vs evolutionary products</h3><p>Henry Ford, the CEO of the Ford Motor Company, guided the production of the Model T car back in 1908 to be the first affordable, easy to operate, and mass-produced car in America. It was an instant success, and Ford often spoke about developing the Model T without research, saying “If I had asked people what they wanted, they would have said faster horses.”</p><p>What he was saying is that most people can’t see beyond the products they utilize day-to-day, except the issues they have with them. Customers would likely ask for improvements to existing technology if you ask them about it (i.e. a faster horse) instead of asking for revolutionary new products.</p><figure><img alt=""Photo of a Ford Model T next to a horse."" src=""https://cdn-images-1.medium.com/max/1024/0*lccE62YSNTDTwYCW"" /><figcaption>Ford Model T and a horse. Generated using Amazon Titan.</figcaption></figure><p>Ford’s argument kinda makes sense, right?</p><p><em>The problem with Ford’s argument is that most of us don’t work on revolutionary new products that ordinary people would have a hard time imagining, let alone understanding.</em></p><p>Most product companies and teams are incrementally improving or evolving current digital products into something slightly better, but rarely working on anything that could be called “revolutionary.” Ford happened to live in an era where everything was revolutionary, so his argument made more sense when he said it in the early 1900’s. There really wasn’t any “technology” prior to that other than the telephone and steam engine, so every invention was considered “revolutionary.”</p><figure><img alt=""Chart showing evolutionary vs revolutionary product development"" src=""https://cdn-images-1.medium.com/max/882/0*NodXzfAxdEjV3OoN"" /><figcaption>source: IDEO Human Centered Design Handbook</figcaption></figure><p>Even for people who do have incredible jobs that are building extraordinary innovations, there is Design Thinking that will help guide this development with Human Centered Design. They obviously didn’t have this back in the early 1900’s, but it exists today. And it involves lots of user research.</p><h3>Steve Jobs’ argument against user research: consumers don’t know what they want</h3><p>The other argument is the famous Steve Jobs quote that “People don’t know what they want until you show it to them. That’s why I never rely on market research. Our task is to read things that are not yet on the page.”</p><figure><img alt=""famous Steve Jobs quote that “People don’t know what they want until you show it to them.”"" src=""https://cdn-images-1.medium.com/max/1024/0*na393-M55IHKhyT6"" /></figure><p>This argument also seems pretty straightforward, and is pretty similar to the evolutionary vs revolutionary argument. But the difference here is that this one, is well, uh, utterly idiotic.</p><p><em>Of course people don’t know what you want until you show it to them!</em></p><p><em>That’s exactly what Agile product development and User Research is for, to quickly show your target audience new product ideas that they hopefully want and need. But to ignore User Research and just launch things into the market blindly is essentially the old Waterfall development method, which left a vast graveyard of failed products in it’s wake.</em></p><p>Most innovation happens within startups, and if you look at the success rate of startups then you’ll realize why innovation is almost impossible without User Research. The fact is 90% of all startups fail, and the #1 reason is because they fail to identify a market for their product and/or service!</p><figure><img alt=""graphic showing a statistic that 42% of startups fail because they misread the market"" src=""https://cdn-images-1.medium.com/max/592/0*vYVuL9LcWi3bZJfR.png"" /><figcaption>source: CBInsights</figcaption></figure><p>That is, startups don’t do enough market or User Research prior to designing, developing, and deploying their product. They dive in, spend a lot of time and money creating a product without talking to users, and then launch it into the market only to watch it fail miserably.</p><h3>The only argument you need for user research: buying a house</h3><p>The best way to win arguments about anything is to help people understand it at their level.</p><p>For example, it’s a lot easier to prove how extreme the spending is in Congress by simply comparing the federal budget to that of a typical household budget.</p><figure><img alt=""chart comparing the federal budget to that of a typical household budget"" src=""https://cdn-images-1.medium.com/max/910/0*p_TA_coBgKOiLY8-"" /><figcaption>source: Congressional Budget Office</figcaption></figure><p>This tactic is really valuable, because most of the time you’ll be arguing with a SVP or VP, or maybe even a product manager (PM) who is trying to speed up the process or save money. So you already have an uphill battle trying to prove the value of research.</p><p>The argument for User Research is simple, and it makes it even more powerful if you utilize the Socratic method, which is named after the Athenian philosopher Socrates who used simple questions to challenges ideas and beliefs.</p><p>Elon Musk (CEO of Tesla, SpaceX, X, Neuralink, and Boring Hole) often uses first principles thinking — which is a form of the Socratic method — by breaking problems down into fundamental truths and then reasoning up from there. This has helped to propel him to become one of the premier innovators of the 21st century in electric vehicles, space travel, and AI.</p><p>Here’s a simplified view of how to utilize the Socratic Method:</p><ol><li><strong>Wonder: </strong>Receive what the other person says, listen to their view or premise.</li><li><strong>Reflect: </strong>Sum up the other person’s view and clarify your understanding.</li><li><strong>Refine &amp; Cross Examine: </strong>Ask the person to provide evidence that supports their view. Discover the thoughts, assumptions, and facts underlying their beliefs. Challenge these assumptions to test their validity.</li><li><strong>Restate: </strong>Note the new assumption resulting from your inquiry.</li><li><strong>Repeat: </strong>Start back at the beginning with the new assumptions.</li></ol><figure><img alt=""graphic showing the 5 steps of utilizing the Socratic Method: wonder, reflect, refine &amp; cross-examine, restate, and repeat"" src=""https://cdn-images-1.medium.com/max/1018/0*NuOWPJmeYGZ_V_wm"" /><figcaption>Illustration: Joe Smiley</figcaption></figure><p>To help illustrate this, imagine your boss is asking you to remove User Research from a project to save time and money, where you’ll start by listening to their perspective and determining their assumptions as to why they would prefer to remove User Research.</p><p>Once you’re at Step 3 of the Socratic Method of Redefine and Cross-Examine, you’ll want to challenge their assumptions that User Research is a waste of time and money. Start by asking your boss if they’ve ever lived in house before? Unless you’re reading this from the Amazon rain forest, I’m hoping that 99% of people you’re going to argue with will say yes.</p><p>Next, ask them how many houses they’ve lived in? Just an estimate is fine. Again, most people will say 2–3 houses or more.</p><p>Continue diving in further, where you’ll ask them “have you ever bought a house before?” Most people have bought at least one house, if not a few in their lifetime.</p><figure><img alt=""Photo of a home with a for sale sign in front of it"" src=""https://cdn-images-1.medium.com/max/1024/1*ZpsrhB1KWGddgsSE9WdtIA.jpeg"" /><figcaption>source: Bankrate</figcaption></figure><p>Then ask them “How did you buy the house? Did you walk up and buy the first house you saw, or did you do some research first on neighborhoods and then look at a lot of houses within a desired neighborhood? Did you use a realtor to help with this research or did you do it all by yourself?”</p><p>And once they admit to getting a realtor, this is where you really dig in… “Wait, you told me you’ve lived in houses your entire life and even bought a home before, which means you’re a certifiable expert on houses, so why did you pay a lot of money to hire a realtor? Why not walk up to the first one you saw and just buy it? Seems like you wasted a lot of time and money on a realtor when you could have done it yourself, riiiiiight?”</p><p><em>Hopefully your boss is having an “aha” moment.</em></p><p>They should understand that while our product design and development team is highly experienced, we still need User Researchers to ensure our products meet customer needs while lowering our risk of developing ineffective and/or unusable features.</p><figure><img alt=""illustration of person trying to lower their risk"" src=""https://cdn-images-1.medium.com/max/1024/1*CiD73R_UxuYlfKEQ6I5KpQ.jpeg"" /><figcaption>source: Vecteezy</figcaption></figure><p>My final key point in the home buying analogy is that many small and mid-sized companies spend the equivalent of a home purchase ($500,000–$1,000,000) each sprint on product development salaries and overhead. Larger companies like Google invest billions annually in product development!</p><p>You wouldn’t risk your life’s savings when buying a house, and so why would companies blindly bet millions or billions every sprint on the chance that their product ideas are successful?</p><p>It’s clear that User Research is not up for debate — it’s a foundational practice that ensures digital products are built with purpose, insight, and a clear understanding of your users’ needs. Dismissing it to save time or money is a short-sighted strategy that ultimately leads to wasted resources and failed products.</p><p>I’ve always loved the Socratic Method because it provides an invaluable tool in advocating for User Research. By guiding skeptics through their own reasoning — using relatable analogies like buying a house — you can help them realize that research is not an impediment but a catalyst for building better products. Just as no one blindly purchases a home without research and/or a realtor, no company should blindly develop digital products without understanding their users. Always remember the Nielsen Norman formula, <strong>UX — U = X</strong>, where “X” now means <em>“don’t do it</em>.”</p><figure><img alt=""graphic showing the Nielsen Norman formula, UX — U = X, where “X” means “don’t do it.”"" src=""https://cdn-images-1.medium.com/max/1023/0*-5JSHWymfTkcsUsd"" /><figcaption>source: Neilsen Norman Group</figcaption></figure><p>So the next time someone challenges the need for User Research, don’t just argue — utilize the Socratic Method to ask questions, lead them to the logical conclusion, and let them see for themselves why research is not optional.</p><p>Ultimately, the companies that invest in User Research are the ones that create products with real impact while saving time and money in the long run by avoiding unnecessary risks.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6af555db12be"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/user-research-is-not-optional-arguing-like-socrates-will-help-you-prove-it-6af555db12be"">User Research is not optional: arguing like Socrates will help you prove it</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
The AI layer: transforming UX design from tools to intelligence,https://uxdesign.cc/the-ai-layer-transforming-ux-design-from-tools-to-intelligence-7d6de37483cd?source=rss----138adf9c44c---4,UX Collective,2025-02-25T12:47:50,UX Collective,https://plus.unsplash.com/premium_photo-1661412938808-a0f7be3c8cf1?q=80&w=3570&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"The AI layer: transforming UX design from tools to intelligence A practical framework for implementing AI as a foundational layer in digital products Adrian Levy · Follow Published in UX Collective · 8 min read · 4 days ago -- 2 Listen Share Figure 1: Conceptual representation of AI as a horizontal enabling layer, illustrating its transformative nature similar to electricity in the 20th century. The bright horizontal line represents the AI layer that powers and connects multiple applications and systems, represented by interconnected nodes. Image generated via Krea.ai. Introduction “AI is a horizontal enabling layer — it can be used to improve everything. It will be in everything,” Jeff Bezos declared, comparing AI to the transformative power of electricity. “These kinds of horizontal layers like electricity, compute, and now artificial intelligence, they go everywhere. I guarantee you there is not a single application that you can think of that is not going to be made better by AI.” Just as electricity revolutionized every industry by becoming a foundational utility, AI is fundamentally reshaping our digital landscape in ways that go far beyond simple automation. At Amazon alone, teams are working on “ literally a thousand applications internally ,” demonstrating AI’s potential for widespread integration. As UX professionals, we’re at the forefront of this transformation , tasked with creating interfaces that make these powerful capabilities accessible and meaningful to users. Through an analysis of pioneering applications, we can establish a framework for designing AI-driven products . These examples serve as both a conceptual model and a practical guide for structuring products where AI acts as a foundational layer. Beyond showcasing technological advancement, this analysis provides a systematic approach to identifying AI opportunities within your own products. Case 1: Perplexity AI From manual information assembly to intelligent discovery The Challenge: Breaking free from traditional search “AI is the first truly new interaction-design paradigm in 60 years,” observes Jakob Nielsen , and nowhere is this more evident than in information search. Traditional search engines, despite their sophistication, still required users to master a complex dance: crafting precise queries, scanning multiple results, clicking through various pages, and mentally synthesizing information. This process, while familiar, placed a significant cognitive burden on users. Figure 2: Traditional search interface. A Google search interface showing the traditional approach where users must manually scan, filter, and synthesize information from multiple sources. Note the cognitive load required to process and connect information. The transformation: Reimagining search with AI Perplexity AI embodies this transformation by fundamentally reimagining how humans interact with information. Instead of users adapting to the system’s requirements, the AI adapts to user intent . Figure 3: Perplexity AI Interface. Perplexity AI’s interface demonstrates the shift from tool-based search to intelligent discovery. Note how it automatically synthesizes information and presents structured comparisons. How it works: The intelligence layer The system processes queries through interconnected layers that understand context, verify information in real-time, and present synthesized answers that feel natural and conversational. Figure 4: System architecture diagram. Perplexity AI’s information discovery system, showing how natural language queries are processed through multiple AI layers for comprehensive, verified answers. Source: Author. Key Components: Query understanding layer : Processes natural language input Information synthesis layer : Connects and verifies multiple sources Response generation layer : Creates coherent, contextual answers Interaction management layer : Maintains conversation flow Impact on user experience The result isn’t just a faster search engine — it’s a new paradigm for knowledge discovery that feels more like consulting a knowledgeable colleague than operating a digital tool. Users can: Ask questions naturally without worrying about keywords Receive comprehensive, synthesized answers Follow up with contextual questions Verify sources and facts in real-time This transformation shows how AI can fundamentally change core user interactions, moving beyond mere automation to create truly intelligent systems that adapt to human needs rather than requiring humans to adapt to them. Case 2: NotebookLM From document management to knowledge orchestration The challenge: Beyond digital paper Traditional research and note-taking tools merely digitized the paper experience while maintaining its fundamental limitations. Knowledge workers and researchers faced persistent challenges: Manual information organization Limited connections between documents Difficulty maintaining coherent management systems Cognitive overload when processing multiple sources Figure 5: Traditional research process. Traditional document management approach showing multiple windows, manual note-taking, and disconnected information sources. Researchers must manually create connections and maintain organization systems. Image generated via Krea.ai. The transformation: AI as a knowledge partner NotebookLM transforms this experience by implementing AI as an intelligent collaboration layer. Instead of being a passive repository, the system: Actively participates in the knowledge work process Automatically maps relationships between documents Suggests connections you might have missed Adapts its organization to your thinking patterns Figure 6: NotebookLM Interface. NotebookLM’s dynamic workspace showing AI-powered suggestions, automatic relationship mapping, and contextual document organization. How it works: The intelligence layer NotebookLM transforms document management through a three-layer intelligence system. The User interaction layer handles direct user engagement through document uploads, natural queries, and note-taking capabilities. The Knowledge processing layer — the system’s core — analyzes documents, maps connections, and synthesizes information using advanced AI algorithms. Finally, the Intelligent output layer presents this processed information as connected insights, related concepts, and research suggestions, creating a dynamic system that actively enhances the research and learning process. Figure 7: Knowledge orchestration system. Visualization of NotebookLM’s approach to transforming document management into an intelligent knowledge system . Source: Author. Impact on research and knowledge work This transformation creates a dynamic workspace that: Feels less like a digital filing cabinet and more like a thinking partner Enhances natural workflow while maintaining flexibility Reduces the cognitive load of organization Facilitates serendipitous discoveries NotebookLM represents a fundamental shift in how we interact with information: from passive document management to active collaboration with a system that understands and amplifies our thought processes. This transformation demonstrates how AI can serve as more than just a tool — it becomes an intelligent partner in the knowledge work process, while maintaining the user’s autonomy and enhancing their natural cognitive workflows. Case 3: Genway AI From individual interviews to scalable intelligence The challenge: Breaking traditional research boundaries The traditional UX research process has remained largely unchanged for decades, requiring intensive manual effort at every stage. Researchers face significant limitations: Time-consuming participant recruitment Limited interview capacity Manual analysis of responses Labor-intensive synthesis of findings Trade-offs between depth and breadth of research Figure 8: Traditional UX Research Session. A traditional user research session showing researchers conducting a one-on-one interview. This manual approach, while thorough, illustrates the core limitation of traditional UX research: the inability to scale without compromising depth of insights. Image generated via Krea.ai. The transformation: AI-enabled research at scale Genway AI revolutionizes this paradigm by implementing AI as a horizontal enabling layer across the entire research workflow. The system: Conducts multiple human-like interviews simultaneously Processes multiple data streams in real-time Analyzes responses across voice, text, and video Synthesizes insights automatically while maintaining research integrity Figure 9: Genway AI Interface. Genway AI’s interface showing parallel interview processing, real-time analysis, and automated insight generation while maintaining human oversight. How it works: The intelligence layer Genway AI operates through a comprehensive three-layer system that transforms traditional user research into a scalable intelligence operation. The Data collection layer captures multiple streams of user input through voice, text, and video channels simultaneously. The AI processing core analyzes this data in real-time, recognizing patterns, performing sentiment analysis, and generating insights. The Research insights layer then delivers both quantitative analysis and qualitative insights, automatically detecting trends and providing strategic recommendations. Figure 10: Research intelligence framework. Visualization of Genway AI’s approach to scaling user research, showing how multiple data streams are processed simultaneously through AI analysis layers. Source: Author. Impact on UX research This transformation fundamentally reshapes UX research capabilities through: Scale with depth : Enables simultaneous processing of hundreds of interviews while maintaining the nuanced understanding essential to quality research. The system can process multiple data streams without sacrificing the depth of analysis traditionally associated with one-on-one interviews. Real-time intelligence : Combines immediate insight generation during research sessions with sophisticated pattern recognition across large datasets, enabling researchers to adapt and refine their approach during the research process. Augmented expertise : Creates a balanced synergy between automation and human expertise, where AI handles data processing and pattern identification while researchers focus on strategic interpretation and decision-making. Genway AI demonstrates how artificial intelligence can transform UX research from a linear, resource-constrained process into a dynamic, scalable system that amplifies rather than replaces human research capabilities. This transformation maintains the nuanced understanding essential to user research while dramatically expanding its scope and efficiency. The impact on UX design and future implications Our analysis of these groundbreaking applications reveals fundamental patterns reshaping the future of UX design. Just as electricity transformed every industry it touched, AI is creating new paradigms for how we think about and design digital experiences. As Jakob Nielsen observes , “the best design for AI will retain some of the old graphical user interface elements, resulting in a hybrid UI, mostly based on user intent. But iterations including tweaks and revisions, specified through GUI commands.” This insight reveals a crucial principle for designing AI-powered products: the need to balance revolutionary capabilities with familiar interaction patterns . How AI is reshaping digital experiences AI isn’t simply adding features to existing tools — it’s fundamentally transforming how we design digital experiences. As Henry Modisett , head of design at Perplexity AI, emphasizes, “This technology is just going to be available in everything and everywhere. It’ll just be a way to enable some core product experience. It’ll make some new software that’s amazing, and it’ll accelerate some old software.” This transformation operates on three integrated levels: Understanding user intent, even when expressed imperfectly Managing complex processing invisibly Adapting interfaces dynamically to user needs We see this already in action through tools like Perplexity AI, which has transformed information search into natural conversation, and NotebookLM, which actively discovers connections across documents that users might miss. These aren’t merely faster versions of existing tools — they represent entirely new paradigms for human-computer interaction. Building trust through smart design The key to successful AI integration lies in balancing transparency and user control . Each of our case studies demonstrates this principle in action; Perplexity AI shows its sources in real-time, NotebookLM visualizes its thought process when connecting ideas across documents, and Genway AI maintains transparency in its research data analysis. These implementations showcase different levels of AI involvement while ensuring users maintain meaningful control. This balanced approach manifests through specific design choices: Verification mechanisms : Users can verify sources directly in Perplexity AI. Selective adoption : NotebookLM allows users to accept or reject AI-suggested connections. Expert oversight : Genway AI enables researchers to validate AI-generated insights. By making AI’s role visible and keeping users in control, these systems create the foundation of trust essential for effective human-AI collaboration while maximizing the benefits of AI capabilities. Conclusion: The path forward with AI in UX design The transformation we’re witnessing in UX design isn’t just another technological shift — it’s a fundamental reimagining of how humans interact with digital products. We’re moving from an era of digital tools to one of intelligent systems, where AI acts as a horizontal enabling layer that enhances and transforms every aspect of the user experience. Looking ahead, UX teams need to focus on three key priorities: Designing with intention : Move beyond surface-level AI integration by identifying where AI can most meaningfully transform your user’s experience, shifting from standalone tools to interconnected intelligent systems. Building trust through transparency : Apply frameworks and design patterns that clearly communicate AI’s role and limitations to users, making the intelligence layer visible and understandable. Preserve human autonomy : Ensure users maintain meaningful control and understanding of AI-driven features, creating a symbiotic relationship between human insight and AI capabilities. The future of UX belongs to designers who can strike the right balance between AI’s capabilities and human needs. Success will come not from maximizing AI usage, but from thoughtfully integrating it as an intelligence layer that augments human capabilities while preserving what makes us uniquely human: our creativity, empathy, and ability to make nuanced judgments based on context and values. The question isn’t whether AI will transform your product’s user experience — it’s how you’ll lead that transformation. Every UX team now faces the opportunity to evolve their products from collections of tools into intelligent systems that adapt, learn, and grow with their users. Are you ready to reimagine what’s possible?","The AI layer: transforming UX design from tools to intelligence A practical framework for implementing AI as a foundational layer in digital products Adrian Levy · Follow Published in UX Collective · 8 min read · 4 days ago -- 2 Listen Share Figure 1: Conceptual representation of AI as a horizontal enabling layer, illustrating its transformative nature similar to electricity in the 20th century. The bright horizontal line represents the AI layer that powers and connects multiple applications and systems, represented by interconnected nodes. Image generated via Krea.ai. Introduction “AI is a horizontal enabling layer — it can be used to improve everything. It will be in everything,” Jeff Bezos declared, comparing AI to the transformative power of electricity. “These kinds of horizontal layers like electricity, compute, and now artificial intelligence, they go everywhere. I guarantee you there is not a single application that you can think of that is not going to be made better by AI.” Just as electricity revolutionized every industry by becoming a foundational utility, AI is fundamentally reshaping our digital landscape in ways that go far beyond simple automation. At Amazon alone, teams are working on “ literally a thousand applications internally ,” demonstrating AI’s potential for widespread integration. As UX professionals, we’re at the forefront of this transformation , tasked with creating interfaces that make these powerful capabilities accessible and meaningful to users. Through an analysis of pioneering applications, we can establish a framework for designing AI-driven products . These examples serve as both a conceptual model and a practical guide for structuring products where AI acts as a foundational layer. Beyond showcasing technological advancement, this analysis provides a systematic approach to identifying AI opportunities within your own products. Case 1: Perplexity AI From manual information assembly to intelligent discovery The Challenge: Breaking free from traditional search “AI is the first truly new interaction-design paradigm in 60 years,” observes Jakob Nielsen , and nowhere is this more evident than in information search. Traditional search engines, despite their sophistication, still required users to master a complex dance: crafting precise queries, scanning multiple results, clicking through various pages, and mentally synthesizing information. This process, while familiar, placed a significant cognitive burden on users. Figure 2: Traditional search interface. A Google search interface showing the traditional approach where users must manually scan, filter, and synthesize information from multiple sources. Note the cognitive load required to process and connect information. The transformation: Reimagining search with AI Perplexity AI embodies this transformation by fundamentally reimagining how humans interact with information. Instead of users adapting to the system’s requirements, the AI adapts to user intent . Figure 3: Perplexity AI Interface. Perplexity AI’s interface demonstrates the shift from tool-based search to intelligent discovery. Note how it automatically synthesizes information and presents structured comparisons. How it works: The intelligence layer The system processes queries through interconnected layers that understand context, verify information in real-time, and present synthesized answers that feel natural and conversational. Figure 4: System architecture diagram. Perplexity AI’s information discovery system, showing how natural language queries are processed through multiple AI layers for comprehensive, verified answers. Source: Author. Key Components: Query understanding layer : Processes natural language input Information synthesis layer : Connects and verifies multiple sources Response generation layer : Creates coherent, contextual answers Interaction management layer : Maintains conversation flow Impact on user experience The result isn’t just a faster search engine — it’s a new paradigm for knowledge discovery that feels more like consulting a knowledgeable colleague than operating a digital tool. Users can: Ask questions naturally without worrying about keywords Receive comprehensive, synthesized answers Follow up with contextual questions Verify sources and facts in real-time This transformation shows how AI can fundamentally change core user interactions, moving beyond mere automation to create truly intelligent systems that adapt to human needs rather than requiring humans to adapt to them. Case 2: NotebookLM From document management to knowledge orchestration The challenge: Beyond digital paper Traditional research and note-taking tools merely digitized the paper experience while maintaining its fundamental limitations. Knowledge workers and researchers faced persistent challenges: Manual information organization Limited connections between documents Difficulty maintaining coherent management systems Cognitive overload when processing multiple sources Figure 5: Traditional research process. Traditional document management approach showing multiple windows, manual note-taking, and disconnected information sources. Researchers must manually create connections and maintain organization systems. Image generated via Krea.ai. The transformation: AI as a knowledge partner NotebookLM transforms this experience by implementing AI as an intelligent collaboration layer. Instead of being a passive repository, the system: Actively participates in the knowledge work process Automatically maps relationships between documents Suggests connections you might have missed Adapts its organization to your thinking patterns Figure 6: NotebookLM Interface. NotebookLM’s dynamic workspace showing AI-powered suggestions, automatic relationship mapping, and contextual document organization. How it works: The intelligence layer NotebookLM transforms document management through a three-layer intelligence system. The User interaction layer handles direct user engagement through document uploads, natural queries, and note-taking capabilities. The Knowledge processing layer — the system’s core — analyzes documents, maps connections, and synthesizes information using advanced AI algorithms. Finally, the Intelligent output layer presents this processed information as connected insights, related concepts, and research suggestions, creating a dynamic system that actively enhances the research and learning process. Figure 7: Knowledge orchestration system. Visualization of NotebookLM’s approach to transforming document management into an intelligent knowledge system . Source: Author. Impact on research and knowledge work This transformation creates a dynamic workspace that: Feels less like a digital filing cabinet and more like a thinking partner Enhances natural workflow while maintaining flexibility Reduces the cognitive load of organization Facilitates serendipitous discoveries NotebookLM represents a fundamental shift in how we interact with information: from passive document management to active collaboration with a system that understands and amplifies our thought processes. This transformation demonstrates how AI can serve as more than just a tool — it becomes an intelligent partner in the knowledge work process, while maintaining the user’s autonomy and enhancing their natural cognitive workflows. Case 3: Genway AI From individual interviews to scalable intelligence The challenge: Breaking traditional research boundaries The traditional UX research process has remained largely unchanged for decades, requiring intensive manual effort at every stage. Researchers face significant limitations: Time-consuming participant recruitment Limited interview capacity Manual analysis of responses Labor-intensive synthesis of findings Trade-offs between depth and breadth of research Figure 8: Traditional UX Research Session. A traditional user research session showing researchers conducting a one-on-one interview. This manual approach, while thorough, illustrates the core limitation of traditional UX research: the inability to scale without compromising depth of insights. Image generated via Krea.ai. The transformation: AI-enabled research at scale Genway AI revolutionizes this paradigm by implementing AI as a horizontal enabling layer across the entire research workflow. The system: Conducts multiple human-like interviews simultaneously Processes multiple data streams in real-time Analyzes responses across voice, text, and video Synthesizes insights automatically while maintaining research integrity Figure 9: Genway AI Interface. Genway AI’s interface showing parallel interview processing, real-time analysis, and automated insight generation while maintaining human oversight. How it works: The intelligence layer Genway AI operates through a comprehensive three-layer system that transforms traditional user research into a scalable intelligence operation. The Data collection layer captures multiple streams of user input through voice, text, and video channels simultaneously. The AI processing core analyzes this data in real-time, recognizing patterns, performing sentiment analysis, and generating insights. The Research insights layer then delivers both quantitative analysis and qualitative insights, automatically detecting trends and providing strategic recommendations. Figure 10: Research intelligence framework. Visualization of Genway AI’s approach to scaling user research, showing how multiple data streams are processed simultaneously through AI analysis layers. Source: Author. Impact on UX research This transformation fundamentally reshapes UX research capabilities through: Scale with depth : Enables simultaneous processing of hundreds of interviews while maintaining the nuanced understanding essential to quality research. The system can process multiple data streams without sacrificing the depth of analysis traditionally associated with one-on-one interviews. Real-time intelligence : Combines immediate insight generation during research sessions with sophisticated pattern recognition across large datasets, enabling researchers to adapt and refine their approach during the research process. Augmented expertise : Creates a balanced synergy between automation and human expertise, where AI handles data processing and pattern identification while researchers focus on strategic interpretation and decision-making. Genway AI demonstrates how artificial intelligence can transform UX research from a linear, resource-constrained process into a dynamic, scalable system that amplifies rather than replaces human research capabilities. This transformation maintains the nuanced understanding essential to user research while dramatically expanding its scope and efficiency. The impact on UX design and future implications Our analysis of these groundbreaking applications reveals fundamental patterns reshaping the future of UX design. Just as electricity transformed every industry it touched, AI is creating new paradigms for how we think about and design digital experiences. As Jakob Nielsen observes , “the best design for AI will retain some of the old graphical user interface elements, resulting in a hybrid UI, mostly based on user intent. But iterations including tweaks and revisions, specified through GUI commands.” This insight reveals a crucial principle for designing AI-powered products: the need to balance revolutionary capabilities with familiar interaction patterns . How AI is reshaping digital experiences AI isn’t simply adding features to existing tools — it’s fundamentally transforming how we design digital experiences. As Henry Modisett , head of design at Perplexity AI, emphasizes, “This technology is just going to be available in everything and everywhere. It’ll just be a way to enable some core product experience. It’ll make some new software that’s amazing, and it’ll accelerate some old software.” This transformation operates on three integrated levels: Understanding user intent, even when expressed imperfectly Managing complex processing invisibly Adapting interfaces dynamically to user needs We see this already in action through tools like Perplexity AI, which has transformed information search into natural conversation, and NotebookLM, which actively discovers connections across documents that users might miss. These aren’t merely faster versions of existing tools — they represent entirely new paradigms for human-computer interaction. Building trust through smart design The key to successful AI integration lies in balancing transparency and user control . Each of our case studies demonstrates this principle in action; Perplexity AI shows its sources in real-time, NotebookLM visualizes its thought process when connecting ideas across documents, and Genway AI maintains transparency in its research data analysis. These implementations showcase different levels of AI involvement while ensuring users maintain meaningful control. This balanced approach manifests through specific design choices: Verification mechanisms : Users can verify sources directly in Perplexity AI. Selective adoption : NotebookLM allows users to accept or reject AI-suggested connections. Expert oversight : Genway AI enables researchers to validate AI-generated insights. By making AI’s role visible and keeping users in control, these systems create the foundation of trust essential for effective human-AI collaboration while maximizing the benefits of AI capabilities. Conclusion: The path forward with AI in UX design The transformation we’re witnessing in UX design isn’t just another technological shift — it’s a fundamental reimagining of how humans interact with digital products. We’re moving from an era of digital tools to one of intelligent systems, where AI acts as a horizontal enabling layer that enhances and transforms every aspect of the user experience. Looking ahead, UX teams need to focus on three key priorities: Designing with intention : Move beyond surface-level AI integration by identifying where AI can most meaningfully transform your user’s experience, shifting from standalone tools to interconnected intelligent systems. Building trust through transparency : Apply frameworks and design patterns that clearly communicate AI’s role and limitations to users, making the intelligence layer visible and understandable. Preserve human autonomy : Ensure users maintain meaningful control and understanding of AI-driven features, creating a symbiotic relationship between human insight and AI capabilities. The future of UX belongs to designers who can strike the right balance between AI’s capabilities and human needs. Success will come not from maximizing AI usage, but from thoughtfully integrating it as an intelligence layer that augments human capabilities while preserving what makes us uniquely human: our creativity, empathy, and ability to make nuanced judgments based on context and values. The question isn’t whether AI will transform your product’s user experience — it’s how you’ll lead that transformation. Every UX team now faces the opportunity to evolve their products from collections of tools into intelligent systems that adapt, learn, and grow with their users. Are you ready to reimagine what’s possible?",The AI layer: transforming UX design from tools to intelligence,"

Key Points:
",UI/UX,"<h4>A practical framework for implementing AI as a foundational layer in digital products</h4><figure><img alt=""Figure 1: Conceptual representation of AI as a horizontal enabling layer, illustrating its transformative nature similar to electricity in the 20th century. The bright horizontal line represents the AI layer that powers and connects multiple applications and systems, represented by interconnected nodes. Image generated using Krea.ai, 2025."" src=""https://cdn-images-1.medium.com/max/1024/1*WghJREvEVDI0RvpHq_1PsA.png"" /><figcaption>Figure 1: Conceptual representation of AI as a horizontal enabling layer, illustrating its transformative nature similar to electricity in the 20th century. The bright horizontal line represents the AI layer that powers and connects multiple applications and systems, represented by interconnected nodes. Image generated via Krea.ai.</figcaption></figure><h3>Introduction</h3><blockquote>“AI is a horizontal enabling layer — it can be used to improve everything. It will be in everything,”</blockquote><blockquote><a href=""https://www.youtube.com/watch?v=s71nJQqzYRQ&amp;t=3144s""><em>Jeff Bezos</em></a><em> declared, comparing AI to the transformative power of electricity.</em></blockquote><blockquote>“These kinds of horizontal layers like electricity, compute, and now artificial intelligence, they go everywhere. I guarantee you there is not a single application that you can think of that is not going to be made better by AI.”</blockquote><p>Just as electricity revolutionized every industry by becoming a foundational utility, AI is fundamentally reshaping our digital landscape in ways that go far beyond simple automation. At Amazon alone, teams are working on “<a href=""https://www.youtube.com/watch?v=s71nJQqzYRQ&amp;t=3093s""><em>literally a thousand applications internally</em></a>,” demonstrating AI’s potential for widespread integration.</p><p>As <a href=""https://uxmag.com/articles/the-future-of-ux-design-how-ai-and-machine-learning-are-changing-the-way-we-design"">UX professionals, we’re at the forefront of this transformation</a>, tasked with creating interfaces that make these powerful capabilities accessible and meaningful to users. Through an analysis of pioneering applications, we can establish <a href=""https://www.uxforai.com/"">a framework for designing AI-driven products</a>. These examples serve as both a conceptual model and a practical guide for structuring products where AI acts as a foundational layer. Beyond showcasing technological advancement, this analysis provides a systematic approach to identifying AI opportunities within your own products.</p><h3>Case 1: Perplexity AI</h3><p><strong>From manual information assembly to intelligent discovery</strong></p><h4>The Challenge: Breaking free from traditional search</h4><blockquote>“AI is the first truly new interaction-design paradigm in 60 years,” <em>observes </em><a href=""https://www.youtube.com/watch?v=n8On7y_0Ans&amp;t=90s""><em>Jakob Nielsen</em></a><em>, and nowhere is this more evident than in information search.</em></blockquote><p>Traditional search engines, despite their sophistication, still required users to master a complex dance: crafting precise queries, scanning multiple results, clicking through various pages, and mentally synthesizing information. This process, while familiar, placed a significant cognitive burden on users.</p><figure><img alt=""Figure 2: Traditional search interface. A Google search interface showing the traditional approach where users must manually scan, filter, and synthesize information from multiple sources. Note the cognitive load required to process and connect information."" src=""https://cdn-images-1.medium.com/max/1024/1*x_2pjHSnCqdEGjLHE7_xcA.png"" /><figcaption>Figure 2: Traditional search interface. A Google search interface showing the traditional approach where users must manually scan, filter, and synthesize information from multiple sources. Note the cognitive load required to process and connect information.</figcaption></figure><h4>The transformation: Reimagining search with AI</h4><p><a href=""https://www.perplexity.ai/"">Perplexity AI</a> embodies this transformation by fundamentally reimagining how humans interact with information. Instead of users adapting to the system’s requirements, the AI adapts to <a href=""https://www.nngroup.com/articles/ai-paradigm/"">user intent</a>.</p><figure><img alt=""Figure 3: Perplexity AI Interface. Perplexity AI’s interface demonstrates the shift from tool-based search to intelligent discovery. Note how it automatically synthesizes information and presents structured comparisons."" src=""https://cdn-images-1.medium.com/max/1024/1*ZzKD-JCrf3uZN0hUpmXznQ.png"" /><figcaption>Figure 3: Perplexity AI Interface. <em>Perplexity AI’s interface demonstrates the shift from tool-based search to intelligent discovery. Note how it automatically synthesizes information and presents structured comparisons.</em></figcaption></figure><h4>How it works: The intelligence layer</h4><p>The system processes queries through interconnected layers that understand context, verify information in real-time, and present synthesized answers that feel natural and conversational.</p><figure><img alt=""Figure 4: System Architecture Diagram. Perplexity AI’s information discovery system, showing how natural language queries are processed through multiple AI layers for comprehensive, verified answers. Source: Author."" src=""https://cdn-images-1.medium.com/max/1024/0*O5bAryHZWeFSoKmT"" /><figcaption>Figure 4: System architecture diagram. <em>Perplexity AI’s information discovery system, showing how natural language queries are processed through multiple AI layers for comprehensive, verified answers.</em> Source: Author.</figcaption></figure><p><strong>Key Components:</strong></p><ul><li><strong>Query understanding layer</strong>: Processes natural language input</li><li><strong>Information synthesis layer</strong>: Connects and verifies multiple sources</li><li><strong>Response generation layer</strong>: Creates coherent, contextual answers</li><li><strong>Interaction management layer</strong>: Maintains conversation flow</li></ul><h4>Impact on user experience</h4><p>The result isn’t just a faster search engine — <a href=""https://aitoday.com/artificial-intelligence/what-is-perplexity-ai-how-it-works-and-how-to-use-it/"">it’s a new paradigm for knowledge discovery</a> that feels more like consulting a knowledgeable colleague than operating a digital tool. Users can:</p><ul><li>Ask questions naturally without worrying about keywords</li><li>Receive comprehensive, synthesized answers</li><li>Follow up with contextual questions</li><li>Verify sources and facts in real-time</li></ul><p>This transformation shows how AI can fundamentally change core user interactions, moving beyond mere automation to create truly intelligent systems that adapt to human needs rather than requiring humans to adapt to them.</p><h3><strong>Case 2: NotebookLM</strong></h3><p><strong>From document management to knowledge orchestration</strong></p><h4>The challenge: Beyond digital paper</h4><p>Traditional research and note-taking tools merely digitized the paper experience while maintaining its fundamental limitations. Knowledge workers and researchers faced persistent challenges:</p><ul><li>Manual information organization</li><li>Limited connections between documents</li><li>Difficulty maintaining coherent management systems</li><li>Cognitive overload when processing multiple sources</li></ul><figure><img alt=""Figure 5: Traditional Research Process. Traditional document management approach showing multiple windows, manual note-taking, and disconnected information sources. Researchers must manually create connections and maintain organization systems. Image generated via Krea.ai."" src=""https://cdn-images-1.medium.com/max/1024/0*wlJdU1uxCh_bdOHK.png"" /><figcaption>Figure 5: Traditional research process. <em>Traditional document management approach showing multiple windows, manual note-taking, and disconnected information sources. Researchers must manually create connections and maintain organization systems. </em>Image generated via Krea.ai.</figcaption></figure><h4>The transformation: AI as a knowledge partner</h4><p><a href=""http://notebooklm.google/"">NotebookLM</a> transforms this experience by implementing AI as an <a href=""https://www.youtube.com/watch?v=ryYV8Ygnwug"">intelligent collaboration</a> layer. Instead of being a passive repository, the system:</p><ul><li>Actively participates in the knowledge work process</li><li>Automatically maps relationships between documents</li><li>Suggests connections you might have missed</li><li>Adapts its organization to your thinking patterns</li></ul><figure><img alt=""Figure 6: NotebookLM Interface. NotebookLM’s dynamic workspace showing AI-powered suggestions, automatic relationship mapping, and contextual document organization."" src=""https://cdn-images-1.medium.com/max/1024/1*XKjVcdUDcIqf64EV8Ww9lQ.png"" /><figcaption>Figure 6: NotebookLM Interface. <em>NotebookLM’s dynamic workspace showing AI-powered suggestions, automatic relationship mapping, and contextual document organization.</em></figcaption></figure><h4>How it works: The intelligence layer</h4><p><a href=""http://notebooklm.google/"">NotebookLM</a> transforms document management through a three-layer intelligence system. The <strong>User interaction layer</strong> handles direct user engagement through document uploads, natural queries, and note-taking capabilities. The <strong>Knowledge processing layer</strong> — the system’s core — analyzes documents, maps connections, and synthesizes information using advanced AI algorithms. Finally, the <strong>Intelligent output layer</strong> presents this processed information as connected insights, related concepts, and research suggestions, creating a dynamic system that actively enhances the research and learning process.</p><figure><img alt=""Figure 7: Knowledge Orchestration System. Visualization of NotebookLM’s approach to transforming document management into an intelligent knowledge system. Source: Author."" src=""https://cdn-images-1.medium.com/max/1024/0*VPNubfACDqIW2bKW"" /><figcaption>Figure 7: Knowledge orchestration system. <em>Visualization of NotebookLM’s approach to transforming document management into an intelligent knowledge system</em>. Source: Author.</figcaption></figure><h4>Impact on research and knowledge work</h4><p>This transformation creates a <a href=""https://learnprompting.org/blog/notebooklm-guide"">dynamic workspace</a> that:</p><ul><li>Feels less like a digital filing cabinet and more like a thinking partner</li><li>Enhances natural workflow while maintaining flexibility</li><li>Reduces the cognitive load of organization</li><li>Facilitates serendipitous discoveries</li></ul><p>NotebookLM represents a fundamental shift in how we interact with information: from passive document management to active collaboration with a system that understands and amplifies our thought processes. This transformation demonstrates how AI can serve as more than just a tool — it becomes an intelligent partner in the knowledge work process, while maintaining the user’s autonomy and enhancing their natural cognitive workflows.</p><h3>Case 3: Genway AI</h3><p><strong>From individual interviews to scalable intelligence</strong></p><h4>The challenge: Breaking traditional research boundaries</h4><p>The traditional UX research process has remained largely unchanged for decades, requiring intensive manual effort at every stage. Researchers face significant limitations:</p><ul><li>Time-consuming participant recruitment</li><li>Limited interview capacity</li><li>Manual analysis of responses</li><li>Labor-intensive synthesis of findings</li><li>Trade-offs between depth and breadth of research</li></ul><figure><img alt=""Figure 8: Traditional UX Research Session. A traditional user research session showing researchers conducting a one-on-one interview. This manual approach, while thorough, illustrates the core limitation of traditional UX research: the inability to scale without compromising depth of insights. Image generated via Krea.ai."" src=""https://cdn-images-1.medium.com/max/1024/0*OcWHjNzHhPTvSjOY.png"" /><figcaption>Figure 8: Traditional UX Research Session. <em>A traditional user research session showing researchers conducting a one-on-one interview. This manual approach, while thorough, illustrates the core limitation of traditional UX research: the inability to scale without compromising depth of insights. </em>Image generated via Krea.ai.</figcaption></figure><h4>The transformation: AI-enabled research at scale</h4><p>Genway AI revolutionizes this paradigm by implementing AI as a horizontal enabling layer across the entire research workflow. The system:</p><ul><li><a href=""https://www.brilliantexperience.com/blog/how-ai-for-research-lets-product-teams-focus-on-strategy-interview-with-natan-voitenkov-genway"">Conducts multiple human-like interviews simultaneously</a></li><li>Processes multiple data streams in real-time</li><li>Analyzes responses across voice, text, and video</li><li>Synthesizes insights automatically while maintaining research integrity</li></ul><figure><img alt=""Figure 9: Genway AI Interface. Genway AI’s interface showing parallel interview processing, real-time analysis, and automated insight generation while maintaining human oversight."" src=""https://cdn-images-1.medium.com/max/841/1*t4kV1_KrHbSTGuVPIMC7Ow.png"" /><figcaption>Figure 9: Genway AI Interface. <em>Genway AI’s interface showing parallel interview processing, real-time analysis, and automated insight generation while maintaining human oversight.</em></figcaption></figure><h4>How it works: The intelligence layer</h4><p><a href=""https://www.genway.ai/"">Genway AI</a> operates through a comprehensive three-layer system that transforms traditional user research into a scalable intelligence operation. The <strong>Data collection layer</strong> captures multiple streams of user input through voice, text, and video channels simultaneously. The <strong>AI processing core</strong> analyzes this data in real-time, recognizing patterns, performing sentiment analysis, and generating insights. The <strong>Research insights layer</strong> then delivers both quantitative analysis and qualitative insights, automatically detecting trends and providing strategic recommendations.</p><figure><img alt=""Figure 10: Research intelligence framework. Visualization of Genway AI’s approach to scaling user research, showing how multiple data streams are processed simultaneously through AI analysis layers. Source: Author."" src=""https://cdn-images-1.medium.com/max/1024/0*iR2wCopnjekxWhg4"" /><figcaption>Figure 10: Research intelligence framework. <em>Visualization of Genway AI’s approach to scaling user research, showing how multiple data streams are processed simultaneously through AI analysis layers.</em> Source: Author.</figcaption></figure><h4>Impact on UX research</h4><p>This transformation fundamentally reshapes UX research capabilities through:</p><p><strong>Scale with depth</strong>: Enables simultaneous processing of hundreds of interviews while maintaining the nuanced understanding essential to quality research. The system can process multiple data streams without sacrificing the depth of analysis traditionally associated with one-on-one interviews.</p><p><strong>Real-time intelligence</strong>: Combines immediate insight generation during research sessions with sophisticated pattern recognition across large datasets, enabling researchers to adapt and refine their approach during the research process.</p><p><strong>Augmented expertise</strong>: Creates a balanced synergy between automation and human expertise, where AI handles data processing and pattern identification while researchers focus on strategic interpretation and decision-making.</p><p><a href=""https://www.genway.ai/"">Genway AI</a> demonstrates how artificial intelligence can transform UX research from a linear, resource-constrained process into a dynamic, scalable system that amplifies rather than replaces human research capabilities. This transformation maintains the nuanced understanding essential to user research while dramatically expanding its scope and efficiency.</p><h3>The impact on UX design and future implications</h3><p>Our analysis of these groundbreaking applications reveals fundamental patterns reshaping the future of UX design. Just as electricity transformed every industry it touched, AI is creating new paradigms for how we think about and design digital experiences.</p><blockquote><em>As </em><a href=""https://www.youtube.com/watch?v=n8On7y_0Ans&amp;t=90s""><em>Jakob Nielsen</em></a><em> observes</em>, “the best design for AI will retain some of the old graphical user interface elements, resulting in a hybrid UI, mostly based on <a href=""https://www.nngroup.com/articles/ai-paradigm/"">user intent.</a> But iterations including tweaks and revisions, specified through GUI commands.” <em>This insight reveals a crucial principle for designing AI-powered products: the need to </em><a href=""https://www.interaction-design.org/literature/article/how-to-design-with-ai-insights-from-the-ixdf-course""><em>balance revolutionary capabilities with familiar interaction patterns</em></a><em>.</em></blockquote><h4>How AI is reshaping digital experiences</h4><blockquote><em>AI isn’t simply adding features to existing tools — it’s fundamentally transforming how we design digital experiences. As </em><a href=""https://www.nngroup.com/articles/perplexity-henry-modisett/""><em>Henry Modisett</em></a><em>, head of design at Perplexity AI, emphasizes, </em>“This technology is just going to be available in everything and everywhere. It’ll just be a way to enable some core product experience. It’ll make some new software that’s amazing, and it’ll accelerate some old software.”</blockquote><p>This transformation operates on three integrated levels:</p><ol><li>Understanding user intent, even when expressed imperfectly</li><li>Managing complex processing invisibly</li><li>Adapting interfaces dynamically to user needs</li></ol><p>We see this already in action through tools like Perplexity AI, which has transformed information search into natural conversation, and NotebookLM, which actively discovers connections across documents that users might miss. These aren’t merely faster versions of existing tools — they represent entirely new paradigms for human-computer interaction.</p><h4>Building trust through smart design</h4><p>The key to successful AI integration lies in balancing <a href=""https://uxdesign.cc/ai-transparency-framework-cf89fa61dc1b"">transparency</a> and <a href=""https://uxofai.com/"">user control</a>. Each of our case studies demonstrates this principle in action; Perplexity AI shows its sources in real-time, NotebookLM visualizes its thought process when connecting ideas across documents, and Genway AI maintains transparency in its research data analysis. These implementations showcase different <a href=""https://uxdesign.cc/ai-transparency-framework-cf89fa61dc1b"">levels of AI involvement</a> while ensuring users maintain meaningful control.</p><p>This balanced approach manifests through specific design choices:</p><ol><li><strong>Verification mechanisms</strong>: Users can verify sources directly in Perplexity AI.</li><li><strong>Selective adoption</strong>: NotebookLM allows users to accept or reject AI-suggested connections.</li><li><strong>Expert oversight</strong>: Genway AI enables researchers to validate AI-generated insights.</li></ol><p>By making AI’s role visible and keeping users in control, these systems create the foundation of trust essential for effective human-AI collaboration while maximizing the benefits of AI capabilities.</p><h3>Conclusion: The path forward with AI in UX design</h3><p>The transformation we’re witnessing in UX design isn’t just another technological shift — it’s a fundamental reimagining of how humans interact with digital products. We’re moving from an era of digital tools to one of intelligent systems, where AI acts as a horizontal enabling layer that enhances and transforms every aspect of the user experience.</p><p>Looking ahead, UX teams need to focus on three key priorities:</p><ol><li><strong>Designing with </strong><a href=""https://uxdesign.cc/the-next-era-of-design-is-intent-driven-f789ee521482""><strong>intention</strong></a>: Move beyond surface-level AI integration by identifying where AI can most meaningfully transform your user’s experience, shifting from standalone tools to interconnected intelligent systems.</li><li><strong>Building trust through </strong><a href=""https://uxdesign.cc/ai-transparency-framework-cf89fa61dc1b""><strong>transparency</strong></a>: Apply frameworks and design patterns that clearly communicate AI’s role and limitations to users, making the intelligence layer visible and understandable.</li><li><strong>Preserve human autonomy</strong>: Ensure users maintain meaningful control and understanding of AI-driven features, creating a symbiotic relationship between human insight and AI capabilities.</li></ol><p>The future of UX belongs to designers who can strike the right balance between AI’s capabilities and human needs. Success will come not from maximizing AI usage, but from thoughtfully integrating it as an intelligence layer that augments human capabilities while preserving what makes us uniquely human: our creativity, empathy, and ability to make nuanced judgments based on context and values.</p><p>The question isn’t whether AI will transform your product’s user experience — it’s how you’ll lead that transformation. Every UX team now faces the opportunity to evolve their products from collections of tools into intelligent systems that adapt, learn, and grow with their users.</p><p><em>Are you ready to reimagine what’s possible?</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7d6de37483cd"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-ai-layer-transforming-ux-design-from-tools-to-intelligence-7d6de37483cd"">The AI layer: transforming UX design from tools to intelligence</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
Amazon Web Services unveils quantum chip it hopes will shave years off development time,https://www.theglobeandmail.com/business/technology/article-amazon-web-services-unveils-quantum-chip-it-hopes-will-shave-years-off/,GNews,2025-02-27T19:02:29Z,The Globe and Mail,https://www.theglobeandmail.com/resizer/v2/IYEZMXD5NBCI5GEH2OE3RF7LKY.JPG?auth=90139cd1a5362a306ea8dd328a4a27118e729c889fff63d296d6cb5ed70e62d0&width=1200&height=800&quality=80&smart=true,"Amazon Web Services on Thursday showed a quantum computing chip with new technology that it hopes will shave as much as five years off its effort to build a commercially useful quantum computer. The chip, named Ocelot, is a prototype that has only a tiny fraction of the computing power needed to create a useful machine. But like its tech rivals, AWS, which is Amazon.com’sAMZN-Qcloud computing unit, believes it has finally hit on a technology that can be scaled up into a working machine, though it has not yet set a date for when it will reach that point. The AWS announcement, which coincides with the publication of a peer-reviewed paper in the scientific journal Nature, comes as quantum computing is sweeping through the technology world, with Alphabet’s Google, Microsoft and startup PsiQuantum all announcing advances in recent months. Quantum computers hold the promise of carrying out computations that would take conventional computers millions of years and could help scientists develop new materials such as batteries and new drugs. But a fundamental building block of quantum computers called a qubit is fast but finicky and prone to errors. Scientists established in the 1990s that some of a quantum computer’s qubits could be dedicated to correcting those errors, and the years since then have been spent searching for ways to construct physical qubits so that enough “logical” qubits are left over to do useful computing work. The standard industry thinking has been that a chip will need about a million physical qubits to yield a useful number of logical qubits. But AWS said it had built a prototype chip that uses only nine physical qubits to yield one working logical qubit, thanks to the use of what is known as a “cat” qubit, so named for physicist Erwin Schrodinger’s famous thought experiment to illustrate principles of quantum mechanics in which an unlucky cat in a box is both dead and alive at the same time. Oskar Painter, AWS director of quantum hardware, said the AWS approach could one day yield useful computers with only 100,000 qubits rather than a million. “It should allow us to provide between five and 10 times lower numbers of physical qubits to implement the error correction in a fully scaled machine. So that’s the real benefit,” Painter told Reuters. Painter said that the current chip was constructed using standard techniques borrowed from the chip industry and a material called tantalum, but that AWS and partners hope to customize those techniques further. “That’s where I think there’s going to be a huge amount of innovation and that will be the thing that could really reel in timelines for development. If we make improvements at the materials and processing level, this will make the underlying technology just much simpler,” Painter said. Report an editorial error Report a technical issue Study and track financial data on any traded entity: click to open the full quote page. Data updated as of28/02/25 4:15pm EST. Authors and topics you follow will be added to your personal news feed inFollowing.","Amazon Web Services on Thursday showed a quantum computing chip with new technology that it hopes will shave as much as five years off its effort to build a commercially useful quantum computer. The chip, named Ocelot, is a prototype that has only a tiny fraction of the computing power needed to create a useful machine. But like its tech rivals, AWS, which is Amazon.com’sAMZN-Qcloud computing unit, believes it has finally hit on a technology that can be scaled up into a working machine, though it has not yet set a date for when it will reach that point. The AWS announcement, which coincides with the publication of a peer-reviewed paper in the scientific journal Nature, comes as quantum computing is sweeping through the technology world, with Alphabet’s Google, Microsoft and startup PsiQuantum all announcing advances in recent months. Quantum computers hold the promise of carrying out computations that would take conventional computers millions of years and could help scientists develop new materials such as batteries and new drugs. But a fundamental building block of quantum computers called a qubit is fast but finicky and prone to errors. Scientists established in the 1990s that some of a quantum computer’s qubits could be dedicated to correcting those errors, and the years since then have been spent searching for ways to construct physical qubits so that enough “logical” qubits are left over to do useful computing work. The standard industry thinking has been that a chip will need about a million physical qubits to yield a useful number of logical qubits. But AWS said it had built a prototype chip that uses only nine physical qubits to yield one working logical qubit, thanks to the use of what is known as a “cat” qubit, so named for physicist Erwin Schrodinger’s famous thought experiment to illustrate principles of quantum mechanics in which an unlucky cat in a box is both dead and alive at the same time. Oskar Painter, AWS director of quantum hardware, said the AWS approach could one day yield useful computers with only 100,000 qubits rather than a million. “It should allow us to provide between five and 10 times lower numbers of physical qubits to implement the error correction in a fully scaled machine. So that’s the real benefit,” Painter told Reuters. Painter said that the current chip was constructed using standard techniques borrowed from the chip industry and a material called tantalum, but that AWS and partners hope to customize those techniques further. “That’s where I think there’s going to be a huge amount of innovation and that will be the thing that could really reel in timelines for development. If we make improvements at the materials and processing level, this will make the underlying technology just much simpler,” Painter said. Report an editorial error Report a technical issue Study and track financial data on any traded entity: click to open the full quote page. Data updated as of28/02/25 4:15pm EST. Authors and topics you follow will be added to your personal news feed inFollowing.",Amazon Web Services unveils quantum chip it hopes will shave years off development time,"

Key Points:
",Web Development,"Amazon Web Services on Thursday showed a quantum computing chip with new technology that it hopes will shave as much as five years off its effort to build a commercially useful quantum computer.
The chip, named Ocelot, is a prototype that has only a ... [2549 chars]"
Sensitive details of Australian IVF patients posted to dark web after Genea data breach,https://www.theguardian.com/society/2025/feb/26/genea-data-breach-hack-ivf-patient-details-leaked-ntwnfb,The Guardian,2025-02-26T07:01:04Z,The Guardian,https://media.guim.co.uk/968661d4e64076779899e61ded419c17b4b27cdc/0_192_5760_3456/500.jpg,"Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images View image in fullscreen Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images IVF Sensitive details of Australian IVF patients posted to dark web after Genea data breach Genea CEO told patients information accessed includes contact details, Medicare card numbers, medical histories, test results and medications Follow our Australia news live blog for latest updates Get our breaking news email , free app or daily news podcast Donna Lu and Josh Taylor Wed 26 Feb 2025 07.01 GMT Last modified on Wed 26 Feb 2025 07.31 GMT Share Sensitive patient information has allegedly been leaked on the dark web after Genea, one of Australia’s leading IVF and fertility services providers, was hacked a fortnight ago. The attack was allegedly carried out by the Termite ransomware group, prompting Genea to obtain a court injunction on Wednesday that criminalises access to the breached patient data. Guardian Australia has seen screenshots posted online by cybersecurity experts who monitor the dark web that appear to show a sample of the breached data. In a statement , Genea said: “Our ongoing investigation has established that on the 26 of February, data taken from our systems appears to have been published externally by the threat actor.” “We understand that this development may be concerning for our patients for which we unreservedly apologise.” Sensitive information including contact details, Medicare card numbers, medical histories, test results and medications may have been compromised in the data breach, Genea said, and it was “working to understand precisely what data has been published”. The court order reveals the alleged attackers were in Genea’s network for over two weeks before being detected starting from 31 January, and on 14 February extracted 940.7GB of data from Genea’s systems. Sign up for Guardian Australia’s breaking news email The company initially advised patients of the suspected data breach on Friday 21 February, and did not reveal the extent of the attack until the following Monday. Patients have not been informed what, if any, of their own personal information has been taken. But in an email sent to customers, Genea’s chief executive, Tim Yeoh, revealed information in the patient management systems accessed included full names and dates of birth, emails, addresses, phone numbers, Medicare card numbers, private health insurance details, medical histories, diagnoses and treatments, medications and prescriptions, test results, notes from doctors and emergency contacts. Yeoh said at that stage there was no evidence that financial information such as credit card details or bank account numbers had been compromised, but the investigation was ongoing. Genea operates fertility clinics in all states and territories excluding the Northern Territory. It provides genetic testing, egg and sperm freezing, fertility testing and treatments including IVF. “We have obtained this injunction as part of our commitment to the protection of our patients, staff and partners’ information, and taking all reasonable steps in response to this incident to protect the impacted data and those most vulnerable,” Genea said in a statement on its website. “We are meeting with the National Office of Cyber Security, the Australian Cyber Security Centre and other government departments to discuss the incident with them.” In 2022, the latest year for which data is available, one in 17 babies born in Australia involved assisted reproductive technologies . There were 108,913 ART treatment cycles in total. Network technology company Broadcom said in a memo issued in November last year that Termite had targeted a wide range of countries and sectors, including in France, Canada, Germany, Oman and the US. The sectors included government agencies, education, disability support services, oil and gas, water treatment and automotive manufacturing. Broadcom said the group’s modus operandi is unknown, but the ransomware will encrypt target files and direct victims to a dark web site to communicate on how to pay ransoms. Explore more on these topics IVF Health (Australia news) Fertility problems Health (Society) news Share Reuse this content","Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images View image in fullscreen Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images IVF Sensitive details of Australian IVF patients posted to dark web after Genea data breach Genea CEO told patients information accessed includes contact details, Medicare card numbers, medical histories, test results and medications Follow our Australia news live blog for latest updates Get our breaking news email , free app or daily news podcast Donna Lu and Josh Taylor Wed 26 Feb 2025 07.01 GMT Last modified on Wed 26 Feb 2025 07.31 GMT Share Sensitive patient information has allegedly been leaked on the dark web after Genea, one of Australia’s leading IVF and fertility services providers, was hacked a fortnight ago. The attack was allegedly carried out by the Termite ransomware group, prompting Genea to obtain a court injunction on Wednesday that criminalises access to the breached patient data. Guardian Australia has seen screenshots posted online by cybersecurity experts who monitor the dark web that appear to show a sample of the breached data. In a statement , Genea said: “Our ongoing investigation has established that on the 26 of February, data taken from our systems appears to have been published externally by the threat actor.” “We understand that this development may be concerning for our patients for which we unreservedly apologise.” Sensitive information including contact details, Medicare card numbers, medical histories, test results and medications may have been compromised in the data breach, Genea said, and it was “working to understand precisely what data has been published”. The court order reveals the alleged attackers were in Genea’s network for over two weeks before being detected starting from 31 January, and on 14 February extracted 940.7GB of data from Genea’s systems. Sign up for Guardian Australia’s breaking news email The company initially advised patients of the suspected data breach on Friday 21 February, and did not reveal the extent of the attack until the following Monday. Patients have not been informed what, if any, of their own personal information has been taken. But in an email sent to customers, Genea’s chief executive, Tim Yeoh, revealed information in the patient management systems accessed included full names and dates of birth, emails, addresses, phone numbers, Medicare card numbers, private health insurance details, medical histories, diagnoses and treatments, medications and prescriptions, test results, notes from doctors and emergency contacts. Yeoh said at that stage there was no evidence that financial information such as credit card details or bank account numbers had been compromised, but the investigation was ongoing. Genea operates fertility clinics in all states and territories excluding the Northern Territory. It provides genetic testing, egg and sperm freezing, fertility testing and treatments including IVF. “We have obtained this injunction as part of our commitment to the protection of our patients, staff and partners’ information, and taking all reasonable steps in response to this incident to protect the impacted data and those most vulnerable,” Genea said in a statement on its website. “We are meeting with the National Office of Cyber Security, the Australian Cyber Security Centre and other government departments to discuss the incident with them.” In 2022, the latest year for which data is available, one in 17 babies born in Australia involved assisted reproductive technologies . There were 108,913 ART treatment cycles in total. Network technology company Broadcom said in a memo issued in November last year that Termite had targeted a wide range of countries and sectors, including in France, Canada, Germany, Oman and the US. The sectors included government agencies, education, disability support services, oil and gas, water treatment and automotive manufacturing. Broadcom said the group’s modus operandi is unknown, but the ransomware will encrypt target files and direct victims to a dark web site to communicate on how to pay ransoms. Explore more on these topics IVF Health (Australia news) Fertility problems Health (Society) news Share Reuse this content",Sensitive details of Australian IVF patients posted to dark web after Genea data breach,"

Key Points:
",Web Development,"Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images View image in fullscreen Patients have not been informed what, if any, of their own personal information has been taken in the Genea data breach. Photograph: Ольга Симонова/Getty Images IVF Sensitive details of Australian IVF patients posted to dark web after Genea data breach Genea CEO told patients information accessed includes contact details, Medicare card numbers, medical histories, test results and medications Follow our Australia news live blog for latest updates Get our breaking news email , free app or daily news podcast Donna Lu and Josh Taylor Wed 26 Feb 2025 07.01 GMT Last modified on Wed 26 Feb 2025 07.31 GMT Share Sensitive patient information has allegedly been leaked on the dark web after Genea, one of Australia’s leading IVF and fertility services providers, was hacked a fortnight ago. The attack was allegedly carried out by the Termite ransomware group, prompting Genea to obtain a court injunction on Wednesday that criminalises access to the breached patient data. Guardian Australia has seen screenshots posted online by cybersecurity experts who monitor the dark web that appear to show a sample of the breached data. In a statement , Genea said: “Our ongoing investigation has established that on the 26 of February, data taken from our systems appears to have been published externally by the threat actor.” “We understand that this development may be concerning for our patients for which we unreservedly apologise.” Sensitive information including contact details, Medicare card numbers, medical histories, test results and medications may have been compromised in the data breach, Genea said, and it was “working to understand precisely what data has been published”. The court order reveals the alleged attackers were in Genea’s network for over two weeks before being detected starting from 31 January, and on 14 February extracted 940.7GB of data from Genea’s systems. Sign up for Guardian Australia’s breaking news email The company initially advised patients of the suspected data breach on Friday 21 February, and did not reveal the extent of the attack until the following Monday. Patients have not been informed what, if any, of their own personal information has been taken. But in an email sent to customers, Genea’s chief executive, Tim Yeoh, revealed information in the patient management systems accessed included full names and dates of birth, emails, addresses, phone numbers, Medicare card numbers, private health insurance details, medical histories, diagnoses and treatments, medications and prescriptions, test results, notes from doctors and emergency contacts. Yeoh said at that stage there was no evidence that financial information such as credit card details or bank account numbers had been compromised, but the investigation was ongoing. Genea operates fertility clinics in all states and territories excluding the Northern Territory. It provides genetic testing, egg and sperm freezing, fertility testing and treatments including IVF. “We have obtained this injunction as part of our commitment to the protection of our patients, staff and partners’ information, and taking all reasonable steps in response to this incident to protect the impacted data and those most vulnerable,” Genea said in a statement on its website. “We are meeting with the National Office of Cyber Security, the Australian Cyber Security Centre and other government departments to discuss the incident with them.” In 2022, the latest year for which data is available, one in 17 babies born in Australia involved assisted reproductive technologies . There were 108,913 ART treatment cycles in total. Network technology company Broadcom said in a memo issued in November last year that Termite had targeted a wide range of countries and sectors, including in France, Canada, Germany, Oman and the US. The sectors included government agencies, education, disability support services, oil and gas, water treatment and automotive manufacturing. Broadcom said the group’s modus operandi is unknown, but the ransomware will encrypt target files and direct victims to a dark web site to communicate on how to pay ransoms. Explore more on these topics IVF Health (Australia news) Fertility problems Health (Society) news Share Reuse this content"
The Human Element: Using Research And Psychology To Elevate Data Storytelling,https://smashingmagazine.com/2025/02/human-element-using-research-psychology-elevate-data-storytelling/,Smashing Magazine,2025-02-26T10:00:00,Smashing Magazine,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Victor Yocco & Angelica Lo Duca Feb 26, 2025 0 comments The Human Element: Using Research And Psychology To Elevate Data Storytelling 23 min read UX , Design , Storytelling Share on Twitter , LinkedIn About The Authors Victor is a Philadelphia-based researcher, author, and speaker. His book Design for the Mind is available from Manning Publications. Victor frequently writes … More about
Victor & Angelica ↬ Email Newsletter Your (smashing) email Weekly tips on front-end & UX . Trusted by 200,000+ folks. Get a Free Trial How To Measure UX and Design Impact, 8h video + UX training Building Modern HTML Emails, with Rémi Parmentier Smart Interface Design Patterns, 10h video + UX training UX Design Leadership Masterclass, with Paul Boag JavaScript Form Builder — Create JSON-driven forms without coding. Try if for free! Effective data storytelling isn’t a black box. By integrating UX research & psychology, you can craft more impactful and persuasive narratives. Victor Yocco and Angelica Lo Duca outline a five-step framework that provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level. Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for user-centered improvements , and ultimately create more impactful and persuasive presentations . This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs. For instance, The New York Times’ Snow Fall data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while The Guardian’s The Counted (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions. Figure 1: The NYT Snow Fall displays data visualizations alongside a narrative of the events preceding and during a deadly avalanche. ( Large preview ) Figure 2: The Guardian The Counted tells a compelling data story of the facts behind people killed by the police in the US. ( Large preview ) The importance of data storytelling lies in its ability to: Simplify complexity It makes data understandable and actionable. Engage and persuade Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act. Bridge gaps Data storytelling connects the dots between information and human experience, making the data relevant and relatable. While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: Narrative Structures : Traditional storytelling models like the hero’s journey ( Vogler, 1992 ) or the Freytag pyramid (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged. Figure 3: Freytag’s Pyramid provides a narrative structure for storytellers. (Image source: scribophile.com) ( Large preview ) Data Visualization : Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics ( Cairo, 2016 ) transform raw data into digestible visuals, making complex information easier to understand and remember. Narrative Structures For Data Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories: The Three-Act Structure This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion ( Few, 2005 ). The Hero’s Journey (Data Edition) We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution. Example: Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement. Problems With Widely Used Data Storytelling Models Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the presentation of data over the experience of the audience, neglecting how different individuals perceive and process information. Figure 4: The traditional flow for creating a data-driven story. ( Large preview ) While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact. Cognitive Overload Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks. Emotional Disconnect Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values. Lack of Personalization Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. Over-Reliance on Visuals While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap, Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage. “ Improving On Traditional Models Traditional models can be improved by focusing more on the following two critical components: Audience understanding : A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading. Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion. Psychological principles : These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles. By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes. Persuasion In Data Storytelling All storytelling involves persuasion . Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience. Applying Psychological Principles To Data Storytelling Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive. Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. The Theory of Planned Behavior While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. The Theory of Planned Behavior (TPB) is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it. The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components: Attitude This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity. Subjective Norms These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. Perceived Behavioral Control This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior? As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991). Figure 5: The factors of the TPB interact with each other, collectively shaping an individual's behavioral intentions, which, in turn, are the most proximal determinant of human social behavior. ( Large preview ) UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article. How To Understand Your Audience And Apply Psychological Principles OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore how to integrate them into your storytelling process. Introducing The Audience Research Informed Data Storytelling Model (ARIDSM) At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories: Figure 6: The 5 steps of the Audience Research Informed Data Storytelling Model (ARIDSM). ( Large preview ) Step 1: Define Clear Objectives Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away? Why it matters : Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact. How to execute Step 1 : Start by asking yourself: What is the core message I want to convey? What do I want my audience to think, feel, or do after experiencing this story? How will I measure the success of my data story? Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.” Example: Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months. Step 2: Conduct UX Research To Understand Your Audience This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information. Why it matters : Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood. How to execute Step 2 : Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members. Example: If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story. Step 3: Analyze and Select Relevant Audience Data This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience. Why it matters : Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive. How to execute Step 3 : Clean and organize your data. Ensure accuracy and consistency before analysis. Identify key variables and metrics. This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items. Example: If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group. Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3] In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power. Why it matters : The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective. How to execute Step 4 : Here’s how to leverage the TPB in your data story: Influence Attitudes : Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. Shape Subjective Norms : Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise. Enhance Perceived Behavioral Control : Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome. Step 5: Craft A Balanced And Persuasive Narrative This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act. Why it matters : A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it. How to execute Step 5 : Structure your story strategically : Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you’ve cultivated throughout the narrative. Example: In a data story about promoting exercise, you could: Determine what stories might be available using the data you have collected or obtained. In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners. Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes). Integrate TPB elements : Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control). Use infographics to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling. Call to action : Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities. Evaluating The Method Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion. We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change. We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate. Step 1 : We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the hottest month ever recorded . Step 2 : We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research. Step 3 : We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a factor analysis to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change. Step 4 [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do. Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are no standardized questions (PDF) validated to measure the concepts for each individual topic. Item Measures Scale How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change? Attitude 1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial” How much do you think the people you care about (family, friends, community) expect you to take action against climate change? Subjective Norms 1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action” How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact? Perceived Behavioral Control 1 to 5 with 1 being “not at all confident” and 5 being “extremely confident” Table 1: Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component. Step 5 : We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created. Figure 7: Control data story. For the control story, we displayed the data around daily surface air temperature with some additional information explaining the chart. We did not attempt to influence behavior or tap into psychology to suggest there was urgency or persuade the participant to want to learn more. The color used in the chart comes from the initial chart generated in the source. We acknowledge that color is likely to present some psychological influence, given the use of red to represent extreme heat and cooler colors like blue to represent cooler time periods. ( Large preview ) Figure 8: Group 1 data story. Our measurements suggested that the participants in Group 1 had a higher level of awareness of climate change and the related negative impacts of more extreme temperatures. Therefore, we didn’t call out the potential negatives of climate change and instead focused on a more positive message of how we might make a positive impact. Group one had higher levels of subjective norms, suggesting that language promoting how others engage in certain behaviors might align with what they believe to be true. We focused on the community aspect of the message, encouraging them to act. ( Large preview ) Step 6 : We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group. Step 7 : We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message. Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic. For example, you might feel Maslow’s hierarchy of needs is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs. Elevate Your Data Storytelling Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as audience understanding and the application of psychological principles . By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives. The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level . This approach ensures that data is not merely presented but is transformed into a meaningful experience that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform. Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience. Please feel free to use our method and provide any feedback on your experience to the author. (yk) Explore more on UX Design Storytelling Smashing Newsletter Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. Front-End & UX Workshops, Online With practical takeaways, live sessions, video recordings and a friendly Q&A. TypeScript in 50 Lessons Everything TypeScript, with code walkthroughs and examples. And other printed books.","Victor Yocco & Angelica Lo Duca Feb 26, 2025 0 comments The Human Element: Using Research And Psychology To Elevate Data Storytelling 23 min read UX , Design , Storytelling Share on Twitter , LinkedIn About The Authors Victor is a Philadelphia-based researcher, author, and speaker. His book Design for the Mind is available from Manning Publications. Victor frequently writes … More about
Victor & Angelica ↬ Email Newsletter Your (smashing) email Weekly tips on front-end & UX . Trusted by 200,000+ folks. Get a Free Trial How To Measure UX and Design Impact, 8h video + UX training Building Modern HTML Emails, with Rémi Parmentier Smart Interface Design Patterns, 10h video + UX training UX Design Leadership Masterclass, with Paul Boag JavaScript Form Builder — Create JSON-driven forms without coding. Try if for free! Effective data storytelling isn’t a black box. By integrating UX research & psychology, you can craft more impactful and persuasive narratives. Victor Yocco and Angelica Lo Duca outline a five-step framework that provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level. Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for user-centered improvements , and ultimately create more impactful and persuasive presentations . This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs. For instance, The New York Times’ Snow Fall data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while The Guardian’s The Counted (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions. Figure 1: The NYT Snow Fall displays data visualizations alongside a narrative of the events preceding and during a deadly avalanche. ( Large preview ) Figure 2: The Guardian The Counted tells a compelling data story of the facts behind people killed by the police in the US. ( Large preview ) The importance of data storytelling lies in its ability to: Simplify complexity It makes data understandable and actionable. Engage and persuade Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act. Bridge gaps Data storytelling connects the dots between information and human experience, making the data relevant and relatable. While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: Narrative Structures : Traditional storytelling models like the hero’s journey ( Vogler, 1992 ) or the Freytag pyramid (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged. Figure 3: Freytag’s Pyramid provides a narrative structure for storytellers. (Image source: scribophile.com) ( Large preview ) Data Visualization : Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics ( Cairo, 2016 ) transform raw data into digestible visuals, making complex information easier to understand and remember. Narrative Structures For Data Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories: The Three-Act Structure This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion ( Few, 2005 ). The Hero’s Journey (Data Edition) We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution. Example: Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement. Problems With Widely Used Data Storytelling Models Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the presentation of data over the experience of the audience, neglecting how different individuals perceive and process information. Figure 4: The traditional flow for creating a data-driven story. ( Large preview ) While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact. Cognitive Overload Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks. Emotional Disconnect Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values. Lack of Personalization Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. Over-Reliance on Visuals While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap, Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage. “ Improving On Traditional Models Traditional models can be improved by focusing more on the following two critical components: Audience understanding : A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading. Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion. Psychological principles : These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles. By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes. Persuasion In Data Storytelling All storytelling involves persuasion . Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience. Applying Psychological Principles To Data Storytelling Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive. Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. The Theory of Planned Behavior While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. The Theory of Planned Behavior (TPB) is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it. The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components: Attitude This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity. Subjective Norms These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. Perceived Behavioral Control This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior? As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991). Figure 5: The factors of the TPB interact with each other, collectively shaping an individual's behavioral intentions, which, in turn, are the most proximal determinant of human social behavior. ( Large preview ) UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article. How To Understand Your Audience And Apply Psychological Principles OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore how to integrate them into your storytelling process. Introducing The Audience Research Informed Data Storytelling Model (ARIDSM) At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories: Figure 6: The 5 steps of the Audience Research Informed Data Storytelling Model (ARIDSM). ( Large preview ) Step 1: Define Clear Objectives Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away? Why it matters : Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact. How to execute Step 1 : Start by asking yourself: What is the core message I want to convey? What do I want my audience to think, feel, or do after experiencing this story? How will I measure the success of my data story? Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.” Example: Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months. Step 2: Conduct UX Research To Understand Your Audience This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information. Why it matters : Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood. How to execute Step 2 : Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members. Example: If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story. Step 3: Analyze and Select Relevant Audience Data This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience. Why it matters : Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive. How to execute Step 3 : Clean and organize your data. Ensure accuracy and consistency before analysis. Identify key variables and metrics. This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items. Example: If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group. Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3] In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power. Why it matters : The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective. How to execute Step 4 : Here’s how to leverage the TPB in your data story: Influence Attitudes : Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. Shape Subjective Norms : Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise. Enhance Perceived Behavioral Control : Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome. Step 5: Craft A Balanced And Persuasive Narrative This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act. Why it matters : A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it. How to execute Step 5 : Structure your story strategically : Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you’ve cultivated throughout the narrative. Example: In a data story about promoting exercise, you could: Determine what stories might be available using the data you have collected or obtained. In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners. Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes). Integrate TPB elements : Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control). Use infographics to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling. Call to action : Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities. Evaluating The Method Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion. We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change. We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate. Step 1 : We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the hottest month ever recorded . Step 2 : We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research. Step 3 : We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a factor analysis to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change. Step 4 [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do. Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are no standardized questions (PDF) validated to measure the concepts for each individual topic. Item Measures Scale How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change? Attitude 1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial” How much do you think the people you care about (family, friends, community) expect you to take action against climate change? Subjective Norms 1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action” How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact? Perceived Behavioral Control 1 to 5 with 1 being “not at all confident” and 5 being “extremely confident” Table 1: Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component. Step 5 : We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created. Figure 7: Control data story. For the control story, we displayed the data around daily surface air temperature with some additional information explaining the chart. We did not attempt to influence behavior or tap into psychology to suggest there was urgency or persuade the participant to want to learn more. The color used in the chart comes from the initial chart generated in the source. We acknowledge that color is likely to present some psychological influence, given the use of red to represent extreme heat and cooler colors like blue to represent cooler time periods. ( Large preview ) Figure 8: Group 1 data story. Our measurements suggested that the participants in Group 1 had a higher level of awareness of climate change and the related negative impacts of more extreme temperatures. Therefore, we didn’t call out the potential negatives of climate change and instead focused on a more positive message of how we might make a positive impact. Group one had higher levels of subjective norms, suggesting that language promoting how others engage in certain behaviors might align with what they believe to be true. We focused on the community aspect of the message, encouraging them to act. ( Large preview ) Step 6 : We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group. Step 7 : We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message. Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic. For example, you might feel Maslow’s hierarchy of needs is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs. Elevate Your Data Storytelling Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as audience understanding and the application of psychological principles . By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives. The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level . This approach ensures that data is not merely presented but is transformed into a meaningful experience that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform. Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience. Please feel free to use our method and provide any feedback on your experience to the author. (yk) Explore more on UX Design Storytelling Smashing Newsletter Tips on front-end & UX, delivered weekly in your inbox. Just the things you can actually use. Front-End & UX Workshops, Online With practical takeaways, live sessions, video recordings and a friendly Q&A. TypeScript in 50 Lessons Everything TypeScript, with code walkthroughs and examples. And other printed books.",The Human Element: Using Research And Psychology To Elevate Data Storytelling,"

Key Points:
",Web Development,"<p>Data storytelling is a powerful communication tool that combines data analysis with narrative techniques to create impactful stories. It goes beyond presenting raw numbers by transforming complex data into meaningful insights that can drive decisions, influence behavior, and spark action. </p>
<p>When done right, data storytelling simplifies complex information, engages the audience, and compels them to act. Effective data storytelling allows UX professionals to effectively communicate the “why” behind their design choices, advocate for <strong>user-centered improvements</strong>, and ultimately create <strong>more impactful and persuasive presentations</strong>. This translates to stronger buy-in for research initiatives, increased alignment across teams, and, ultimately, products and experiences that truly meet user needs.</p>
<p>For instance, <a href=""https://www.nytimes.com/projects/2012/snow-fall/index.html#/?part=tunnel-creek"">The New York Times’ <em>Snow Fall</em></a> data story (Figure 1) used data to immerse readers in the tale of a deadly avalanche through interactive visuals and text, while <a href=""https://www.theguardian.com/us-news/ng-interactive/2015/jun/01/the-counted-police-killings-us-database"">The Guardian’s <em>The Counted</em></a> (Figure 2) powerfully illustrated police violence in the U.S. by humanizing data through storytelling. These examples show that effective data storytelling can leave lasting impressions, prompting readers to think differently, act, or make informed decisions.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/1-nyt-snow-fall-data-visualization.png"" /></p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/2-guardian-data-story.png"" /></p>
<p>The importance of data storytelling lies in its ability to:</p>
<ul>
<li><strong>Simplify complexity</strong><br />It makes data understandable and actionable.</li>
<li><strong>Engage and persuade</strong><br />Emotional and cognitive engagement ensures audiences not only understand but also feel compelled to act.</li>
<li><strong>Bridge gaps</strong><br />Data storytelling connects the dots between information and human experience, making the data relevant and relatable.</li>
</ul>
<p>While there are numerous models of data storytelling, here are a few high-level areas of focus UX practitioners should have a grasp on: </p>
<p><strong>Narrative Structures</strong>: Traditional storytelling models like the <strong>hero’s journey</strong> (<a href=""https://www.amazon.com/Writers-Journey-Storytellers-Screenwriters-Christopher/dp/B00EKYS38S"">Vogler, 1992</a>) or the <strong><a href=""https://www.masterclass.com/articles/freytags-pyramid"">Freytag pyramid</a></strong> (Figure 3) provide a backbone for structuring data stories. These models help create a beginning, rising action, climax, falling action, and resolution, keeping the audience engaged.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/3-freytag-pyramid.png"" /></p>
<p><strong>Data Visualization</strong>: Broadly speaking, these are the tools and techniques for visualizing data in our stories. Interactive charts, maps, and infographics (<a href=""https://www.semanticscholar.org/paper/The-Truthful-Art%3A-Data%2C-Charts%2C-and-Maps-for-by-Marchese/60ec847558e415d72e1ed10ec2f32bd97486a510?p2df"">Cairo, 2016</a>) transform raw data into digestible visuals, making complex information easier to understand and remember.</p>
<h4>Narrative Structures For Data</h4>
<p>Moving beyond these basic structures, let’s explore how more sophisticated narrative techniques can enhance the impact of data stories:</p>
<ul>
<li><strong>The Three-Act Structure</strong><br />This approach divides the data story into setup, confrontation, and resolution. It helps build context, present the problem or insight, and offer a solution or conclusion (<a href=""https://www.amazon.com/Screenplay-Foundations-Screenwriting-Syd-Field/dp/0385339038"">Few, 2005</a>).</li>
<li><strong>The Hero’s Journey (Data Edition)</strong><br />We can frame a data set as a problem that needs a hero to overcome. In this case, the hero is often the audience or the decision-maker who needs to use the data to solve a problem. The data itself becomes the journey, revealing challenges, insights, and, ultimately, a path to resolution.</li>
</ul>
<blockquote>Example:<br />Presenting data on declining user engagement could follow the hero’s journey. The “call to adventure” is the declining engagement. The “challenges” are revealed through data points showing where users are dropping off. The “insights” are uncovered through further analysis, revealing the root causes. The “resolution” is the proposed solution, supported by data, that the audience (the hero) can implement.</blockquote>

Problems With Widely Used Data Storytelling Models
<p>Many data storytelling models follow a traditional, linear structure: data selection, audience tailoring, storyboarding with visuals, and a call to action. While these models aim to make data more accessible, they often fail to engage the audience on a deeper level, leading to missed opportunities. This happens because they prioritize the <em>presentation</em> of data over the <em>experience</em> of the audience, neglecting how different individuals perceive and process information.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/4-data-storytelling-structure.jpg"" /></p>
<p>While existing data storytelling models adhere to a structured and technically correct approach to data creation, they often fall short of fully analyzing and understanding their audience. This gap weakens their overall effectiveness and impact.</p>
<ul>
<li><strong>Cognitive Overload</strong><br />Presenting too much data without context or a clear narrative overwhelms the audience. Instead of enlightenment, they experience confusion and disengagement. It’s like trying to drink from a firehose; the sheer volume becomes counterproductive. This overload can be particularly challenging for individuals with cognitive differences who may require information to be presented in smaller, more digestible chunks.</li>
<li><strong>Emotional Disconnect</strong><br />Data-heavy presentations often fail to establish an emotional connection, which is crucial for driving audience engagement and action. People are more likely to remember and act upon information that resonates with their feelings and values.</li>
<li><strong>Lack of Personalization</strong><br />Many data stories adopt a one-size-fits-all approach. Without tailoring the narrative to specific audience segments, the impact is diluted. A message that resonates with a CEO might not land with frontline employees. </li>
<li><strong>Over-Reliance on Visuals</strong><br />While visuals are essential for simplifying data, they are insufficient without a cohesive narrative to provide context and meaning, and they may not be accessible to all audience members. </li>
</ul>
<p>These shortcomings reveal a critical flaw: while current models successfully follow a structured data creation process, they often neglect the deeper, audience-centered analysis required for actual storytelling effectiveness. To bridge this gap,</p>
<p>Data storytelling must evolve beyond simply presenting information — it should prioritize audience understanding, engagement, and accessibility at every stage.</p>
<h3>Improving On Traditional Models</h3>
<p>Traditional models can be improved by focusing more on the following two critical components:</p>
<p><strong>Audience understanding</strong>: A greater focus can be concentrated on who the audience is, what they need, and how they perceive information. Traditional models should consider the unique characteristics and needs of specific audiences. This lack of audience understanding can lead to data stories that are irrelevant, confusing, or even misleading.</p>
<p>Effective data storytelling requires a deep understanding of the audience’s demographics, psychographics, and information needs. This includes understanding their level of knowledge about the topic, their prior beliefs and attitudes, and their motivations for seeking information. By tailoring the data story to a specific audience, storytellers can increase engagement, comprehension, and persuasion.</p>
<p><strong>Psychological principles</strong>: These models could be improved with insights from psychology that explain how people process information and make decisions. Without these elements, even the most beautifully designed data story may fall flat. Traditional models of data storytelling can be improved with two critical components that are essential for creating impactful and persuasive narratives: audience understanding and psychological principles.</p>
<p>By incorporating audience understanding and psychological principles into their storytelling process, data storytellers can create more effective and engaging narratives that resonate with their audience and drive desired outcomes.</p>
<h4>Persuasion In Data Storytelling</h4>
<p>All storytelling involves <strong>persuasion</strong>. Even if it’s a poorly told story and your audience chooses to ignore your message, you’ve persuaded them to do that. When your audience feels that you understand them, they are more likely to be persuaded by your message. Data-driven stories that speak to their hearts and minds are more likely to drive action. You can frame your message effectively when you have a deeper understanding of your audience.</p>
<h3>Applying Psychological Principles To Data Storytelling</h3>
<p>Humans process information based on psychological cues such as cognitive ease, social proof, and emotional appeal. By incorporating these principles, data storytellers can make their narratives more engaging, memorable, and persuasive.</p>
<p>Psychological principles help data storytellers tap into how people perceive, interpret, and remember information. </p>
<p><strong>The Theory of Planned Behavior</strong></p>
<p>While there is no single truth when it comes to how human behavior is created or changed, it is important for a data storyteller to use a theoretical framework to ensure they address the appropriate psychological factors of their audience. <strong>The Theory of Planned Behavior (TPB)</strong> is a commonly cited theory of behavior change in academic psychology research and courses. It’s useful for creating a reasonably effective framework to collect audience data and build a data story around it.</p>
<p>The TPB (Ajzen 1991) (Figure 5) aims to predict and explain human behavior. It consists of three key components:</p>
<ol>
<li><strong>Attitude</strong><br />This refers to the degree to which a person has a favorable or unfavorable evaluation of the behavior in question. An example of attitudes in the TPB is a person’s belief about the importance of regular exercise for good health. If an individual strongly believes that exercise is beneficial, they are likely to have a favorable attitude toward engaging in regular physical activity.</li>
<li><strong>Subjective Norms</strong><br />These are the perceived social pressures to perform or not perform the behavior. Keeping with the exercise example, this would be how a person thinks their family, peers, community, social media, and others perceive the importance of regular exercise for good health. </li>
<li><strong>Perceived Behavioral Control</strong><br />This component reflects the perceived ease or difficulty of performing the behavior. For our physical activity example, does the individual believe they have access to exercise in terms of time, equipment, physical capability, and other potential aspects that make them feel more or less capable of engaging in the behavior?</li>
</ol>
<p>As shown in Figure 5, these three components interact to create behavioral intentions, which are a proxy for actual behaviors that we often don’t have the resources to measure in real-time with research participants (Ajzen, 1991).</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/5-theory-planned-behaviour.png"" /></p>
<p>UX researchers and data storytellers should develop a working knowledge of the TPB or another suitable psychological theory before moving on to measure the audience’s attitudes, norms, and perceived behavioral control. We have included additional resources to support your learning about the TPB in the references section of this article.</p>
How To Understand Your Audience And Apply Psychological Principles
<p>OK, we’ve covered the importance of audience understanding and psychology. These two principles serve as the foundation of the proposed model of storytelling we’re putting forth. Let’s explore <em>how</em> to integrate them into your storytelling process.</p>
<h3>Introducing The Audience Research Informed Data Storytelling Model (ARIDSM)</h3>
<p>At the core of successful data storytelling lies a deep understanding of your audience’s psychology. Here’s a five-step process to integrate UX research and psychological principles effectively into your data stories:</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/6-five-step-aridsm.png"" /></p>
<h4>Step 1: Define Clear Objectives</h4>
<p>Before diving into data, it’s crucial to establish precisely what you aim to achieve with your story. Do you want to inform, persuade, or inspire action? What specific message do you want your audience to take away?</p>
<p><strong>Why it matters</strong>: Defining clear objectives provides a roadmap for your storytelling journey. It ensures that your data, narrative, and visuals are all aligned toward a common goal. Without this clarity, your story risks becoming unfocused and losing its impact.</p>
<p><strong>How to execute Step 1</strong>: Start by asking yourself:</p>
<ul>
<li>What is the core message I want to convey?</li>
<li>What do I want my audience to think, feel, or do after experiencing this story?</li>
<li>How will I measure the success of my data story?</li>
</ul>
<p>Frame your objectives using action verbs and quantifiable outcomes. For example, instead of “raise awareness about climate change,” aim to “persuade 20% of the audience to adopt one sustainable practice.”</p>
<blockquote>Example:<br />Imagine you’re creating a data story about employee burnout. Your objective might be to convince management to implement new policies that promote work-life balance, with the goal of reducing reported burnout cases by 15% within six months.</blockquote>

<h4>Step 2: Conduct UX Research To Understand Your Audience</h4>
<p>This step involves gathering insights about your audience: their demographics, needs, motivations, pain points, and how they prefer to consume information.</p>
<p><strong>Why it matters</strong>: Understanding your audience is fundamental to crafting a story that resonates. By knowing their preferences and potential biases, you can tailor your narrative and data presentation to capture their attention and ensure the message is clearly understood.</p>
<p><strong>How to execute Step 2</strong>: Employ UX research methods like surveys, interviews, persona development, and testing the message with potential audience members.</p>
<blockquote>Example:<br />If your data story aims to encourage healthy eating habits among college students, your research might conduct a survey of students to determine what types of attitudes exist towards specific types of healthy foods for eating, to apply that knowledge in your data story.</blockquote>

<h4>Step 3: Analyze and Select Relevant Audience Data</h4>
<p>This step bridges the gap between raw data and meaningful insights. It involves exploring your data to identify patterns, trends, and key takeaways that support your objectives and resonate with your audience.</p>
<p><strong>Why it matters</strong>: Careful data analysis ensures that your story is grounded in evidence and that you’re using the most impactful data points to support your narrative. This step adds credibility and weight to your story, making it more convincing and persuasive.</p>
<p><strong>How to execute Step 3</strong>:</p>
<ul>
<li><strong>Clean and organize your data.</strong><br />Ensure accuracy and consistency before analysis.</li>
<li><strong>Identify key variables and metrics.</strong><br />This will be determined by the psychological principle you used to inform your research. Using the TPB, we might look closely at how we measured social norms to understand directionally how the audience perceives social norms around the topic of the data story you are sharing, allowing you to frame your call to action in ways that resonate with these norms. You might run a variety of statistics at this point, including factor analysis to create groups based on similar traits, t-tests to determine if averages on your measurements are significantly different between groups, and correlations to see if there might be an assumed direction between scores on various items.</li>
</ul>
<blockquote>Example:<br />If your objective is to demonstrate the effectiveness of a new teaching method, analyzing how your audience perceives their peers to be open to adopting new methods, their belief that they are in control over the decision to use a new teaching method, and their attitude towards the effectiveness of their current teaching methods to create groups that have various levels of receptivity in trying new methods, allowing you to later tailor your data story for each group.</blockquote>

<h4>Step 4: Apply The Theory of Planned Behavior Or Your Psychological Principle Of Choice [Done Simultaneous With Step 3]</h4>
<p>In this step, you will see that The Theory of Planned Behavior (TPB) provides a robust framework for understanding the factors that drive human behavior. It posits that our intentions, which are the strongest predictors of our actions, are shaped by three core components: attitudes, subjective norms, and perceived behavioral control. By consciously incorporating these elements into your data story, you can significantly enhance its persuasive power.</p>
<p><strong>Why it matters</strong>: The TPB offers valuable insights into how people make decisions. By aligning your narrative with these psychological drivers, you increase the likelihood of influencing your audience’s intentions and, ultimately, their behavior. This step adds a layer of strategic persuasion to your data storytelling, making it more impactful and effective.</p>
<p><strong>How to execute Step 4</strong>:</p>
<p>Here’s how to leverage the TPB in your data story:</p>
<p><strong>Influence Attitudes</strong>: Present data and evidence that highlight the positive consequences of adopting the desired behavior. Frame the behavior as beneficial, valuable, and aligned with the audience’s values and aspirations. </p>
<p>This is where having a deep knowledge of the audience is helpful. Let’s imagine you are creating a data story on exercise and your call to action promoting exercise daily. If you know your audience has a highly positive attitude towards exercise, you can capitalize on that and frame your language around the benefits of exercising, increasing exercise, or specific exercises that might be best suited for the audience. It’s about framing exercise not just as a physical benefit but as a holistic improvement to their life. You can also tie it to their identity, positioning exercise as an integral part of living the kind of life they aspire to. </p>
<p><strong>Shape Subjective Norms</strong>: Demonstrate that the desired behavior is widely accepted and practiced by others, especially those the audience admires or identifies with. Knowing ahead of time if your audience thinks daily exercise is something their peers approve of or engage in will allow you to shape your messaging accordingly. Highlight testimonials, success stories, or case studies from individuals who mirror the audience’s values. </p>
<p>If you were to find that the audience does not consider exercise to be normative amongst peers, you would look for examples of similar groups of people who do exercise. For example, if your audience is in a certain age group, you might focus on what data you have that supports a large percentage of those in their age group engaging in exercise.</p>
<p><strong>Enhance Perceived Behavioral Control</strong>: Address any perceived barriers to adopting the desired behavior and provide practical solutions. For instance, when promoting daily exercise, it’s important to acknowledge the common obstacles people face — lack of time, resources, or physical capability — and demonstrate how these can be overcome.</p>
<h4>Step 5: Craft A Balanced And Persuasive Narrative</h4>
<p>This is where you synthesize your data, audience insights, psychological principles (including the TPB), and storytelling techniques into a compelling and persuasive narrative. It’s about weaving together the logical and emotional elements of your story to create an experience that resonates with your audience and motivates them to act.</p>
<p><strong>Why it matters</strong>: A well-crafted narrative transforms data from dry statistics into a meaningful and memorable experience. It ensures that your audience not only understands the information but also feels connected to it on an emotional level, increasing the likelihood of them internalizing the message and acting upon it.</p>
<p><strong>How to execute Step 5</strong>:</p>
<p><strong>Structure your story strategically</strong>: Use a clear narrative arc that guides your audience through the information. Begin by establishing the context and introducing the problem, then present your data-driven insights in a way that supports your objectives and addresses the TPB components. Conclude with a compelling call to action that aligns with the attitudes, norms, and perceived control you've cultivated throughout the narrative.</p>
<blockquote>Example:<br />In a data story about promoting exercise, you could:<ul><li><strong>Determine what stories might be available using the data you have collected or obtained.</strong> In this example, let’s say you work for a city planning office and have data suggesting people aren’t currently biking as frequently as they could, even if they are bike owners.</li><li>Begin with a relatable story about lack of exercise and its impact on people’s lives. Then, present data on the benefits of cycling, highlighting its positive impact on health, socializing, and personal feelings of well-being (attitudes).</li><li><strong>Integrate TPB elements</strong>: Showcase stories of people who have successfully incorporated cycling into their daily commute (subjective norms). Provide practical tips on bike safety, route planning, and finding affordable bikes (perceived behavioral control).</li><li><strong>Use infographics</strong> to compare commute times and costs between driving and cycling. Show maps of bike-friendly routes and visually appealing images of people enjoying cycling.</li><li><strong>Call to action</strong>: Encourage the audience to try cycling for a week and provide links to resources like bike share programs, cycling maps, and local cycling communities.</li></ul></blockquote>

<h3>Evaluating The Method</h3>
<p>Our next step is to test our hypothesis that incorporating audience research and psychology into creating a data story will lead to more powerful results. We have conducted preliminary research using messages focused on climate change, and our results suggest some support for our assertion.</p>
<p>We purposely chose a controversial topic because we believe data storytelling can be a powerful tool. If we want to truly realize the benefits of effective data storytelling, we need to focus on topics that matter. We also know that academic research suggests it is more difficult to shift opinions or generate behavior around topics that are polarizing (at least in the US), such as climate change.</p>
<p>We are not ready to share the full results of our study. We will share those in an academic journal and in conference proceedings. Here is a look at how we set up the study and how you might do something similar when either creating a data story using our method or doing your own research to test our model. You will see that it closely aligns with the model itself, with the added steps of testing the message against a control message and taking measurements of the actions the message(s) are likely to generate.</p>
<p><strong>Step 1</strong>: We chose our topic and the data set we wanted to explore. As I mentioned, we purposely went with a polarizing topic. My academic background was in messaging around conservation issues, so we explored that. We used data from a publicly available data set that states July 2023 was the <a href=""https://climate.copernicus.eu/surface-air-temperature-july-2023"">hottest month ever recorded</a>.</p>
<p><strong>Step 2</strong>: We identified our audience and took basic measurements. We decided our audience would be members of the general public who do not have jobs working directly with climate data or other relevant fields for climate change scientists. </p>
<p>We wanted a diverse range of ages and backgrounds, so we screened for this in our questions on the survey to measure the TPB components as well. We created a survey to measure the elements of the TPB as it relates to climate change and administered the survey via a Google Forms link that we shared directly, on social media posts, and in online message boards related to topics of climate change and survey research.</p>
<p><strong>Step 3</strong>: We analyzed our data and broke our audience into groups based on key differences. This part required a bit of statistical know-how. Essentially, we entered all of the responses into a spreadsheet and ran a <a href=""https://en.wikipedia.org/wiki/Factor_analysis"">factor analysis</a> to define groups based on shared attributes. In our case, we found two distinct groups for our respondents. We then looked deeper into the individual differences between the groups, e.g., group 1 had a notably higher level of positive attitude towards taking action to remediate climate change.</p>
<p><strong>Step 4</strong> [remember this happens simultaneously with step 3]: We incorporated aspects of the TPB in how we framed our data analysis. As we created our groups and looked at the responses to the survey, we made sure to note how this might impact the story for our various groups. Using our previous example, a group with a higher positive attitude toward taking action might need less convincing to do something about climate change and more information on what exactly they can do.</p>
<p>Table 1 contains examples of the questions we asked related to the TPB. We used the guidance provided here to generate the survey items to measure the TPB related to climate change activism. Note that even the academic who created the TPB states there are <a href=""https://people.umass.edu/aizen/pdf/tpb.measurement.pdf"">no standardized questions</a> (PDF) validated to measure the concepts for each individual topic. </p>
<table>
    <thead>
        <tr>
            <th>Item</th>
            <th>Measures</th>
      <th>Scale</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>How beneficial do you believe individual actions are compared to systemic changes (e.g., government policies) in tackling climate change?</td>
            <td>Attitude</td>
      <td>1 to 5 with 1 being “not beneficial” and 5 being “extremely beneficial”</td>
        </tr>
        <tr>
            <td>How much do you think the people you care about (family, friends, community) expect you to take action against climate change?</td>
            <td>Subjective Norms</td>
      <td>1 to 5 with 1 being “they do not expect me to take action” and 5 being “they expect me to take action”</td>
        </tr>
        <tr>
            <td>How confident are you in your ability to overcome personal barriers when trying to reduce your environmental impact?</td>
            <td>Perceived Behavioral Control</td>
      <td>1 to 5 with 1 being “not at all confident” and 5 being “extremely confident”</td>
        </tr>
    </tbody>
</table>

<p><strong><em>Table 1:</em></strong> <em>Examples of questions we used to measure the TPB factors. We asked multiple questions for each factor and then generated a combined mean score for each component.</em></p>
<p><strong>Step 5</strong>: We created data stories aligned with the groups and a control story. We created multiple stories to align with the groups we identified in our audience. We also created a control message that lacked substantial framing in any direction. See below for an example of the control data story (Figure 7) and one of the customized data stories (Figure 8) we created.</p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/7-control-data-story.jpg"" /></p>
<p><img src=""https://files.smashing.media/articles/human-element-using-research-psychology-elevate-data-storytelling/8-customized-data-stories.jpg"" /></p>
<p><strong>Step 6</strong>: We released the stories and took measurements of the likelihood of acting. Specific to our study, we asked the participants how likely they were to “Click here to LEARN MORE.” Our hypothesis was that individuals would express a notably higher likelihood to want to click to learn more on the data story aligned with their grouping, as compared to the competing group and the control group.</p>
<p><strong>Step 7</strong>: We analyzed the differences between the preexisting groups and what they stated was their likelihood of acting. As I mentioned, our findings are still preliminary, and we are looking at ways to increase our response rate so we can present statistically substantiated findings. Our initial findings are that we do see small differences between the responses to the tailored data stories and the control data story. This is directionally what we would be expecting to see. If you are going to conduct a similar study or test out your messages, you would also be looking for results that suggest your ARIDS-derived message is more likely to generate the expected outcome than a control message or a non-tailored message.</p>
<p>Overall, we feel there is an exciting possibility and that future research will help us refine exactly what is critical about generating a message that will have a positive impact on your audience. We also expect there are better models of psychology to use to frame your measurements and message depending on the audience and topic.</p>
<p>For example, you might feel <a href=""https://www.simplypsychology.org/maslow.html"">Maslow’s hierarchy of needs</a> is more relevant to your data storytelling. You would want to take measurements related to these needs from your audience and then frame the data story using how a decision might help meet their needs.</p>
Elevate Your Data Storytelling
<p>Traditional models of data storytelling, while valuable, often fall short of effectively engaging and persuading audiences. This is primarily due to their neglect of crucial aspects such as <strong>audience understanding</strong> and <strong>the application of psychological principles</strong>. By incorporating these elements into the data storytelling process, we can create more impactful and persuasive narratives.</p>
<p>The five-step framework proposed in this article — defining clear objectives, conducting UX research, analyzing data, applying psychological principles, and crafting a balanced narrative — provides <strong>a roadmap for creating data stories that resonate with audiences on both a cognitive and emotional level</strong>. This approach ensures that data is not merely presented but is transformed into <strong>a meaningful experience</strong> that drives action and fosters change. As data storytellers, embracing this human-centric approach allows us to unlock the full potential of data and create narratives that truly inspire and inform.</p>
<p>Effective data storytelling isn’t a black box. You can test your data stories for effectiveness using the same research process we are using to test our hypothesis as well. While there are additional requirements in terms of time as a resource, you will make this back in the form of a stronger impact on your audience when they encounter your data story if it is shown to be significantly greater than the impact of a control message or other messages you were considering that don’t incorporate the psychological traits of your audience.</p>
<p>Please feel free to use our method and provide any feedback on your experience to the author.</p>"
The iPhone 16e: a “just enough tech” innovation for value,https://uxdesign.cc/the-iphone-16e-a-just-enough-tech-innovation-for-value-8b4ce63e5635?source=rss----138adf9c44c---4,UX Collective,2025-02-26T23:17:22,UX Collective,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"The iPhone 16e: a “just enough tech” innovation for value Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. Ian Batterbee · Follow Published in UX Collective · 9 min read · 3 days ago -- 4 Listen Share iPhone 16e. Credit: Apple Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the viral meme of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough. Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the iPhone 16e . Tim Cook unveiling the newest member of the family. Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025. That said, there is something intriguing about Apple’s latest offering: an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack. The “Just Enough Tech” New Family Member On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³ Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access. As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag. Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200! OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture? Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play: Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.” “Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose. This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s Just Enough Design .⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess. Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective. Zooming Out to View the Bigger Picture The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience. Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶ Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶ “A camera so advanced, it’s like getting two in one.” — Apple. A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses. The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: Apple Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess. When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷ Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap. Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features. “Just Enough Tech” in Other Fields Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs. Let’s explore three examples proving this beyond Apple. DeepSeek: The AI Underdog Built with Less DeepSeek launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market. DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design . The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry. Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game. Notion: Productivity Without the Bloat The Notion app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity. By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.” Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰ Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value. Be My Eyes: Vision Assistance, Human Touch Be My Eyes is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs. “Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes. Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹ Be My Eyes. Credit: Be My Eyes With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users. “Just Enough Tech” is a Strategic Choice Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most. Instead of asking, “What else can we add?” we focus on, “What do people need most?” “Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out. Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions: Who are we innovating for, and what problem are we solving? How do our audience’s needs, circumstances, and cultural contexts shape their priorities? What are the essential features that provide real value without unnecessary complexity? How do we balance diverse needs while keeping the experience simple and intuitive? How do we measure whether our technology delivers “enough” value or is missing the mark? How do we decide which features to keep, refine, or remove to maximise impact? Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters. The Takeaway The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability. Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. “Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined. What’s your version of “enough”? Tell me in the comments — I’m curious! References [1]: Apple. (February 24th, 2025). Apple will spend more than $500 billion in the U.S. over the next four years https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/ [2]: The Verge. (February 21st, 2025). Apple pulls encryption feature from UK over government spying demands https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor [3], [7]: Reuters. (February 19th, 2025). Apple reveals its version of budget AI: the $599 iPhone 16e https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19 [4]: Bora — UX Collective. (October 7th, 2022). Dieter Rams and 10 principles for good design https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6 [5]: Taku Satoh. (Published October 4th, 2022). Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo https://www.goodreads.com/en/book/show/60420633-just-enough-design [6]: Business Insider. (February 19th, 2025). Apple’s new $599 iPhone with AI is the Hail Mary it needs https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2 [8]: TechTarget. (February 6th, 2025). DeepSeek explained: Everything you need to know https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know [9]: Notion. (September 3rd, 2024). 100 Million of You https://www.notion.com/blog/100-million-of-you [10]: Wayne Yap. Notion’s three philosophies https://x.com/wayneyap/status/1894337810813128795 [11]: Be My Eyes. Bringing Sight To Blind and Low-vision People https://www.bemyeyes.com [12]: Be My Eyes. Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation https://www.bemyeyes.com/blog/10-years-of-be-my-eyes","The iPhone 16e: a “just enough tech” innovation for value Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. Ian Batterbee · Follow Published in UX Collective · 9 min read · 3 days ago -- 4 Listen Share iPhone 16e. Credit: Apple Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the viral meme of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough. Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the iPhone 16e . Tim Cook unveiling the newest member of the family. Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025. That said, there is something intriguing about Apple’s latest offering: an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack. The “Just Enough Tech” New Family Member On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³ Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access. As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag. Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200! OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture? Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play: Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.” “Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose. This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s Just Enough Design .⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess. Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective. Zooming Out to View the Bigger Picture The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience. Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶ Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶ “A camera so advanced, it’s like getting two in one.” — Apple. A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses. The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: Apple Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess. When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷ Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap. Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features. “Just Enough Tech” in Other Fields Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs. Let’s explore three examples proving this beyond Apple. DeepSeek: The AI Underdog Built with Less DeepSeek launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market. DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design . The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry. Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game. Notion: Productivity Without the Bloat The Notion app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity. By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.” Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰ Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value. Be My Eyes: Vision Assistance, Human Touch Be My Eyes is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs. “Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes. Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹ Be My Eyes. Credit: Be My Eyes With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users. “Just Enough Tech” is a Strategic Choice Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most. Instead of asking, “What else can we add?” we focus on, “What do people need most?” “Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out. Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions: Who are we innovating for, and what problem are we solving? How do our audience’s needs, circumstances, and cultural contexts shape their priorities? What are the essential features that provide real value without unnecessary complexity? How do we balance diverse needs while keeping the experience simple and intuitive? How do we measure whether our technology delivers “enough” value or is missing the mark? How do we decide which features to keep, refine, or remove to maximise impact? Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters. The Takeaway The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability. Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose. “Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined. What’s your version of “enough”? Tell me in the comments — I’m curious! References [1]: Apple. (February 24th, 2025). Apple will spend more than $500 billion in the U.S. over the next four years https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/ [2]: The Verge. (February 21st, 2025). Apple pulls encryption feature from UK over government spying demands https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor [3], [7]: Reuters. (February 19th, 2025). Apple reveals its version of budget AI: the $599 iPhone 16e https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19 [4]: Bora — UX Collective. (October 7th, 2022). Dieter Rams and 10 principles for good design https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6 [5]: Taku Satoh. (Published October 4th, 2022). Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo https://www.goodreads.com/en/book/show/60420633-just-enough-design [6]: Business Insider. (February 19th, 2025). Apple’s new $599 iPhone with AI is the Hail Mary it needs https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2 [8]: TechTarget. (February 6th, 2025). DeepSeek explained: Everything you need to know https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know [9]: Notion. (September 3rd, 2024). 100 Million of You https://www.notion.com/blog/100-million-of-you [10]: Wayne Yap. Notion’s three philosophies https://x.com/wayneyap/status/1894337810813128795 [11]: Be My Eyes. Bringing Sight To Blind and Low-vision People https://www.bemyeyes.com [12]: Be My Eyes. Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation https://www.bemyeyes.com/blog/10-years-of-be-my-eyes",The iPhone 16e: a “just enough tech” innovation for value,"

Key Points:
",Web Development,"<h4><strong>Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose.</strong></h4><figure><img alt=""Close-up of the iPhone 16e’s bottom edge, showcasing the USB-C port, speaker grilles, and a sleek metallic frame. Credit: Apple."" src=""https://cdn-images-1.medium.com/max/1024/1*TWm2M5KQ7J5WRR6xTrFvsg.jpeg"" /><figcaption>iPhone 16e. Credit: <a href=""https://www.apple.com/uk/iphone-16e/"">Apple</a></figcaption></figure><p>Apple spent several days teasing social media users about its newest addition to the iPhone family. Critics joked it would be another “same shirt, different day” moment — just like the <a href=""https://www.reddit.com/r/Wellthatsucks/comments/efgcbv/not_only_did_grandpa_already_have_this_shirt_he/"">viral meme</a> of a man happily unboxing a shirt identical to the one he was already wearing. Still, some fans remained hopeful for a breakthrough.</p><p>Then, on February 19th, Tim Cook finally pulled back the curtain — only to reveal what many expected: another (drumroll) iPhone — the <a href=""https://www.apple.com/uk/iphone-16e/?afid=p238%7Cshln14Dmp-dc_mtid_20925ukn39931_pcrid_733787381714_pgrid_182022441584_pexid__ptid_kwd-2466237873037_&amp;cid=wwa-uk-kwgo-iphone-slid---productid--NonCore--Announce-"">iPhone 16e</a>.</p><a href=""https://medium.com/media/6e916140935ead96fa497019607aff4f/href"">https://medium.com/media/6e916140935ead96fa497019607aff4f/href</a><p>Apple certainly had a busy month, making headlines with its $500 billion investment plan in the U.S. and a high-stakes clash with the UK government over encryption.¹ ² However, Tim Cook’s new iPhone announcement was not exactly a flagship nor a groundbreaking innovation many expected for early 2025.</p><p>That said, there is something intriguing about Apple’s latest offering: <strong>an approach on “Just Enough Tech,” which prioritises relevance over excess — a philosophy we’ll unpack.</strong></p><h3>The “Just Enough Tech” New Family Member</h3><p>On February 19th, 2025, Apple unveiled the iPhone 16e, a budget-friendly smartphone targeting mid-market customers in the growing markets of China and India, as well as those seeking simplicity and affordability.³</p><p>Unlike its pricier siblings, the iPhone 16e skips the frills of dual cameras and MagSafe. Instead, it offers “Just Enough Tech,” including a powerful A18 chip, solid battery, a 48MP camera, and integrated Apple Intelligence to deliver what users need most: reliable performance, great photos, and ecosystem access.</p><p>As inflation squeezes budgets globally, the balance of affordability and practicality becomes more relevant. Consider a small business owner in Mumbai using the iPhone’s AI features to take high-quality product photos. Similarly, think of a student in Shanghai who relies on it to stream lectures without lag.</p><p>Yet critics pounced, expressing their opinions on social media. Common sentiments included phrases like, “Apple’s playing catch-up,” “The iPhone 16e lacks features,” and “$600 is steep for just one camera!” While some praised the iPhone’s affordability, sceptics disagreed, addressing the fact you can upgrade to the iPhone 16 for just an extra $200!</p><p>OK, criticism of the iPhone 16e’s lack of features and price point is understandable, but are we missing the bigger picture?</p><p>Let me explain. When we zoom out and shift perspective, we can see a different kind of progress at play:</p><blockquote><strong>Apple’s latest innovation isn’t about maxing specifications but achieving relevance through “Just Enough Tech.”</strong></blockquote><p>“Just Enough Tech” aligns with established methodologies such as Lean design, Agile software development, and minimalism, notably Dieter Rams’ principle of “Less, but better.”⁴ Instead of overloading products with excessive features, “Just Enough Tech” focuses on usability, efficiency, and simplicity, ensuring the technology serves a clear purpose.</p><p>This philosophy particularly resonates with the Japanese “hodo-hodo” concept explored in Taku Satoh’s <em>Just Enough Design</em>.⁵ In his book, Satoh describes the ideology as a commitment to providing “just enough” — emphasising simplicity and intentional restraint to meet real needs without unnecessary excess.</p><p>Ultimately, Apple’s strategy for the iPhone 16e reflects this thinking, prioritising essential features that focus on realising value rather than overwhelming users with unnecessary complexity. Now, let’s zoom further out to understand more about this perspective.</p><h3>Zooming Out to View the Bigger Picture</h3><p>The iPhone 16e doesn’t push the boundaries of cutting-edge technology; that is not Apple’s goal. Instead, they’re aiming for something different: a balance of affordability with technology that is “good enough” to appeal to a broader audience.</p><p>Analyst Dipanjan Chatterjee expands on Apple’s “Just Enough Tech” strategy, stating, “The iPhone 16e is the ‘on-ramp’ for customers who want the status symbol of an iPhone without spending up to $1000.”⁶</p><p>Furthermore, Jacob Bourne, an analyst at eMarketer, reflects on Apple’s broader strategic play, explaining, “It’s about expanding its ecosystem reach at a crucial moment when it’s rolling out Apple Intelligence and revamping Siri.”⁶</p><blockquote><strong>“A camera so advanced, it’s like getting two in one.” — Apple.</strong></blockquote><p>A key example of Apple’s “Just Enough Tech” strategy is the iPhone 16e’s 48MP Fusion camera, which harnesses the power of two. Using fewer, cost-effective materials, Apple enables users to capture stunning, high-resolution photos and 4K videos without the need for extra lenses.</p><figure><img alt=""A woman in a red outfit leans on a tree branch, posing against a backdrop of vibrant yellow autumn leaves. Captured using the iPhone 16e’s camera. Credit: Apple."" src=""https://cdn-images-1.medium.com/max/1024/1*8fOAnsHQ_Zd94sCulbHZLA.jpeg"" /><figcaption>The iPhone 16e’s 48MP Fusion camera that harnesses the power of two. Credit: <a href=""https://www.apple.com/uk/iphone-16e/"">Apple</a></figcaption></figure><p>Apple’s “Just Enough Tech” strategy extends to the A18 chip. While it’s not the most advanced processor in the iPhone lineup, it balances speed and power efficiency, delivering smooth performance, longer battery life, and lower costs without unnecessary excess.</p><p>When we take a step back to consider these perspectives, we gain greater insight into Apple’s innovation strategy. They want to understand what “enough” means for people worldwide and how to help them realise value. Doing so has enabled them to create more sustainable and accessible products for a broader audience, including customers in China and India.⁷</p><p>Beyond Apple’s balanced approach, they have also embraced a humanity-centred design philosophy. By making the iPhone 16e affordable and practical, Apple will provide millions more access to essential tools that level the playing field instead of widening the gap.</p><p>Ultimately, this means greater access to opportunities. When people have the technology to enhance their lives, they gain dignity, empowerment, and the ability to thrive — a deeper level of progress beyond just features.</p><h3>“Just Enough Tech” in Other Fields</h3><p>Apple’s not alone here — other innovators have proven that “Just Enough Tech” can reshape entire fields. For example, the Chinese AI platform DeepSeek recently announced it can develop its model at a fraction of the cost compared to its rivals.⁸ Productivity tool Notion has also showcased the effect of using fewer features to meet user needs.</p><p>Let’s explore three examples proving this beyond Apple.</p><h4>DeepSeek: The AI Underdog Built with Less</h4><p><a href=""https://www.deepseek.com/"">DeepSeek</a> launched as an open-source app offering free, high-performing AI to coders, students, and small firms. In January 2025, they caused quite a storm by topping the app charts and shaking the U.S. tech market.</p><blockquote>DeepSeek is proving that “Just Enough Tech” can level the playing field through open-source accessibility and efficient AI design<strong>.</strong></blockquote><p>The ChatGPT rival demonstrated the demand for affordable AI by using Nvidia H800 chips and optimised processing strategies, reducing hardware costs to around $6 million compared to hundreds of millions.⁸ Being open-source, DeepSeek effectively broadens access to AI technology, levelling the playing field in the industry.</p><p>Like the iPhone 16e’s combined camera innovation, DeepSeek demonstrates the “Just Enough Tech” philosophy by meeting critical needs — accessible and reliable AI — without the excessive infrastructure of its competitors. While some critics question its claims, its rapid adoption suggests it may be a wake-up call for Silicon Valley to up its game.</p><h4>Notion: Productivity Without the Bloat</h4><p>The <a href=""https://www.notion.com/"">Notion</a> app is a go-to productivity tool, especially among startups and small teams, offering a lightweight alternative to Microsoft and Google. With over 100 million users,⁹ it proves that streamlined, small-scale workflows can be just as powerful without unnecessary complexity.</p><blockquote><strong>By making an interface “disappear,” Notion shows that the best design is the one that “gets out of the way.”</strong></blockquote><p>Co-founder Ivan Zhao drew inspiration from cognitive science and Japanese minimalism to create an interface that “disappears.” By removing unnecessary features and offering customisation, Notion adapts to users’ needs, making interactions a natural part of the creative process.¹⁰</p><p>Like the iPhone 16e, Notion shows that “Just Enough Tech” can be more effective than feature overload. In a world where software and hardware often become cluttered with unnecessary add-ons, both Notion and the iPhone 16e demonstrate that simplicity, when executed well, delivers the most value.</p><h4>Be My Eyes: Vision Assistance, Human Touch</h4><p><a href=""https://www.bemyeyes.com/"">Be My Eyes </a>is a free-to-download app that links blind and low-vision users with minimal technology, maximising their connection to the world. It is a wonderful example of how we can shift the lens to humanity, showing how “Just Enough Tech” can serve profound human needs.</p><blockquote><strong>“Our mission is to make the world more accessible for 340 million people who are blind or have low vision.” — Be My Eyes.</strong></blockquote><p>Through live video and AI, the app connects those who need sighted assistance with volunteers — 8 million as of January 2025. You can use Be My Eyes for anything that requires visual help, such as grocery shopping or fixing computer issues.¹¹</p><figure><img alt=""A woman in a white shirt holds a smartphone frame over her eyes, aligning them with the screen cutout against a solid blue background."" src=""https://cdn-images-1.medium.com/max/811/0*LvmtyxBd6YGFVGSZ.jpg"" /><figcaption>Be My Eyes. Credit: Be My Eyes</figcaption></figure><p>With over one million monthly calls, Be My Eyes demonstrates a significant positive impact on humanity. Its “Just Enough Tech” approach prioritises accessibility over complexity, making the world more inclusive for 340 million people.¹² This philosophy mirrors Apple’s iPhone 16e, which combines the right technology with affordability to empower users.</p><h3>“Just Enough Tech” is a Strategic Choice</h3><p>Like DeepSeek, Notion, and Be My Eyes, Apple has designed the iPhone 16e for those using it, not the headlines reviewing it. These products prove that “Just Enough Tech” is not about compromising quality but prioritising the needs of the people who matter most.</p><blockquote><strong>Instead of asking, “What else can we add?” we focus on, “What do people need most?”</strong></blockquote><p>“Just Enough Tech” is a strategic choice rather than a compromise. Instead of asking, “What else can we add?” we focus on “What do people need most?” This shift from adding unnecessary features to prioritising value-driven design helps a product, service, or innovation stand out.</p><p>Apple will continue to drive high-tech innovations in its premium models; however, the iPhone 16e demonstrates that companies can meet more of their audience’s needs by focusing on the essentials. If you’re considering a “Just Enough Tech” strategy, try answering the following questions:</p><ul><li>Who are we innovating for, and what problem are we solving?</li><li>How do our audience’s needs, circumstances, and cultural contexts shape their priorities?</li><li>What are the essential features that provide real value without unnecessary complexity?</li><li>How do we balance diverse needs while keeping the experience simple and intuitive?</li><li>How do we measure whether our technology delivers “enough” value or is missing the mark?</li><li>How do we decide which features to keep, refine, or remove to maximise impact?</li></ul><p>Ultimately, “Just Enough Tech” isn’t about doing less but delivering more of what truly matters.</p><h3>The Takeaway</h3><p>The iPhone 16e, alongside DeepSeek, Notion, and Be My Eyes, offers a lesson in prioritising real needs over flashy features. It challenges the idea that innovation is solely about cutting-edge technology; instead, it shows that real progress comes from shifting perspectives towards access and affordability.</p><blockquote><strong>Apple’s modest new iPhone isn’t here to impress — it’s here to give more people access, value, and purpose.</strong></blockquote><p>“Just Enough Tech,” similar to “Just Enough Design” and other simplicity-driven philosophies, is about using technology with intention. Ultimately, innovation isn’t about having everything — it’s about knowing what’s enough. And with enough, we can achieve more value than we ever imagined.</p><p>What’s your version of “enough”? Tell me in the comments — I’m curious!</p><h3>References</h3><p>[1]: Apple. (February 24th, 2025). <em>Apple will spend more than $500 billion in the U.S. over the next four years<br /></em><a href=""https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/"">https://www.apple.com/newsroom/2025/02/apple-will-spend-more-than-500-billion-usd-in-the-us-over-the-next-four-years/</a></p><p>[2]: The Verge. (February 21st, 2025). <em>Apple pulls encryption feature from UK over government spying demands<br /></em><a href=""https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor"">https://www.theverge.com/news/617273/apple-removes-encryption-advanced-data-protection-adp-uk-spying-backdoor</a></p><p>[3], [7]: Reuters. (February 19th, 2025). <em>Apple reveals its version of budget AI: the $599 iPhone 16e<br /></em><a href=""https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19"">https://www.reuters.com/technology/apple-launch-new-lower-cost-iphone-capture-broader-market-2025-02-19</a></p><p>[4]: Bora — UX Collective. (October 7th, 2022).<em> Dieter Rams and 10 principles for good design<br /></em><a href=""https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6"">https://uxdesign.cc/dieter-rams-and-ten-principles-for-good-design-61cc32bcd6e6</a></p><p>[5]: Taku Satoh. (Published October 4th, 2022). <em>Just Enough Design: Reflections on the Japanese Philosophy of Hodo-hodo<br /></em><a href=""https://www.goodreads.com/en/book/show/60420633-just-enough-design"">https://www.goodreads.com/en/book/show/60420633-just-enough-design</a></p><p>[6]: Business Insider. (February 19th, 2025). <em>Apple’s new $599 iPhone with AI is the Hail Mary it needs<br /></em><a href=""https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2"">https://www.businessinsider.com/apple-iphone-16e-brings-ai-to-more-sales-boost-2025-2</a></p><p>[8]: TechTarget. (February 6th, 2025). <em>DeepSeek explained: Everything you need to know<br /></em><a href=""https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know"">https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know</a></p><p>[9]: Notion. (September 3rd, 2024). <em>100 Million of You<br /></em><a href=""https://www.notion.com/blog/100-million-of-you"">https://www.notion.com/blog/100-million-of-you</a></p><p>[10]: Wayne Yap. <em>Notion’s three philosophies<br /></em><a href=""https://x.com/wayneyap/status/1894337810813128795"">https://x.com/wayneyap/status/1894337810813128795</a></p><p>[11]: Be My Eyes. <em>Bringing Sight To Blind and Low-vision People<br /></em><a href=""https://www.bemyeyes.com/language/english"">https://www.bemyeyes.com</a></p><p>[12]: Be My Eyes. <em>Be My Eyes Celebrates 10 Years and a Decade of Accessibility Innovation<br /></em><a href=""https://www.bemyeyes.com/blog/10-years-of-be-my-eyes"">https://www.bemyeyes.com/blog/10-years-of-be-my-eyes</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8b4ce63e5635"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-iphone-16e-a-just-enough-tech-innovation-for-value-8b4ce63e5635"">The iPhone 16e: a “just enough tech” innovation for value</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
"UX, how can I trust you?",https://uxdesign.cc/ux-how-can-i-trust-you-47f1c8a71213?source=rss----138adf9c44c---4,UX Collective,2025-02-26T12:32:45,UX Collective,https://images.unsplash.com/photo-1605379399642-870262d3d051?q=80&w=3606&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D,"Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening.","Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening.","UX, how can I trust you?","

Key Points:
",Web Development,"Member-only story UX, how can I trust you? Steps for designers to foster confidence in the age of mistrust. Darren Yeo · Follow Published in UX Collective · 8 min read · 3 days ago -- 4 Share Trust in design is fading due to deception and tech misuse, but designers can restore it by fostering relationships, confidence, and ethical UX. Trust isn’t a feature — it’s the result of honest, user-focused design. (image source: Cal Fire ) Trust is in short supply these days. Yet in the most unlikely places, a designer can learn from another expert how to be a better designer and design for trust. This serendipitous moment happened with a conversation of two experts over the topic of trustworthiness. As Rachel Botsman observed, the decline of institutional trust is probably now a 25-year trend, but it rapidly accelerated over the last two years, particularly in the military, judges, and law. And possibly design too. The Two Forms of Mistrust in Design Much of mistrust stems from uncertainty about which information to trust. So far, there are at least two types of mistrust: Explicit Deception They are in a world where scams and hacking have implored creative methods to deceive people. Whether it is through phishing emails, deepfake calls, or website lookalikes, if the intention is to take away something from the original owner, deception usually follows. Victims falling under this category can be identified, and immediate actions can be catered to either recover the loss, mitigate further damage, or prevent future attacks from happening."
Data centres reshaping B.C.’s commercial real estate market,https://www.vancouverisawesome.com/real-estate/data-centres-reshaping-bcs-commercial-real-estate-market-10297057,GNews,2025-02-28T14:00:00Z,Vancouver Is Awesome,https://www.vmcdn.ca/f/files/biv/cologix-van3.jpg;w=1200;h=800;mode=crop,"Data centres, which quietly connect the digital world, are emerging as a darling of commercial real estate in B.C. and beyond. Although the data centre industry has complex needs and high barriers to entry, experts say it’s only just blooming, and that the industry will grow exponentially with the development and adoption of artificial intelligence, cloud computing and other technologies. Data centres, initially developed by financial institutions and others to house critical data, offer five key services to a wide range of customers: Space, power, cooling, security and interconnection. Companies used to operate “switch rooms” or “telco rooms” in-house before outsourcing that work. “It’s really exciting that data centres have now come into the mainstream a bit more,” said Jordan Scott, account director with Denver-based Cologix, Inc., which operates five data centres in and around Vancouver. “For a while there, it felt like we were kind of just operating behind a curtain or behind the shadows.” Scott, who is based in Vancouver, said data centres can be thought of as airports. They are the hubs where all the activity happens—where data is processed, transmitted and stored. “In the airports, it’s a multi-tenant environment. Many, many customers come into our facilities and they co-locate their IT infrastructure within our walls,” he said. Data centres are buildings for powering and cooling racks of servers and connecting them with fibre-optic and network cabling. Customers generally bring their own equipment, such as servers, switches, cabling, cabinets and power distribution units (PDUs). Today’s data centres are expected to provide adequate physical space to host client equipment, sufficient electrical power, liquid cooling to create an ideal environment for running equipment, physical security and interconnection with the outside world. Cologix’s 45-plus data centres in North America operate on a lease model. In addition to certain one-time costs, customers pay monthly lease rates for the square footage they occupy and their power usage, and can choose a two-, five- or 10-year term. Although real estate is scarce and expensive in Greater Vancouver, urban hubs are needed for proximity to clients, electrical power and connectivity. “None of our customers are going to want to drive six hours to the middle of nowhere if they have to reboot a server,” Scott said. Data centres must coordinate with utilities and telecommunications companies to access power and fibre networks, and the barriers to entry are significant due to the amount of time and capital involved. “You can’t just set up shop overnight,” he said. AI boosts demand for data centres “Data centres are ultimately real estate businesses,” said Marc Mondesir, Canadian managing director with California-based Equinix, Inc., which bills itself as the world’s largest data-centre company with 470,000-plus physical interconnections available to clients. “We are really in the business of providing space and power for our customers, so that they can house the computer equipment that’s required to power the internet.” Mondesir said his industry is positioned for massive growth as AI, cloud and other technologies—which ultimately runs on computers that in turn run within data centres—evolve. “What’s at the core of all this infrastructure is the network. The network is at the core of the internet. The network is at the core of AI. The network is at the core of large language models,” he said. An AI prompt currently requires 10 to 15 times more computing power compared to a keyword search, he said. This places extra demands on the grid, one reason for BC Hydro’s moratorium on Bitcoin mining announced in December 2022. Crypto volatility aside, Mondesir said there is considerable room for long-term growth in the sector. “Data centres account for less than one per cent of electricity consumption in this country,” he said. “The U.S. hovers in that three- to four-per-cent mark—these numbers change constantly—and then you’ve got China that’s reported to consume even more electricity than that, so [Canada is] sort of skimming the surface.” By concentrating providers and equipment under one roof, data centres can unlock synergies for complex computing tasks that require low latency and high security. “The closer that these network nodes are to one another, the better they perform and the more securely they perform,” he said. As demand for energy grows, so do concerns about the environment. Mondesir said Equinix data centres use 100-per-cent renewable energy and that the company aims to be carbon-neutral by 2030. He also said heat from hardware can be repurposed. “The Olympic pool in Paris was heated by Equinix data centres,” he said. Data centre operators face challenges Wholesale colocation data centres are seen as desirable because infrastructure funds are able to deploy large chunks of capital into real estate assets that produce stable income, said Scott Harper, Toronto-based vice-president with CBRE Limited. But operating a data centre is no walk in the park, as it brings legal risk, and the technical complexities of data centres make the market difficult to penetrate. “If there is a problem in a data centre and your client needs an operating platform 24-7 and you cause there to be some type of interruption in that service, the penalties that you’re going to pay for each minute that you’re down are substantial,” he said. “It’s like the difference between owning a retirement home as a real estate play and then owning and operating a hospital that operates 24-7. It’s just much more technical, and you’re having to buy equipment that’s very expensive and back up the data centre twice.” The hardest thing for an operator right now is aligning a facility to the power supply on a client’s timeline, said Dave Cervantes, CBRE’s Montreal-based executive vice-president. There is exponential demand for built data-centre space and large quantities of power, he said. The challenge for an operator is getting that space to market with a scarcity of utility power on the grid and significant capital costs to develop the data centre around that power. Despite these logistical challenges, demand is only beginning to grow, Cervantes said. “We are continuing to develop at a very, very accelerated pace,” he said. According to Scott, there are now more eyes and ears focused on the data centre industry in B.C. and elsewhere. “There’s a ton of investment flowing into this space right now,” he said. “I think it’s really just the beginning of data centres and everything that’s going to be possible.” [email protected] x.com/jamimakan jamimakan.bsky.social","Data centres, which quietly connect the digital world, are emerging as a darling of commercial real estate in B.C. and beyond. Although the data centre industry has complex needs and high barriers to entry, experts say it’s only just blooming, and that the industry will grow exponentially with the development and adoption of artificial intelligence, cloud computing and other technologies. Data centres, initially developed by financial institutions and others to house critical data, offer five key services to a wide range of customers: Space, power, cooling, security and interconnection. Companies used to operate “switch rooms” or “telco rooms” in-house before outsourcing that work. “It’s really exciting that data centres have now come into the mainstream a bit more,” said Jordan Scott, account director with Denver-based Cologix, Inc., which operates five data centres in and around Vancouver. “For a while there, it felt like we were kind of just operating behind a curtain or behind the shadows.” Scott, who is based in Vancouver, said data centres can be thought of as airports. They are the hubs where all the activity happens—where data is processed, transmitted and stored. “In the airports, it’s a multi-tenant environment. Many, many customers come into our facilities and they co-locate their IT infrastructure within our walls,” he said. Data centres are buildings for powering and cooling racks of servers and connecting them with fibre-optic and network cabling. Customers generally bring their own equipment, such as servers, switches, cabling, cabinets and power distribution units (PDUs). Today’s data centres are expected to provide adequate physical space to host client equipment, sufficient electrical power, liquid cooling to create an ideal environment for running equipment, physical security and interconnection with the outside world. Cologix’s 45-plus data centres in North America operate on a lease model. In addition to certain one-time costs, customers pay monthly lease rates for the square footage they occupy and their power usage, and can choose a two-, five- or 10-year term. Although real estate is scarce and expensive in Greater Vancouver, urban hubs are needed for proximity to clients, electrical power and connectivity. “None of our customers are going to want to drive six hours to the middle of nowhere if they have to reboot a server,” Scott said. Data centres must coordinate with utilities and telecommunications companies to access power and fibre networks, and the barriers to entry are significant due to the amount of time and capital involved. “You can’t just set up shop overnight,” he said. AI boosts demand for data centres “Data centres are ultimately real estate businesses,” said Marc Mondesir, Canadian managing director with California-based Equinix, Inc., which bills itself as the world’s largest data-centre company with 470,000-plus physical interconnections available to clients. “We are really in the business of providing space and power for our customers, so that they can house the computer equipment that’s required to power the internet.” Mondesir said his industry is positioned for massive growth as AI, cloud and other technologies—which ultimately runs on computers that in turn run within data centres—evolve. “What’s at the core of all this infrastructure is the network. The network is at the core of the internet. The network is at the core of AI. The network is at the core of large language models,” he said. An AI prompt currently requires 10 to 15 times more computing power compared to a keyword search, he said. This places extra demands on the grid, one reason for BC Hydro’s moratorium on Bitcoin mining announced in December 2022. Crypto volatility aside, Mondesir said there is considerable room for long-term growth in the sector. “Data centres account for less than one per cent of electricity consumption in this country,” he said. “The U.S. hovers in that three- to four-per-cent mark—these numbers change constantly—and then you’ve got China that’s reported to consume even more electricity than that, so [Canada is] sort of skimming the surface.” By concentrating providers and equipment under one roof, data centres can unlock synergies for complex computing tasks that require low latency and high security. “The closer that these network nodes are to one another, the better they perform and the more securely they perform,” he said. As demand for energy grows, so do concerns about the environment. Mondesir said Equinix data centres use 100-per-cent renewable energy and that the company aims to be carbon-neutral by 2030. He also said heat from hardware can be repurposed. “The Olympic pool in Paris was heated by Equinix data centres,” he said. Data centre operators face challenges Wholesale colocation data centres are seen as desirable because infrastructure funds are able to deploy large chunks of capital into real estate assets that produce stable income, said Scott Harper, Toronto-based vice-president with CBRE Limited. But operating a data centre is no walk in the park, as it brings legal risk, and the technical complexities of data centres make the market difficult to penetrate. “If there is a problem in a data centre and your client needs an operating platform 24-7 and you cause there to be some type of interruption in that service, the penalties that you’re going to pay for each minute that you’re down are substantial,” he said. “It’s like the difference between owning a retirement home as a real estate play and then owning and operating a hospital that operates 24-7. It’s just much more technical, and you’re having to buy equipment that’s very expensive and back up the data centre twice.” The hardest thing for an operator right now is aligning a facility to the power supply on a client’s timeline, said Dave Cervantes, CBRE’s Montreal-based executive vice-president. There is exponential demand for built data-centre space and large quantities of power, he said. The challenge for an operator is getting that space to market with a scarcity of utility power on the grid and significant capital costs to develop the data centre around that power. Despite these logistical challenges, demand is only beginning to grow, Cervantes said. “We are continuing to develop at a very, very accelerated pace,” he said. According to Scott, there are now more eyes and ears focused on the data centre industry in B.C. and elsewhere. “There’s a ton of investment flowing into this space right now,” he said. “I think it’s really just the beginning of data centres and everything that’s going to be possible.” [email protected] x.com/jamimakan jamimakan.bsky.social",Data centres reshaping B.C.’s commercial real estate market,"

Key Points:
",Cloud Computing,"While ultimately real estate plays, barriers to entering the data centre game are high
Data centres, which quietly connect the digital world, are emerging as a darling of commercial real estate in B.C. and beyond.
Although the data centre industry ha... [6527 chars]"
Indian IT ‘Should Be Paranoid’ About AI & Ditch its 30-Year-Old Business Model,https://analyticsindiamag.com/it-services/indian-it-should-be-paranoid-about-ai-ditch-its-30-year-old-business-model/,GNews,2025-02-26T14:30:00Z,Analytics India Magazine,https://analyticsindiamag.com/wp-content/uploads/2025/02/Indian-IT-‘Should-Be-Paranoid-About-AI-Ditch-its-30-Year-Old-Business-Model.jpg,"Indian IT giants like TCS, Infosys, HCLTech, and Wiprohave largely stayed away from building foundational AI models. This remained true even after the launch of China’s DeepSeek, which shook up the entire Western world. However, India’s focus has remained on adoption. Now, things might finally change as AI tools are looking to make the future difficult for Indian IT firms, and they must rethink their strategies and invest in indigenous language models to remain competitive. Speaking at an industry event in Mumbai, HCLTech CEOVijayakumar Cemphasised that AI’s disruption in IT services is unlike previous technological shifts such as cloud computing and digital transformation. “The changes AI is assuring are very different, and we need to be more proactive to even categorise our revenues to create completely new businesses,” he said. Generative AI is expected to acceleratesoftware development by automating coding and reducing project timelines. Vijayakumar pointed to a financial services firm where AI-driven efficiencies reduced the timelines of a $1 billion technology transformation program from five years to three-and-a-half years. Highlighting the strategic need for India to build its own language models, he cautioned against over-reliance on foreign AI infrastructure. “We should not assume that these (language) models will continue to be open source. I think these are going to be the coins on which the geopolitics will be played off,” he warned. Vijayakumar stressed that with declining costs of AI training, India must invest in economic ways to develop its own models. “I strongly believe that the business model is ripe for disruption. What we saw in the last 30 years was a fairly linear scaling of revenues and people. I think time is already up for that (business model),” he added. Infosys CEO Salil Parekh echoed the call for agility, urging Indian IT firms to remain proactive in navigating AI-driven transformations. “I think we have to be paranoid. We have to be non-complacent. That is how we can manage to keep up with what’s going on in the industry,” Parekh said. Tanay Pratap, YouTuber and founder and CEO of Invact Metaversity, while speaking toAIMearlier,stressed thatAI coding tools and agents coming up in the market could threaten Indian IT employees. “Thirty years of IT revolution in the country, but we still don’t know how to produce coders at scale,” Pratap said. He added that even when graduates come out of universities with a computer science degree in India and join IT firms, their ability to code still remains questionable. Currently, the biggest exporter in India that contributes to the economy is the IT service companies. However, instead of having coders and programmers, these IT services support testing-related roles. “The whole business model [of Indian IT] is about exporting services like testing to global customers,” Pratap said, adding that this could be under threat. Vijaykumar also believes that a large part of the IT industry is driven by input-based models. “People are delivering certain outcomes. We need to dramatically change the output.” He added that a lot of services that these firms deliver need to become platform-based from people-based. This aligns with K Krithivasan, CEO and MD of TCS, who earlier pointed out that building LLMs has no huge advantage as the cost outweighs the benefit. He added that since most organisations in India are system integrators, companies need to use products as software and ensure that clients receive the benefits. At the same time, he also agreed that building it for regional languages makes sense for democratising the technology. Recently, discussions on a bustling Reddit thread titled ‘I don’t see any hope in the future of this IT industry’ centred around how AI is definitely stronger than humans and will get better with time. Even though the IT firms have enough funds to make a foundational model,they won’t build one unless their clients ask them to or there is some requirement from their side. The whole idea of IT companies not building products might need to change completely if it needs to survive. While TCS, Infosys, Wipro, and HCLTech have started developing agentic AI frameworks,small language models, and even drug discovery, their efforts remain focused on clients alone. These initiatives do not prioritise building foundational technologies for the country. Tech Mahindra built itsProject Indus, the only foundational model emerging from an IT firm in India. Infosys co-founders Nandan Nilekani and Kris Gopalakrishnan are still debating whether they need one. Gopalakrishnan earlier wrote on X that India needs to build its foundation model for a cultural and strategic economy, while Nilekani recently reiterated that a foundational model is unnecessary as long as use cases are built. Meanwhile, several industry experts like Ajai Chowdhry andCP Gurnaniearlier toldAIMthat it is important for Indian firms to build foundational models. It seems like it is finally going to take place with HCLTech’s change of plans. 📣 Want to advertise in AIM?Book here “People went abroad because of better projects and financial incentives, but if you are paid well here, you could visit those countries while still building in India,” said Yashas Karanam,","Indian IT giants like TCS, Infosys, HCLTech, and Wiprohave largely stayed away from building foundational AI models. This remained true even after the launch of China’s DeepSeek, which shook up the entire Western world. However, India’s focus has remained on adoption. Now, things might finally change as AI tools are looking to make the future difficult for Indian IT firms, and they must rethink their strategies and invest in indigenous language models to remain competitive. Speaking at an industry event in Mumbai, HCLTech CEOVijayakumar Cemphasised that AI’s disruption in IT services is unlike previous technological shifts such as cloud computing and digital transformation. “The changes AI is assuring are very different, and we need to be more proactive to even categorise our revenues to create completely new businesses,” he said. Generative AI is expected to acceleratesoftware development by automating coding and reducing project timelines. Vijayakumar pointed to a financial services firm where AI-driven efficiencies reduced the timelines of a $1 billion technology transformation program from five years to three-and-a-half years. Highlighting the strategic need for India to build its own language models, he cautioned against over-reliance on foreign AI infrastructure. “We should not assume that these (language) models will continue to be open source. I think these are going to be the coins on which the geopolitics will be played off,” he warned. Vijayakumar stressed that with declining costs of AI training, India must invest in economic ways to develop its own models. “I strongly believe that the business model is ripe for disruption. What we saw in the last 30 years was a fairly linear scaling of revenues and people. I think time is already up for that (business model),” he added. Infosys CEO Salil Parekh echoed the call for agility, urging Indian IT firms to remain proactive in navigating AI-driven transformations. “I think we have to be paranoid. We have to be non-complacent. That is how we can manage to keep up with what’s going on in the industry,” Parekh said. Tanay Pratap, YouTuber and founder and CEO of Invact Metaversity, while speaking toAIMearlier,stressed thatAI coding tools and agents coming up in the market could threaten Indian IT employees. “Thirty years of IT revolution in the country, but we still don’t know how to produce coders at scale,” Pratap said. He added that even when graduates come out of universities with a computer science degree in India and join IT firms, their ability to code still remains questionable. Currently, the biggest exporter in India that contributes to the economy is the IT service companies. However, instead of having coders and programmers, these IT services support testing-related roles. “The whole business model [of Indian IT] is about exporting services like testing to global customers,” Pratap said, adding that this could be under threat. Vijaykumar also believes that a large part of the IT industry is driven by input-based models. “People are delivering certain outcomes. We need to dramatically change the output.” He added that a lot of services that these firms deliver need to become platform-based from people-based. This aligns with K Krithivasan, CEO and MD of TCS, who earlier pointed out that building LLMs has no huge advantage as the cost outweighs the benefit. He added that since most organisations in India are system integrators, companies need to use products as software and ensure that clients receive the benefits. At the same time, he also agreed that building it for regional languages makes sense for democratising the technology. Recently, discussions on a bustling Reddit thread titled ‘I don’t see any hope in the future of this IT industry’ centred around how AI is definitely stronger than humans and will get better with time. Even though the IT firms have enough funds to make a foundational model,they won’t build one unless their clients ask them to or there is some requirement from their side. The whole idea of IT companies not building products might need to change completely if it needs to survive. While TCS, Infosys, Wipro, and HCLTech have started developing agentic AI frameworks,small language models, and even drug discovery, their efforts remain focused on clients alone. These initiatives do not prioritise building foundational technologies for the country. Tech Mahindra built itsProject Indus, the only foundational model emerging from an IT firm in India. Infosys co-founders Nandan Nilekani and Kris Gopalakrishnan are still debating whether they need one. Gopalakrishnan earlier wrote on X that India needs to build its foundation model for a cultural and strategic economy, while Nilekani recently reiterated that a foundational model is unnecessary as long as use cases are built. Meanwhile, several industry experts like Ajai Chowdhry andCP Gurnaniearlier toldAIMthat it is important for Indian firms to build foundational models. It seems like it is finally going to take place with HCLTech’s change of plans. 📣 Want to advertise in AIM?Book here “People went abroad because of better projects and financial incentives, but if you are paid well here, you could visit those countries while still building in India,” said Yashas Karanam,",Indian IT ‘Should Be Paranoid’ About AI & Ditch its 30-Year-Old Business Model,"

Key Points:
",Cloud Computing,"Indian IT giants like TCS, Infosys, HCLTech, and Wipro have largely stayed away from building foundational AI models. This remained true even after the launch of China’s DeepSeek, which shook up the entire Western world. However, India’s focus has re... [4877 chars]"
Trott defends Afghanistan’s men with cloud hanging over England game,https://www.theguardian.com/sport/2025/feb/25/afghanistan-england-cricket-champions-trophy-jonathan-trott,The Guardian,2025-02-25T19:00:01Z,The Guardian,https://media.guim.co.uk/e32f7d6181429cae3f09d6caff0c03e6e208eab5/0_320_6000_3600/500.jpg,"Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA View image in fullscreen Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA ICC Champions Trophy Trott defends Afghanistan’s men with cloud hanging over England game Taliban’s ban on women playing cricket has led to calls for a boycott by the ECB, but Champions Trophy match proceeds Ali Martin in Lahore Tue 25 Feb 2025 19.00 GMT Last modified on Tue 25 Feb 2025 19.27 GMT Share A day out from what is a must-win game for both Afghanistan and England, Jonathan Trott, head coach of the former, addressed the issue that is impossible to ignore during this Champions Trophy. The Taliban continues its brutal clampdown on women’s rights back in Afghanistan and Trott outlined the position his players find themselves in. “They know the difference between right and wrong,” said Trott, drawing a distinction between a team that play their cricket in exile under the previous Afghan flag and the regime at home. “They are under no illusions about where they have come from and they’re very courageous. You see that in their cricket and that’s the one thing I’ve not tried to change at all. They know who they are playing for and representing.” Jos Buttler admits stakes high for England and his captaincy against Afghanistan Read more It is not the first time England have met Afghanistan since western forces withdrew from the country in 2021, creating a power vacuum into which the Taliban returned. In the most recent, during the 2023 World Cup, Jos Buttler’s men were spun out for a 69-run humbling in Delhi. Rashid Khan and Mohammad Nabi, who have since publicly denounced the country’s ban on medical education for women, were the match‑winners. As things worsen in the country, as basic human rights for women and girls continue to be stripped away, calls for cricket to expel Afghanistan have grown. After all, the Afghanistan Cricket Board (ACB) enjoys the perks of being a full member of the International Cricket Council (ICC) but is also barred from making any provision for the women’s game by the Taliban. Back in the UK, this culminated in nearly 200 MPs, led by Labour’s Tonia Antoniazzi, writing to the England and Wales Cricket Board on 7 January to ask it to consider boycotting this Champions Trophy fixture. The ECB resisted, citing the need for collective action. They were backed by the British government, which called on the ICC to enforce its own rules. But due to the geopolitics at play here, the notion of the sport penalising the ACB appears to be a depressing nonstarter. On Tuesday afternoon, on the eve of the Champions Trophy match, protestors were due to gather at Lord’s in the face of the inaction. Just a day after that letter was sent to the ECB a far more significant development took place in Dubai when India’s foreign minister, Vikram Misri, met with his Afghan equivalent, Amir Khan Muttaqi, for talks; the first tentative discussions in what, according to recent reports , could soon mean a Taliban-run Afghan embassy restored in New Delhi. India, with China hovering, is looking to deepen its influence in the region. Those talks reportedly revolved around humanitarian assistance and future development projects, as war-torn Afghanistan attempts to rebuild its infrastructure. But “ways to strengthen cooperation in sports, particularly cricket” were also on the agenda. Along with some recent, awkward photo opportunities with the team, this rather undermines the notion that the Taliban cares little about its men’s national team. View image in fullscreen Jos Buttler (right) and Tom Banton take a breather during an England net session in Lahore. Photograph: Matthew Lewis-ICC/ICC/Getty Images The ICC’s position ultimately depends on that of India. As the cameras kept reminding us when panning to the VIP box during India’s win against Pakistan in Dubai on Sunday, the ICC’s notionally independent chair is Jay Shah, son of India’s home minister, Amit Shah. He took the role in December, fresh from six years running a board that is heavily intertwined with the ruling Bharatiya Janata Party by way of personnel and outlook. One of the things said about the ACB during all this – including by the British prime minister, Sir Keir Starmer – is that by not running a women’s programme, it is in breach of its obligations as an ICC full member. The ECB is understood to have sought clarification from the ICC’s legal team, with sources suggesting the issue has exposed more of a grey area in governance than may appear from the outside. The ICC’s criteria do state that in order to become a full member, a country must have an established women’s cricket programme, with the ACB granted dispensation to step up in 2017 on the proviso it would address this shortcoming in time. But now it is part of the club – and unlike for those countries with associate status – sources suggest there is no mechanism by which this can be monitored, or any action taken. skip past newsletter promotion Sign up to The Spin Free weekly newsletter Subscribe to our cricket newsletter for our writers' thoughts on the biggest stories and a review of the week’s action Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion As two of the boards to suspend bilateral fixtures against Afghanistan, the ECB and Cricket Australia are also lobbying to see a portion of Afghan’s ICC disbursements – worth $17m (£13.4m) per year overall – moved into an escrow account until such time as the situation in the country improves. It would still be the ACB’s money, just on hold. Another proposal is to divert part of the ICC’s development fund towards the women’s players who fled the country – the majority to Australia – and train female coaches and administrators. The ECB, when confirming this week’s match would go ahead, also called for the ICC to consider recognising an Afghanistan Women’s Refugee Team. But so far, beyond CA’s support of the Afghan women who made it to Australia and the ECB donating £100,000 to a refugee cricketer fund set up by the MCC Foundation, the situation appears to be going nowhere. These are small steps. Indeed, only last year, when Shah was chair of the Asian Cricket Council that distributes revenues from the biennial Asia Cup, Afghanistan’s split of that particular pot was increased . Given the men’s team is based in the United Arab Emirates – Trott has never set foot in Afghanistan – one of the arguments put forward by their allies at the top table is that their cost base is already extraordinarily high. The cynics in the room also suspect there are full members who do not want scrutiny of their own women’s affairs; that to penalise Afghanistan would be to set what they view as a dangerous precedent. “Every time we play the Afghanistan men’s team this issue will keep coming up,” said Richard Gould, the ECB chief executive who is currently in Lahore. “And that has got to be better than the alternative, where they are pushed out, cricket dies a death in Afghanistan, and we never have to worry about it again because it’s not in our field.” From playing in refugee camps to competing with the world’s best, the rise of Afghanistan’s men is one of cricket’s great stories. But right now, with half of its population barred from doing the same, the sport’s shortcomings are being exposed. Explore more on these topics ICC Champions Trophy Afghanistan cricket team England cricket team Cricket features Share Reuse this content","Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA View image in fullscreen Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA ICC Champions Trophy Trott defends Afghanistan’s men with cloud hanging over England game Taliban’s ban on women playing cricket has led to calls for a boycott by the ECB, but Champions Trophy match proceeds Ali Martin in Lahore Tue 25 Feb 2025 19.00 GMT Last modified on Tue 25 Feb 2025 19.27 GMT Share A day out from what is a must-win game for both Afghanistan and England, Jonathan Trott, head coach of the former, addressed the issue that is impossible to ignore during this Champions Trophy. The Taliban continues its brutal clampdown on women’s rights back in Afghanistan and Trott outlined the position his players find themselves in. “They know the difference between right and wrong,” said Trott, drawing a distinction between a team that play their cricket in exile under the previous Afghan flag and the regime at home. “They are under no illusions about where they have come from and they’re very courageous. You see that in their cricket and that’s the one thing I’ve not tried to change at all. They know who they are playing for and representing.” Jos Buttler admits stakes high for England and his captaincy against Afghanistan Read more It is not the first time England have met Afghanistan since western forces withdrew from the country in 2021, creating a power vacuum into which the Taliban returned. In the most recent, during the 2023 World Cup, Jos Buttler’s men were spun out for a 69-run humbling in Delhi. Rashid Khan and Mohammad Nabi, who have since publicly denounced the country’s ban on medical education for women, were the match‑winners. As things worsen in the country, as basic human rights for women and girls continue to be stripped away, calls for cricket to expel Afghanistan have grown. After all, the Afghanistan Cricket Board (ACB) enjoys the perks of being a full member of the International Cricket Council (ICC) but is also barred from making any provision for the women’s game by the Taliban. Back in the UK, this culminated in nearly 200 MPs, led by Labour’s Tonia Antoniazzi, writing to the England and Wales Cricket Board on 7 January to ask it to consider boycotting this Champions Trophy fixture. The ECB resisted, citing the need for collective action. They were backed by the British government, which called on the ICC to enforce its own rules. But due to the geopolitics at play here, the notion of the sport penalising the ACB appears to be a depressing nonstarter. On Tuesday afternoon, on the eve of the Champions Trophy match, protestors were due to gather at Lord’s in the face of the inaction. Just a day after that letter was sent to the ECB a far more significant development took place in Dubai when India’s foreign minister, Vikram Misri, met with his Afghan equivalent, Amir Khan Muttaqi, for talks; the first tentative discussions in what, according to recent reports , could soon mean a Taliban-run Afghan embassy restored in New Delhi. India, with China hovering, is looking to deepen its influence in the region. Those talks reportedly revolved around humanitarian assistance and future development projects, as war-torn Afghanistan attempts to rebuild its infrastructure. But “ways to strengthen cooperation in sports, particularly cricket” were also on the agenda. Along with some recent, awkward photo opportunities with the team, this rather undermines the notion that the Taliban cares little about its men’s national team. View image in fullscreen Jos Buttler (right) and Tom Banton take a breather during an England net session in Lahore. Photograph: Matthew Lewis-ICC/ICC/Getty Images The ICC’s position ultimately depends on that of India. As the cameras kept reminding us when panning to the VIP box during India’s win against Pakistan in Dubai on Sunday, the ICC’s notionally independent chair is Jay Shah, son of India’s home minister, Amit Shah. He took the role in December, fresh from six years running a board that is heavily intertwined with the ruling Bharatiya Janata Party by way of personnel and outlook. One of the things said about the ACB during all this – including by the British prime minister, Sir Keir Starmer – is that by not running a women’s programme, it is in breach of its obligations as an ICC full member. The ECB is understood to have sought clarification from the ICC’s legal team, with sources suggesting the issue has exposed more of a grey area in governance than may appear from the outside. The ICC’s criteria do state that in order to become a full member, a country must have an established women’s cricket programme, with the ACB granted dispensation to step up in 2017 on the proviso it would address this shortcoming in time. But now it is part of the club – and unlike for those countries with associate status – sources suggest there is no mechanism by which this can be monitored, or any action taken. skip past newsletter promotion Sign up to The Spin Free weekly newsletter Subscribe to our cricket newsletter for our writers' thoughts on the biggest stories and a review of the week’s action Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion As two of the boards to suspend bilateral fixtures against Afghanistan, the ECB and Cricket Australia are also lobbying to see a portion of Afghan’s ICC disbursements – worth $17m (£13.4m) per year overall – moved into an escrow account until such time as the situation in the country improves. It would still be the ACB’s money, just on hold. Another proposal is to divert part of the ICC’s development fund towards the women’s players who fled the country – the majority to Australia – and train female coaches and administrators. The ECB, when confirming this week’s match would go ahead, also called for the ICC to consider recognising an Afghanistan Women’s Refugee Team. But so far, beyond CA’s support of the Afghan women who made it to Australia and the ECB donating £100,000 to a refugee cricketer fund set up by the MCC Foundation, the situation appears to be going nowhere. These are small steps. Indeed, only last year, when Shah was chair of the Asian Cricket Council that distributes revenues from the biennial Asia Cup, Afghanistan’s split of that particular pot was increased . Given the men’s team is based in the United Arab Emirates – Trott has never set foot in Afghanistan – one of the arguments put forward by their allies at the top table is that their cost base is already extraordinarily high. The cynics in the room also suspect there are full members who do not want scrutiny of their own women’s affairs; that to penalise Afghanistan would be to set what they view as a dangerous precedent. “Every time we play the Afghanistan men’s team this issue will keep coming up,” said Richard Gould, the ECB chief executive who is currently in Lahore. “And that has got to be better than the alternative, where they are pushed out, cricket dies a death in Afghanistan, and we never have to worry about it again because it’s not in our field.” From playing in refugee camps to competing with the world’s best, the rise of Afghanistan’s men is one of cricket’s great stories. But right now, with half of its population barred from doing the same, the sport’s shortcomings are being exposed. Explore more on these topics ICC Champions Trophy Afghanistan cricket team England cricket team Cricket features Share Reuse this content",Trott defends Afghanistan’s men with cloud hanging over England game,"

Key Points:
",Cloud Computing,"Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA View image in fullscreen Protesters outside Lord's in London on Tuesday, demonstrating against the Afghanistan v England match. Photograph: Lucy North/PA ICC Champions Trophy Trott defends Afghanistan’s men with cloud hanging over England game Taliban’s ban on women playing cricket has led to calls for a boycott by the ECB, but Champions Trophy match proceeds Ali Martin in Lahore Tue 25 Feb 2025 19.00 GMT Last modified on Tue 25 Feb 2025 19.27 GMT Share A day out from what is a must-win game for both Afghanistan and England, Jonathan Trott, head coach of the former, addressed the issue that is impossible to ignore during this Champions Trophy. The Taliban continues its brutal clampdown on women’s rights back in Afghanistan and Trott outlined the position his players find themselves in. “They know the difference between right and wrong,” said Trott, drawing a distinction between a team that play their cricket in exile under the previous Afghan flag and the regime at home. “They are under no illusions about where they have come from and they’re very courageous. You see that in their cricket and that’s the one thing I’ve not tried to change at all. They know who they are playing for and representing.” Jos Buttler admits stakes high for England and his captaincy against Afghanistan Read more It is not the first time England have met Afghanistan since western forces withdrew from the country in 2021, creating a power vacuum into which the Taliban returned. In the most recent, during the 2023 World Cup, Jos Buttler’s men were spun out for a 69-run humbling in Delhi. Rashid Khan and Mohammad Nabi, who have since publicly denounced the country’s ban on medical education for women, were the match‑winners. As things worsen in the country, as basic human rights for women and girls continue to be stripped away, calls for cricket to expel Afghanistan have grown. After all, the Afghanistan Cricket Board (ACB) enjoys the perks of being a full member of the International Cricket Council (ICC) but is also barred from making any provision for the women’s game by the Taliban. Back in the UK, this culminated in nearly 200 MPs, led by Labour’s Tonia Antoniazzi, writing to the England and Wales Cricket Board on 7 January to ask it to consider boycotting this Champions Trophy fixture. The ECB resisted, citing the need for collective action. They were backed by the British government, which called on the ICC to enforce its own rules. But due to the geopolitics at play here, the notion of the sport penalising the ACB appears to be a depressing nonstarter. On Tuesday afternoon, on the eve of the Champions Trophy match, protestors were due to gather at Lord’s in the face of the inaction. Just a day after that letter was sent to the ECB a far more significant development took place in Dubai when India’s foreign minister, Vikram Misri, met with his Afghan equivalent, Amir Khan Muttaqi, for talks; the first tentative discussions in what, according to recent reports , could soon mean a Taliban-run Afghan embassy restored in New Delhi. India, with China hovering, is looking to deepen its influence in the region. Those talks reportedly revolved around humanitarian assistance and future development projects, as war-torn Afghanistan attempts to rebuild its infrastructure. But “ways to strengthen cooperation in sports, particularly cricket” were also on the agenda. Along with some recent, awkward photo opportunities with the team, this rather undermines the notion that the Taliban cares little about its men’s national team. View image in fullscreen Jos Buttler (right) and Tom Banton take a breather during an England net session in Lahore. Photograph: Matthew Lewis-ICC/ICC/Getty Images The ICC’s position ultimately depends on that of India. As the cameras kept reminding us when panning to the VIP box during India’s win against Pakistan in Dubai on Sunday, the ICC’s notionally independent chair is Jay Shah, son of India’s home minister, Amit Shah. He took the role in December, fresh from six years running a board that is heavily intertwined with the ruling Bharatiya Janata Party by way of personnel and outlook. One of the things said about the ACB during all this – including by the British prime minister, Sir Keir Starmer – is that by not running a women’s programme, it is in breach of its obligations as an ICC full member. The ECB is understood to have sought clarification from the ICC’s legal team, with sources suggesting the issue has exposed more of a grey area in governance than may appear from the outside. The ICC’s criteria do state that in order to become a full member, a country must have an established women’s cricket programme, with the ACB granted dispensation to step up in 2017 on the proviso it would address this shortcoming in time. But now it is part of the club – and unlike for those countries with associate status – sources suggest there is no mechanism by which this can be monitored, or any action taken. skip past newsletter promotion Sign up to The Spin Free weekly newsletter Subscribe to our cricket newsletter for our writers' thoughts on the biggest stories and a review of the week’s action Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion As two of the boards to suspend bilateral fixtures against Afghanistan, the ECB and Cricket Australia are also lobbying to see a portion of Afghan’s ICC disbursements – worth $17m (£13.4m) per year overall – moved into an escrow account until such time as the situation in the country improves. It would still be the ACB’s money, just on hold. Another proposal is to divert part of the ICC’s development fund towards the women’s players who fled the country – the majority to Australia – and train female coaches and administrators. The ECB, when confirming this week’s match would go ahead, also called for the ICC to consider recognising an Afghanistan Women’s Refugee Team. But so far, beyond CA’s support of the Afghan women who made it to Australia and the ECB donating £100,000 to a refugee cricketer fund set up by the MCC Foundation, the situation appears to be going nowhere. These are small steps. Indeed, only last year, when Shah was chair of the Asian Cricket Council that distributes revenues from the biennial Asia Cup, Afghanistan’s split of that particular pot was increased . Given the men’s team is based in the United Arab Emirates – Trott has never set foot in Afghanistan – one of the arguments put forward by their allies at the top table is that their cost base is already extraordinarily high. The cynics in the room also suspect there are full members who do not want scrutiny of their own women’s affairs; that to penalise Afghanistan would be to set what they view as a dangerous precedent. “Every time we play the Afghanistan men’s team this issue will keep coming up,” said Richard Gould, the ECB chief executive who is currently in Lahore. “And that has got to be better than the alternative, where they are pushed out, cricket dies a death in Afghanistan, and we never have to worry about it again because it’s not in our field.” From playing in refugee camps to competing with the world’s best, the rise of Afghanistan’s men is one of cricket’s great stories. But right now, with half of its population barred from doing the same, the sport’s shortcomings are being exposed. Explore more on these topics ICC Champions Trophy Afghanistan cricket team England cricket team Cricket features Share Reuse this content"
"Trump Criticizes UK's Data Access Request, Comparing It to China's Approach",https://www.devdiscourse.com/article/politics/3282172-trump-criticizes-uks-data-access-request-comparing-it-to-chinas-approach,GNews,2025-02-28T22:25:55Z,Devdiscourse,https://www.devdiscourse.com/remote.axd?https://devdiscourse.blob.core.windows.net/imagegallery/27_05_2019_12_11_15_3417518.jpg?width=920&format=jpeg,"Trump Criticizes UK's Data Access Request, Comparing It to China's Approach U.S. President Donald Trump criticized the UK government's data access request from Apple, comparing it to China's practices. During talks with UK Prime Minister Keir Starmer, Trump opposed the request. The Spectator published the interview amid ongoing discussions about cybersecurity and international intelligence cooperation. Devdiscourse News Desk | Updated: 01-03-2025 03:55 IST | Created: 01-03-2025 03:55 IST SHARE U.S. President Donald Trump expressed strong criticism towards the UK government's request for Apple to provide access to some user data, likening the demand to practices typically associated with China. The remarks were made during an interview with The Spectator, published on Friday. Trump recounted a conversation with British Prime Minister Keir Starmer, in which he opposed the data access request. The leaders met at the White House for discussions that also included topics like the situation in Ukraine and a potential bilateral trade deal. Trump's comments were made during his first magazine interview of his second term, shared with The Spectator's editor-at-large Ben Domenech. While the British government reiterated its commitment to a strong intelligence partnership with the U.S., it refrained from commenting on the specifics of the case involving Apple. The technology giant had already discontinued an advanced security encryption feature for UK users, reflecting government pressures. Meanwhile, U.S. Director of National Intelligence Tulsi Gabbard is investigating whether the UK's actions may have breached the CLOUD Act, highlighting concerns over international data privacy standards. (With inputs from agencies.) READ MORE ON: Trump Starmer UK government Apple data access China interview The Spectator CLOUD Act cybersecurity Advertisement ALSO READ China Extends a Hand to Spanish Enterprises for Joint Growth China's Record Haul at Asian Winter Games Amid Calls for Improvement Philippines Fortifies Defense Alliances Amid Rising South China Sea Tensions China Advocates for Europe's Role in Ukraine Peace Talks US-India F35 Fighter Deal Draws Criticism from China, Pakistan POST / READ COMMENTS","Trump Criticizes UK's Data Access Request, Comparing It to China's Approach U.S. President Donald Trump criticized the UK government's data access request from Apple, comparing it to China's practices. During talks with UK Prime Minister Keir Starmer, Trump opposed the request. The Spectator published the interview amid ongoing discussions about cybersecurity and international intelligence cooperation. Devdiscourse News Desk | Updated: 01-03-2025 03:55 IST | Created: 01-03-2025 03:55 IST SHARE U.S. President Donald Trump expressed strong criticism towards the UK government's request for Apple to provide access to some user data, likening the demand to practices typically associated with China. The remarks were made during an interview with The Spectator, published on Friday. Trump recounted a conversation with British Prime Minister Keir Starmer, in which he opposed the data access request. The leaders met at the White House for discussions that also included topics like the situation in Ukraine and a potential bilateral trade deal. Trump's comments were made during his first magazine interview of his second term, shared with The Spectator's editor-at-large Ben Domenech. While the British government reiterated its commitment to a strong intelligence partnership with the U.S., it refrained from commenting on the specifics of the case involving Apple. The technology giant had already discontinued an advanced security encryption feature for UK users, reflecting government pressures. Meanwhile, U.S. Director of National Intelligence Tulsi Gabbard is investigating whether the UK's actions may have breached the CLOUD Act, highlighting concerns over international data privacy standards. (With inputs from agencies.) READ MORE ON: Trump Starmer UK government Apple data access China interview The Spectator CLOUD Act cybersecurity Advertisement ALSO READ China Extends a Hand to Spanish Enterprises for Joint Growth China's Record Haul at Asian Winter Games Amid Calls for Improvement Philippines Fortifies Defense Alliances Amid Rising South China Sea Tensions China Advocates for Europe's Role in Ukraine Peace Talks US-India F35 Fighter Deal Draws Criticism from China, Pakistan POST / READ COMMENTS","Trump Criticizes UK's Data Access Request, Comparing It to China's Approach","

Key Points:
",Cybersecurity,"U.S. President Donald Trump expressed strong criticism towards the UK government's request for Apple to provide access to some user data, likening the demand to practices typically associated with China. The remarks were made during an interview with... [988 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.vancouverisawesome.com/national-business/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10304790,GNews,2025-02-28T20:10:46Z,Vancouver Is Awesome,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
QUÉBEC — Quebec Premier François Legault ... [3119 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.guelphtoday.com/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303311,GNews,2025-02-28T18:11:43Z,GuelphToday,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
Premier François Legault today named backbencher Gilles Bél... [914 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.vancouverisawesome.com/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303316,GNews,2025-02-28T18:11:43Z,Vancouver Is Awesome,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
QUÉBEC — Quebec's premier has replaced his cybersecurity an... [1105 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.sootoday.com/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303311,GNews,2025-02-28T18:11:43Z,SooToday,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
Premier François Legault today named backbencher Gilles Bél... [914 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.elliotlaketoday.com/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303311,GNews,2025-02-28T18:11:43Z,ElliotLakeToday.com,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
Premier François Legault today named backbencher Gilles Bél... [914 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.bradfordtoday.ca/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303311,GNews,2025-02-28T18:11:43Z,BradfordToday,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
Premier François Legault today named backbencher Gilles Bél... [914 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.baytoday.ca/national-news/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal-10303311,GNews,2025-02-28T18:11:43Z,BayToday,https://www.vmcdn.ca/f/files/shared/feeds/cp/2025/02/6001a34e26ca2b8966be83af87094a59c12a04efdbe83eb3299a81a9e4bf63b3.jpg;w=1200;h=800;mode=crop,"QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press","QUÉBEC — Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire's replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ""He's a seasoned businessman,"" Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ""(Legault) is giving me a lot of responsibility … it's really exciting … there are a number files related to information technology, so I'm going to start by familiarizing myself with those files."" Caire resigned on Thursday, saying he had ""nothing to reproach myself for, apart from not having been suspicious enough,"" but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid ""media and political risk."" The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire's decision to resign ""honourable"" and said his government would ""get to the bottom of it."" ""We've had a report from the auditor general saying that (the auto insurance board) executives didn't inform the ministers. There are still all kinds of questions. I'm going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,"" he said. However, opposition parties weren't quelled by the resignation, and called for a public inquiry into how the platform's cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform's problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its ""disastrous deployment"" of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Caroline Plante, The Canadian Press",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"QUÉBEC — Quebec's premier has replaced his cybersecurity and digital technology minister after a scandal with the auto insurance board's online platform forced the resignation of Éric Caire.
Premier François Legault today named backbencher Gilles Bél... [914 chars]"
Quebec replaces digital technology minister who resigned over auto board scandal,https://www.thestar.com/news/canada/quebec/quebec-replaces-digital-technology-minister-who-resigned-over-auto-board-scandal/article_b8e83cec-6f25-50da-934e-2742ac155174.html,GNews,2025-02-28T18:11:00Z,Toronto Star,https://bloximages.chicago2.vip.townnews.com/thestar.com/content/tncms/assets/v3/editorial/b/32/b3248fa0-f606-11ef-85c3-df8aca3007e0/67c20882b8262.image.jpg?crop=1400%2C735%2C0%2C136&resize=1200%2C630&order=crop%2Cresize,"Home News Canada Quebec Quebec Quebec replaces digital technology minister who resigned over auto board scandal QUÉBEC - Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board’s online platform forced the resignation of Éric Caire. Updated Feb. 28, 2025 at 3:15 p.m. Feb. 28, 2025 Feb. 28, 2025 2 min read Save Copy article link Email Share on X Share on LinkedIn Share on Reddit Share on Whatsapp Quebec Minister of Cybersecurity and Digital Technology Gilles Bélanger speaks after he was sworn into cabinet on Friday, Feb. 28, 2025. THE CANADIAN PRESS/Jacques Boissinot jqb flag wire: true flag sponsored: false article_type: pubinfo.section: cms.site.custom.site_domain : thestar.com sWebsitePrimaryPublication : publications/toronto_star bHasMigratedAvatar : false firstAuthor.avatar : By Caroline Plante The Canadian Press QUÉBEC - Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board’s online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire’s replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ARTICLE CONTINUES BELOW “He’s a seasoned businessman,” Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ”(Legault) is giving me a lot of responsibility … it’s really exciting … there are a number files related to information technology, so I’m going to start by familiarizing myself with those files.” Caire resigned on Thursday, saying he had “nothing to reproach myself for, apart from not having been suspicious enough,” but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid “media and political risk.” The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire’s decision to resign “honourable” and said his government would “get to the bottom of it.” ARTICLE CONTINUES BELOW ARTICLE CONTINUES BELOW “We’ve had a report from the auditor general saying that (the auto insurance board) executives didn’t inform the ministers. There are still all kinds of questions. I’m going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,” he said. However, opposition parties weren’t quelled by the resignation, and called for a public inquiry into how the platform’s cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform’s problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its “disastrous deployment” of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Related Stories Quebec minister Éric Caire resigns over auto board scandal Report an error Journalistic Standards About The Star Trending Real Estate The Toronto condo market is in dire straits as investors jump ship. But developers have found a silver lining 16 hrs ago Comments Contributors Opinion Justin Ling: Elon Musk isn’t the Trump adviser Canadians should fear the most. This man is 9 hrs ago Comments Federal Politics Mark Carney’s been branded a ‘globalist’ — and he’s just fine with that. Inside the world of the man who could be Canada’s next prime minister 4 hrs ago Comments Contributors Opinion I’m the co-author of Wayne Gretzky’s book. Let’s all leave this great Canadian alone 15 hrs ago Comments Movies Opinion Briony Smith: Worst Oscar winners ever: our staffers share their most hated Academy Award wins 7 hrs ago Comments United States Trump’s Oval Office thrashing of Zelenskyy shows limits of Western allies’ ability to sway US leader 5 hrs ago Business Opinion David Olive: Canada’s tech leaders are condemning efforts to erase DEI — and that very Canadian response will help their long-term success 10 hrs ago Comments Gta Was that thunder? Environment Canada confirms rare winter lightning — or ‘thundersnow’ — in Toronto Friday 13 hrs ago Comments JOIN THE CONVERSATION To join the conversation set a first and last name in your user profile. Update Profile Conversations are opinions of our readers and are subject to the Community Guidelines . Toronto Star does not endorse these opinions. Sign in or register for free to join the Conversation Sign In Register More from The Star & partners","Home News Canada Quebec Quebec Quebec replaces digital technology minister who resigned over auto board scandal QUÉBEC - Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board’s online platform forced the resignation of Éric Caire. Updated Feb. 28, 2025 at 3:15 p.m. Feb. 28, 2025 Feb. 28, 2025 2 min read Save Copy article link Email Share on X Share on LinkedIn Share on Reddit Share on Whatsapp Quebec Minister of Cybersecurity and Digital Technology Gilles Bélanger speaks after he was sworn into cabinet on Friday, Feb. 28, 2025. THE CANADIAN PRESS/Jacques Boissinot jqb flag wire: true flag sponsored: false article_type: pubinfo.section: cms.site.custom.site_domain : thestar.com sWebsitePrimaryPublication : publications/toronto_star bHasMigratedAvatar : false firstAuthor.avatar : By Caroline Plante The Canadian Press QUÉBEC - Quebec Premier François Legault replaced his cybersecurity and digital technology minister Friday after a scandal with the auto insurance board’s online platform forced the resignation of Éric Caire. Legault named backbencher Gilles Bélanger as Caire’s replacement, after an explosive report by the province’s auditor general last week revealed cost overruns of at least $500 million in the creation of the online platform known as SAAQclic. Bélanger, first elected in the Orford riding in 2018, has been legislature assistant to Legault and to Finance Minister Eric Girard. Notably, after the Coalition Avenir Québec won its first mandate in 2018, Bélanger was responsible for the deployment of high-speed internet in rural areas. ARTICLE CONTINUES BELOW “He’s a seasoned businessman,” Legault told reporters Friday after Bélanger was sworn in at the office of Lt-Gov. Manon Jeannotte. Bélanger said he was eager to take on the new role. ”(Legault) is giving me a lot of responsibility … it’s really exciting … there are a number files related to information technology, so I’m going to start by familiarizing myself with those files.” Caire resigned on Thursday, saying he had “nothing to reproach myself for, apart from not having been suspicious enough,” but he also concluded that he had become a distraction to the government. While Caire maintains that he had been kept in the dark about the cost overruns, news reports Thursday alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. The auditor general found that the auto insurance board chose to mask $222 million in cost overruns to avoid “media and political risk.” The botched 2023 rollout of the online platform led to major delays and long lineups at insurance board branches, where Quebecers take road tests, register vehicles, and access other services. Legault called Caire’s decision to resign “honourable” and said his government would “get to the bottom of it.” ARTICLE CONTINUES BELOW ARTICLE CONTINUES BELOW “We’ve had a report from the auditor general saying that (the auto insurance board) executives didn’t inform the ministers. There are still all kinds of questions. I’m going to be very clear with Quebecers: there will be zero tolerance. I want to know everything that happened in this case,” he said. However, opposition parties weren’t quelled by the resignation, and called for a public inquiry into how the platform’s cost ballooned to more than $1.1 billion. The opposition has also said the debacle raises questions about Transport Minister Geneviève Guilbault and her predecessor François Bonnardel, now public security minister. Both held the transport portfolio during the period of cost overruns, and both have said they were unaware off the extent of the online platform’s problems and that information was kept from them. Parti Québécois Leader Paul St-Pierre Plamondon told reporters in Montreal Friday that corruption may have played a part in the exorbitant costs and that the government was likely being overcharged in its “disastrous deployment” of the platform. This report by The Canadian Press was first published Feb. 28, 2025. Related Stories Quebec minister Éric Caire resigns over auto board scandal Report an error Journalistic Standards About The Star Trending Real Estate The Toronto condo market is in dire straits as investors jump ship. But developers have found a silver lining 16 hrs ago Comments Contributors Opinion Justin Ling: Elon Musk isn’t the Trump adviser Canadians should fear the most. This man is 9 hrs ago Comments Federal Politics Mark Carney’s been branded a ‘globalist’ — and he’s just fine with that. Inside the world of the man who could be Canada’s next prime minister 4 hrs ago Comments Contributors Opinion I’m the co-author of Wayne Gretzky’s book. Let’s all leave this great Canadian alone 15 hrs ago Comments Movies Opinion Briony Smith: Worst Oscar winners ever: our staffers share their most hated Academy Award wins 7 hrs ago Comments United States Trump’s Oval Office thrashing of Zelenskyy shows limits of Western allies’ ability to sway US leader 5 hrs ago Business Opinion David Olive: Canada’s tech leaders are condemning efforts to erase DEI — and that very Canadian response will help their long-term success 10 hrs ago Comments Gta Was that thunder? Environment Canada confirms rare winter lightning — or ‘thundersnow’ — in Toronto Friday 13 hrs ago Comments JOIN THE CONVERSATION To join the conversation set a first and last name in your user profile. Update Profile Conversations are opinions of our readers and are subject to the Community Guidelines . Toronto Star does not endorse these opinions. Sign in or register for free to join the Conversation Sign In Register More from The Star & partners",Quebec replaces digital technology minister who resigned over auto board scandal,"

Key Points:
",Cybersecurity,"Country
United States of America US Virgin Islands United States Minor Outlying Islands Canada Mexico, United Mexican States Bahamas, Commonwealth of the Cuba, Republic of Dominican Republic Haiti, Republic of Jamaica Afghanistan Albania, People's So... [4996 chars]"
Quebec minister Éric Caire resigns over auto board scandal,https://www.theglobeandmail.com/canada/article-quebec-minister-eric-caire-resigns-over-auto-board-scandal/,GNews,2025-02-27T19:56:16Z,The Globe and Mail,https://www.theglobeandmail.com/resizer/v2/RICZHQUYQZFA7EYDZOF2Z7W5CU.jpg?auth=efca964ace035fbedfec437017194e697294f0ee95adf8f8939f7b3343fe0ccb&width=1200&height=800&quality=80&smart=true,"Quebec Minister of Cybersecurity and Digital Technology Éric Caire speaks at a news conference on artificial intelligence in Quebec City in 2024.Jacques Boissinot/The Canadian Press A long-serving minister in Quebec’s Coalition Avenir Quebec government has resigned amid a scandal over at least half a billion dollars in cost overruns related to the auto insurance board’s online platform. Cybersecurity and Digital Technology Minister Eric Caire made the announcement Thursday on social media, calling the controversy surrounding the digital transformation of the Societe de l’assurance automobile du Quebec (SAAQ) “totally unacceptable.” “Even though I assumed my responsibilities as minister in this matter and I have nothing to reproach myself for, apart from not having been suspicious enough, I came to the conclusion that it had become a distraction that was harmful to my government and my premier,” he said. The resignation follows an explosive report last week by the province’s auditor general that revealed cost overruns of at least $500 million in the creation of the online platform, for a total cost of more than $1.1 billion. On Thursday, news reports alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. Quebec Premier Francois Legault, who had previously defended his minister, told reporters Thursday that Caire chose to resign. “I’m going to get to the bottom of things,” he said. “I have absolutely nothing to hide.” He said he will name Caire’s replacement in a matter of days. The auto insurance board thought it would save hundreds of millions of dollars with the new online portal, SAAQclic. But auditor Guylaine Leclerc found that two years after it was implemented, it takes longer to deliver services and the cost is higher than with the previous system. The botched 2023 rollout of the online platform led to major delays and long lineups at SAAQ branches, where Quebeckers take road tests, register vehicles, and access other services. The auditor also found that the auto insurance board chose to mask $222 million in cost overruns to avoid “media and political risk.” In response, Caire and other Quebec ministers claimed they had been unaware of the cost overruns and accused the SAAQ of lying to them. Earlier this week, the government asked the province’s financial watchdog – Autorites des marches publics – and the anti-corruption police to investigate the matter. Still, Caire has faced mounting pressure to resign over the last week. On Thursday, news reports claimed he had known about the spiralling costs and had helped the auto insurance board hide the $222-million overrun. After Caire’s resignation, Quebec’s opposition parties reiterated their demand for a public inquiry to shed more light on the affair. “Just because they’ve sacrificed Mr. Caire today, that doesn’t mean there is no more problem,” Liberal House leader Monsef Derraji told reporters in Quebec City. “It’s not over,” said Quebec solitaire co-spokesperson Ruba Ghazal. “This is the beginning of the story.” Legault said he doesn’t think a public inquiry is necessary. “I don’t think there’s corruption,” he said. Still, he left the door open to further investigation, saying he’s going to “shine light” on the matter. Report an editorial error Report a technical issue Authors and topics you follow will be added to your personal news feed inFollowing.","Quebec Minister of Cybersecurity and Digital Technology Éric Caire speaks at a news conference on artificial intelligence in Quebec City in 2024.Jacques Boissinot/The Canadian Press A long-serving minister in Quebec’s Coalition Avenir Quebec government has resigned amid a scandal over at least half a billion dollars in cost overruns related to the auto insurance board’s online platform. Cybersecurity and Digital Technology Minister Eric Caire made the announcement Thursday on social media, calling the controversy surrounding the digital transformation of the Societe de l’assurance automobile du Quebec (SAAQ) “totally unacceptable.” “Even though I assumed my responsibilities as minister in this matter and I have nothing to reproach myself for, apart from not having been suspicious enough, I came to the conclusion that it had become a distraction that was harmful to my government and my premier,” he said. The resignation follows an explosive report last week by the province’s auditor general that revealed cost overruns of at least $500 million in the creation of the online platform, for a total cost of more than $1.1 billion. On Thursday, news reports alleged Caire had helped the auto insurance board hide the rising cost from the public. Caire denied the reports. Quebec Premier Francois Legault, who had previously defended his minister, told reporters Thursday that Caire chose to resign. “I’m going to get to the bottom of things,” he said. “I have absolutely nothing to hide.” He said he will name Caire’s replacement in a matter of days. The auto insurance board thought it would save hundreds of millions of dollars with the new online portal, SAAQclic. But auditor Guylaine Leclerc found that two years after it was implemented, it takes longer to deliver services and the cost is higher than with the previous system. The botched 2023 rollout of the online platform led to major delays and long lineups at SAAQ branches, where Quebeckers take road tests, register vehicles, and access other services. The auditor also found that the auto insurance board chose to mask $222 million in cost overruns to avoid “media and political risk.” In response, Caire and other Quebec ministers claimed they had been unaware of the cost overruns and accused the SAAQ of lying to them. Earlier this week, the government asked the province’s financial watchdog – Autorites des marches publics – and the anti-corruption police to investigate the matter. Still, Caire has faced mounting pressure to resign over the last week. On Thursday, news reports claimed he had known about the spiralling costs and had helped the auto insurance board hide the $222-million overrun. After Caire’s resignation, Quebec’s opposition parties reiterated their demand for a public inquiry to shed more light on the affair. “Just because they’ve sacrificed Mr. Caire today, that doesn’t mean there is no more problem,” Liberal House leader Monsef Derraji told reporters in Quebec City. “It’s not over,” said Quebec solitaire co-spokesperson Ruba Ghazal. “This is the beginning of the story.” Legault said he doesn’t think a public inquiry is necessary. “I don’t think there’s corruption,” he said. Still, he left the door open to further investigation, saying he’s going to “shine light” on the matter. Report an editorial error Report a technical issue Authors and topics you follow will be added to your personal news feed inFollowing.",Quebec minister Éric Caire resigns over auto board scandal,"

Key Points:
",Cybersecurity,"Open this photo in gallery: Quebec Minister of Cybersecurity and Digital Technology Éric Caire speaks at a news conference on artificial intelligence in Quebec City in 2024.Jacques Boissinot/The Canadian Press
A long-serving minister in Quebec’s Coal... [884 chars]"
